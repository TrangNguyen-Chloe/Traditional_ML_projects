{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_From_Scratch_(Lab).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbxAt696ms58"
      },
      "source": [
        "# Lab - Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkoZMtDfmyO7"
      },
      "source": [
        "## Logistic Regression from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaFlqj51nbGm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEGtW99Am75l"
      },
      "source": [
        "Dataset **Titanic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l2ZsmISOlGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b070ad3b-1761-4ef3-cb32-a18beea10996"
      },
      "source": [
        "titanic = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/titanic.csv')\n",
        "\n",
        "# Data manipulation\n",
        "titanic.fillna(titanic['Age'].mean(), inplace=True)\n",
        "titanic.replace({'Sex':{'male':0, 'female':1}}, inplace=True)\n",
        "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
        "titanic.drop(columns=['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], inplace=True)\n",
        "titanic.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>FamilySize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Survived  Pclass  Sex   Age  FamilySize\n",
              "0         0       3    0  22.0           2\n",
              "1         1       1    1  38.0           2\n",
              "2         1       3    1  26.0           1\n",
              "3         1       1    1  35.0           2\n",
              "4         0       3    0  35.0           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjGt2y_-pPHz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = titanic[titanic.columns[~titanic.columns.isin(['Survived'])]].values\n",
        "y = titanic[['Survived']].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NeIl8K9sozy",
        "outputId": "e1f2e2c5-6d96-4505-e8f4-fd59aaa2dc77"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)\n",
        "\n",
        "print('Training set:', X_train.shape, y_train.shape)\n",
        "print('Test set:', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (712, 4) (712, 1)\n",
            "Test set: (179, 4) (179, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S79QyfXSqeXj"
      },
      "source": [
        "### Scikit-learn Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta6jRuopqPVV"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Create Logistics Regression model from X and y\n",
        "lg = LogisticRegression()\n",
        "lg.fit(X_train, y_train)\n",
        "train_predictions_label = lg.predict(X_train)\n",
        "train_predictions_proba = lg.predict_proba(X_train)\n",
        "test_predictions_label = lg.predict(X_test)\n",
        "test_predictions_proba = lg.predict_proba(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6m4MjXoSBmv",
        "outputId": "0711bfef-1519-494b-d454-c4b97a4d03a0"
      },
      "source": [
        "test_predictions_label[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSWrGIO2SGHd",
        "outputId": "dae13780-3005-4b31-dc5d-3df3c349a8ae"
      },
      "source": [
        "test_predictions_proba[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11821434, 0.88178566],\n",
              "       [0.07056997, 0.92943003],\n",
              "       [0.39505241, 0.60494759],\n",
              "       [0.24810099, 0.75189901],\n",
              "       [0.79814993, 0.20185007]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ9o74VB4LPV",
        "outputId": "c94652ac-1fa7-40a7-bf5d-8389c298c6f9"
      },
      "source": [
        "\n",
        "# Show metrics\n",
        "print(\"Accuracy score on train set: %f\" % accuracy_score(y_train, train_predictions_label))\n",
        "print(\"Accuracy score on test set: %f\" % accuracy_score(y_test, test_predictions_label))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, test_predictions_label))\n",
        "print(classification_report(y_test, test_predictions_label))\n",
        "print('-----------')\n",
        "print('Log loss on train set:', log_loss(y_train, train_predictions_proba))\n",
        "print('Log loss on test set:', log_loss(y_test, test_predictions_proba))\n",
        "# Show parameters\n",
        "print('w = ', lg.coef_)\n",
        "print('b = ', lg.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score on train set: 0.799157\n",
            "Accuracy score on test set: 0.793296\n",
            "Confusion Matrix:\n",
            "[[97 17]\n",
            " [20 45]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       114\n",
            "           1       0.73      0.69      0.71        65\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.77      0.77       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "-----------\n",
            "Log loss on train set: 0.4452845704337929\n",
            "Log loss on test set: 0.4466709454181867\n",
            "w =  [[-1.18387774  2.56284417 -0.04074789 -0.21591208]]\n",
            "b =  [2.84100084]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keQAhakRrJWs"
      },
      "source": [
        "### Handmade Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0ZhoR3rXbO"
      },
      "source": [
        "**Forward Propagation:**\n",
        "$$Z = Xw + b$$\n",
        "$$\\hat{y} = \\sigma(Z) =\\sigma(Xw + b) $$\n",
        "$$J(w, b) = -\\frac{1}{m}\\sum_{i=1}^m{ \\Big( y^{(i)} log( \\hat{y}^{(i)}) + (1-y^{(i)}) log(1 - \\hat{y}^{(i)}) \\Big)} \\tag{5}$$\n",
        "\n",
        "**and Backward**\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X^T(\\hat{y}-y)\\tag{6}$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)}-y^{(i)})\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adHOPx63q5xI"
      },
      "source": [
        "# Initialize params\n",
        "def initialize_params(X):\n",
        "    '''Initialize w, b with zeros and return'''\n",
        "    s = X.shape[1]\n",
        "    w = np.zeros((s,1), dtype=float)\n",
        "    b = np.zeros((1,1), dtype=float)\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgabqgGhr8B5"
      },
      "source": [
        "# Implement sigmoid\n",
        "def sigmoid(Z):\n",
        "    y = 1/(1 + np.exp(-Z))\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK_0xR_fsFaD"
      },
      "source": [
        "# Forward propagation\n",
        "def forward(w, b, X):\n",
        "    '''Return y_hat'''\n",
        "\n",
        "    y_hat = sigmoid(X@w + b)\n",
        "    return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCzZpPg9sX4c"
      },
      "source": [
        "# Binary cross entropy loss\n",
        "# m, n = X.shape\n",
        "def binary_cross_entropy(y, y_hat):\n",
        "    '''Calculate loss function J and return'''\n",
        "    \n",
        "    J = (-1*(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))).mean()\n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhuGl5ATtT2d"
      },
      "source": [
        "# Backward propagation\n",
        "def backward(X, y, y_hat): #, w, b\n",
        "    '''Calculate dw, db and return'''\n",
        "    m = X.shape[0]\n",
        "    dw = (X.T@ (y_hat - y))/m\n",
        "    db = (y_hat-y).mean() #db = (1/m)*np.sum(y_hat-y, keepdims=True)\n",
        "    return dw, db\n",
        "\n",
        "# Update parameters\n",
        "def update_params(w, b, dw, db, learning_rate):\n",
        "    '''Update w, b and return'''\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J3nYXjMupQa"
      },
      "source": [
        "# Training process\n",
        "def train(X, y, iterations, learning_rate):\n",
        "    \n",
        "    history = {'train_loss': [],\n",
        "               'test_loss': []}\n",
        "\n",
        "\n",
        "    w, b = initialize_params(X)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Similary to Linear Regression lab, you need to perform all 4 steps of training per iteration\n",
        "        # Note: As we use train and validation set, let's calculate train loss and test loss at every iteration\n",
        "        # then save it to history dictionary defined above\n",
        "        # Your progress now should look like this\n",
        "        # train => train loss => save train loss => test loss => save test loss => backward => params update\n",
        "        \n",
        "        y_hat = forward(w, b, X)\n",
        "        J = binary_cross_entropy(y, y_hat)\n",
        "        history['train_loss'].append(J) \n",
        "        #update parameters\n",
        "        \n",
        "        \n",
        "        y_hat_test = forward(w, b, X_test)\n",
        "        test_loss = binary_cross_entropy(y_test, y_hat_test)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        \n",
        "        dw, db = backward(X, y, y_hat)\n",
        "        w, b = update_params(w, b, dw, db, learning_rate)\n",
        "#test every 1000 iters\n",
        "        if i%100:\n",
        "            print(f'Step {i}, train_loss {J}, test_loss {test_loss}')\n",
        "    return w, b, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6NGSUVpuVNA"
      },
      "source": [
        "# Predict\n",
        "def predict(w, b, X):\n",
        "    '''Return predicted y of X. \n",
        "    Note that you will return the label (class), not the probability\n",
        "    '''\n",
        "    y_hat = forward(w, b, X)\n",
        "    # return np.round(y_hat)\n",
        "    return (y_hat>=0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwjw8DWT7h4l"
      },
      "source": [
        "# if y_hat >= 0.5:\n",
        "#     return y_hat.astype(int)\n",
        "# return (y_hat>=0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTUDeY_fwIXx"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flsnpRQ20jsB",
        "outputId": "40052db5-ed2e-4675-d087-b40b192bc9a5"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(179, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDgSepfDwEHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44dd1b27-24cf-4a36-f558-43cbf04a3d2a"
      },
      "source": [
        "# Train the model and predict X_test\n",
        "w, b, history = train(X_train, y_train, iterations=20000, learning_rate=5e-3)\n",
        "predictions = predict(w, b, X_test) # label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Step 14950, train_loss 0.4554115665563523, test_loss 0.44267298995053633\n",
            "Step 14951, train_loss 0.4554108109664653, test_loss 0.44267241782370365\n",
            "Step 14952, train_loss 0.4554100554677923, test_loss 0.4426718458152502\n",
            "Step 14953, train_loss 0.4554093000603116, test_loss 0.4426712739251485\n",
            "Step 14954, train_loss 0.4554085447440021, test_loss 0.44267070215337073\n",
            "Step 14955, train_loss 0.455407789518842, test_loss 0.44267013049988907\n",
            "Step 14956, train_loss 0.45540703438480995, test_loss 0.4426695589646757\n",
            "Step 14957, train_loss 0.45540627934188455, test_loss 0.44266898754770323\n",
            "Step 14958, train_loss 0.4554055243900443, test_loss 0.44266841624894365\n",
            "Step 14959, train_loss 0.4554047695292679, test_loss 0.44266784506836937\n",
            "Step 14960, train_loss 0.4554040147595337, test_loss 0.44266727400595274\n",
            "Step 14961, train_loss 0.45540326008082044, test_loss 0.44266670306166594\n",
            "Step 14962, train_loss 0.45540250549310674, test_loss 0.4426661322354815\n",
            "Step 14963, train_loss 0.45540175099637104, test_loss 0.44266556152737147\n",
            "Step 14964, train_loss 0.4554009965905922, test_loss 0.4426649909373084\n",
            "Step 14965, train_loss 0.45540024227574843, test_loss 0.44266442046526466\n",
            "Step 14966, train_loss 0.45539948805181885, test_loss 0.44266385011121245\n",
            "Step 14967, train_loss 0.4553987339187816, test_loss 0.4426632798751241\n",
            "Step 14968, train_loss 0.45539797987661573, test_loss 0.44266270975697225\n",
            "Step 14969, train_loss 0.4553972259252995, test_loss 0.442662139756729\n",
            "Step 14970, train_loss 0.4553964720648118, test_loss 0.44266156987436694\n",
            "Step 14971, train_loss 0.4553957182951312, test_loss 0.44266100010985837\n",
            "Step 14972, train_loss 0.45539496461623646, test_loss 0.44266043046317566\n",
            "Step 14973, train_loss 0.4553942110281061, test_loss 0.44265986093429127\n",
            "Step 14974, train_loss 0.4553934575307189, test_loss 0.4426592915231777\n",
            "Step 14975, train_loss 0.4553927041240535, test_loss 0.4426587222298071\n",
            "Step 14976, train_loss 0.4553919508080886, test_loss 0.4426581530541523\n",
            "Step 14977, train_loss 0.4553911975828029, test_loss 0.4426575839961853\n",
            "Step 14978, train_loss 0.455390444448175, test_loss 0.4426570150558791\n",
            "Step 14979, train_loss 0.4553896914041839, test_loss 0.44265644623320577\n",
            "Step 14980, train_loss 0.455388938450808, test_loss 0.4426558775281378\n",
            "Step 14981, train_loss 0.45538818558802624, test_loss 0.44265530894064786\n",
            "Step 14982, train_loss 0.45538743281581706, test_loss 0.44265474047070824\n",
            "Step 14983, train_loss 0.4553866801341595, test_loss 0.44265417211829144\n",
            "Step 14984, train_loss 0.4553859275430323, test_loss 0.4426536038833701\n",
            "Step 14985, train_loss 0.45538517504241405, test_loss 0.4426530357659168\n",
            "Step 14986, train_loss 0.4553844226322837, test_loss 0.44265246776590383\n",
            "Step 14987, train_loss 0.4553836703126196, test_loss 0.44265189988330383\n",
            "Step 14988, train_loss 0.45538291808340114, test_loss 0.4426513321180893\n",
            "Step 14989, train_loss 0.4553821659446067, test_loss 0.4426507644702329\n",
            "Step 14990, train_loss 0.45538141389621506, test_loss 0.44265019693970703\n",
            "Step 14991, train_loss 0.4553806619382052, test_loss 0.4426496295264843\n",
            "Step 14992, train_loss 0.4553799100705557, test_loss 0.44264906223053735\n",
            "Step 14993, train_loss 0.4553791582932456, test_loss 0.4426484950518387\n",
            "Step 14994, train_loss 0.4553784066062536, test_loss 0.4426479279903609\n",
            "Step 14995, train_loss 0.4553776550095587, test_loss 0.44264736104607666\n",
            "Step 14996, train_loss 0.4553769035031394, test_loss 0.44264679421895853\n",
            "Step 14997, train_loss 0.4553761520869748, test_loss 0.442646227508979\n",
            "Step 14998, train_loss 0.4553754007610435, test_loss 0.4426456609161108\n",
            "Step 14999, train_loss 0.4553746495253247, test_loss 0.44264509444032657\n",
            "Step 15001, train_loss 0.45537314732443945, test_loss 0.44264396183990046\n",
            "Step 15002, train_loss 0.45537239635923066, test_loss 0.44264339571520395\n",
            "Step 15003, train_loss 0.45537164548414977, test_loss 0.44264282970748203\n",
            "Step 15004, train_loss 0.4553708946991755, test_loss 0.4426422638167071\n",
            "Step 15005, train_loss 0.4553701440042868, test_loss 0.44264169804285247\n",
            "Step 15006, train_loss 0.45536939339946264, test_loss 0.4426411323858899\n",
            "Step 15007, train_loss 0.4553686428846819, test_loss 0.44264056684579284\n",
            "Step 15008, train_loss 0.4553678924599234, test_loss 0.44264000142253374\n",
            "Step 15009, train_loss 0.45536714212516616, test_loss 0.44263943611608536\n",
            "Step 15010, train_loss 0.455366391880389, test_loss 0.44263887092642035\n",
            "Step 15011, train_loss 0.455365641725571, test_loss 0.44263830585351127\n",
            "Step 15012, train_loss 0.4553648916606911, test_loss 0.44263774089733116\n",
            "Step 15013, train_loss 0.45536414168572814, test_loss 0.44263717605785263\n",
            "Step 15014, train_loss 0.45536339180066115, test_loss 0.4426366113350485\n",
            "Step 15015, train_loss 0.4553626420054691, test_loss 0.4426360467288914\n",
            "Step 15016, train_loss 0.4553618923001309, test_loss 0.4426354822393542\n",
            "Step 15017, train_loss 0.4553611426846255, test_loss 0.44263491786640957\n",
            "Step 15018, train_loss 0.4553603931589321, test_loss 0.44263435361003045\n",
            "Step 15019, train_loss 0.45535964372302956, test_loss 0.44263378947018955\n",
            "Step 15020, train_loss 0.4553588943768968, test_loss 0.44263322544685957\n",
            "Step 15021, train_loss 0.4553581451205128, test_loss 0.44263266154001346\n",
            "Step 15022, train_loss 0.4553573959538568, test_loss 0.442632097749624\n",
            "Step 15023, train_loss 0.45535664687690774, test_loss 0.44263153407566397\n",
            "Step 15024, train_loss 0.45535589788964453, test_loss 0.44263097051810624\n",
            "Step 15025, train_loss 0.4553551489920463, test_loss 0.4426304070769237\n",
            "Step 15026, train_loss 0.45535440018409207, test_loss 0.4426298437520889\n",
            "Step 15027, train_loss 0.4553536514657609, test_loss 0.4426292805435752\n",
            "Step 15028, train_loss 0.45535290283703184, test_loss 0.442628717451355\n",
            "Step 15029, train_loss 0.455352154297884, test_loss 0.4426281544754016\n",
            "Step 15030, train_loss 0.45535140584829636, test_loss 0.44262759161568743\n",
            "Step 15031, train_loss 0.455350657488248, test_loss 0.44262702887218575\n",
            "Step 15032, train_loss 0.4553499092177182, test_loss 0.4426264662448691\n",
            "Step 15033, train_loss 0.45534916103668566, test_loss 0.4426259037337108\n",
            "Step 15034, train_loss 0.45534841294512995, test_loss 0.44262534133868336\n",
            "Step 15035, train_loss 0.45534766494302986, test_loss 0.4426247790597601\n",
            "Step 15036, train_loss 0.45534691703036456, test_loss 0.44262421689691367\n",
            "Step 15037, train_loss 0.4553461692071132, test_loss 0.44262365485011707\n",
            "Step 15038, train_loss 0.4553454214732549, test_loss 0.4426230929193432\n",
            "Step 15039, train_loss 0.45534467382876875, test_loss 0.4426225311045651\n",
            "Step 15040, train_loss 0.455343926273634, test_loss 0.4426219694057558\n",
            "Step 15041, train_loss 0.45534317880782976, test_loss 0.44262140782288795\n",
            "Step 15042, train_loss 0.4553424314313352, test_loss 0.44262084635593496\n",
            "Step 15043, train_loss 0.45534168414412934, test_loss 0.4426202850048695\n",
            "Step 15044, train_loss 0.4553409369461915, test_loss 0.4426197237696647\n",
            "Step 15045, train_loss 0.4553401898375008, test_loss 0.44261916265029344\n",
            "Step 15046, train_loss 0.45533944281803646, test_loss 0.44261860164672895\n",
            "Step 15047, train_loss 0.4553386958877776, test_loss 0.442618040758944\n",
            "Step 15048, train_loss 0.4553379490467034, test_loss 0.44261747998691175\n",
            "Step 15049, train_loss 0.45533720229479313, test_loss 0.44261691933060526\n",
            "Step 15050, train_loss 0.45533645563202607, test_loss 0.44261635878999755\n",
            "Step 15051, train_loss 0.4553357090583813, test_loss 0.4426157983650616\n",
            "Step 15052, train_loss 0.45533496257383815, test_loss 0.4426152380557705\n",
            "Step 15053, train_loss 0.4553342161783758, test_loss 0.4426146778620974\n",
            "Step 15054, train_loss 0.4553334698719734, test_loss 0.4426141177840152\n",
            "Step 15055, train_loss 0.45533272365461025, test_loss 0.44261355782149725\n",
            "Step 15056, train_loss 0.4553319775262658, test_loss 0.4426129979745163\n",
            "Step 15057, train_loss 0.45533123148691895, test_loss 0.4426124382430458\n",
            "Step 15058, train_loss 0.4553304855365493, test_loss 0.44261187862705853\n",
            "Step 15059, train_loss 0.45532973967513596, test_loss 0.4426113191265279\n",
            "Step 15060, train_loss 0.4553289939026583, test_loss 0.4426107597414268\n",
            "Step 15061, train_loss 0.45532824821909534, test_loss 0.4426102004717285\n",
            "Step 15062, train_loss 0.45532750262442667, test_loss 0.4426096413174061\n",
            "Step 15063, train_loss 0.4553267571186316, test_loss 0.44260908227843265\n",
            "Step 15064, train_loss 0.45532601170168924, test_loss 0.44260852335478146\n",
            "Step 15065, train_loss 0.4553252663735789, test_loss 0.44260796454642565\n",
            "Step 15066, train_loss 0.45532452113428, test_loss 0.4426074058533384\n",
            "Step 15067, train_loss 0.45532377598377205, test_loss 0.4426068472754928\n",
            "Step 15068, train_loss 0.45532303092203413, test_loss 0.44260628881286207\n",
            "Step 15069, train_loss 0.4553222859490454, test_loss 0.44260573046541946\n",
            "Step 15070, train_loss 0.45532154106478573, test_loss 0.44260517223313817\n",
            "Step 15071, train_loss 0.45532079626923405, test_loss 0.4426046141159912\n",
            "Step 15072, train_loss 0.45532005156236993, test_loss 0.44260405611395215\n",
            "Step 15073, train_loss 0.45531930694417266, test_loss 0.442603498226994\n",
            "Step 15074, train_loss 0.4553185624146216, test_loss 0.44260294045509\n",
            "Step 15075, train_loss 0.45531781797369625, test_loss 0.4426023827982135\n",
            "Step 15076, train_loss 0.4553170736213759, test_loss 0.4426018252563376\n",
            "Step 15077, train_loss 0.45531632935764, test_loss 0.4426012678294357\n",
            "Step 15078, train_loss 0.45531558518246784, test_loss 0.44260071051748096\n",
            "Step 15079, train_loss 0.4553148410958389, test_loss 0.44260015332044667\n",
            "Step 15080, train_loss 0.45531409709773274, test_loss 0.4425995962383063\n",
            "Step 15081, train_loss 0.45531335318812843, test_loss 0.44259903927103283\n",
            "Step 15082, train_loss 0.45531260936700585, test_loss 0.4425984824185997\n",
            "Step 15083, train_loss 0.455311865634344, test_loss 0.4425979256809802\n",
            "Step 15084, train_loss 0.45531112199012275, test_loss 0.44259736905814784\n",
            "Step 15085, train_loss 0.45531037843432115, test_loss 0.44259681255007577\n",
            "Step 15086, train_loss 0.4553096349669189, test_loss 0.4425962561567372\n",
            "Step 15087, train_loss 0.4553088915878955, test_loss 0.44259569987810565\n",
            "Step 15088, train_loss 0.4553081482972302, test_loss 0.4425951437141544\n",
            "Step 15089, train_loss 0.45530740509490264, test_loss 0.44259458766485676\n",
            "Step 15090, train_loss 0.45530666198089237, test_loss 0.4425940317301862\n",
            "Step 15091, train_loss 0.4553059189551786, test_loss 0.4425934759101161\n",
            "Step 15092, train_loss 0.4553051760177411, test_loss 0.4425929202046196\n",
            "Step 15093, train_loss 0.45530443316855934, test_loss 0.4425923646136704\n",
            "Step 15094, train_loss 0.4553036904076127, test_loss 0.4425918091372417\n",
            "Step 15095, train_loss 0.4553029477348809, test_loss 0.442591253775307\n",
            "Step 15096, train_loss 0.45530220515034325, test_loss 0.4425906985278398\n",
            "Step 15097, train_loss 0.4553014626539795, test_loss 0.4425901433948132\n",
            "Step 15098, train_loss 0.455300720245769, test_loss 0.4425895883762009\n",
            "Step 15099, train_loss 0.4552999779256913, test_loss 0.4425890334719761\n",
            "Step 15101, train_loss 0.45529849354985297, test_loss 0.44258792400658353\n",
            "Step 15102, train_loss 0.4552977514940512, test_loss 0.4425873694453624\n",
            "Step 15103, train_loss 0.45529700952630076, test_loss 0.4425868149984228\n",
            "Step 15104, train_loss 0.4552962676465809, test_loss 0.4425862606657381\n",
            "Step 15105, train_loss 0.4552955258548714, test_loss 0.44258570644728196\n",
            "Step 15106, train_loss 0.4552947841511517, test_loss 0.44258515234302764\n",
            "Step 15107, train_loss 0.45529404253540157, test_loss 0.44258459835294867\n",
            "Step 15108, train_loss 0.4552933010076005, test_loss 0.44258404447701866\n",
            "Step 15109, train_loss 0.45529255956772824, test_loss 0.44258349071521114\n",
            "Step 15110, train_loss 0.45529181821576425, test_loss 0.44258293706749946\n",
            "Step 15111, train_loss 0.45529107695168813, test_loss 0.4425823835338572\n",
            "Step 15112, train_loss 0.4552903357754797, test_loss 0.442581830114258\n",
            "Step 15113, train_loss 0.4552895946871186, test_loss 0.4425812768086754\n",
            "Step 15114, train_loss 0.4552888536865843, test_loss 0.4425807236170829\n",
            "Step 15115, train_loss 0.45528811277385656, test_loss 0.442580170539454\n",
            "Step 15116, train_loss 0.455287371948915, test_loss 0.4425796175757625\n",
            "Step 15117, train_loss 0.4552866312117393, test_loss 0.4425790647259817\n",
            "Step 15118, train_loss 0.4552858905623092, test_loss 0.44257851199008524\n",
            "Step 15119, train_loss 0.4552851500006044, test_loss 0.44257795936804684\n",
            "Step 15120, train_loss 0.4552844095266044, test_loss 0.44257740685984004\n",
            "Step 15121, train_loss 0.45528366914028917, test_loss 0.4425768544654384\n",
            "Step 15122, train_loss 0.45528292884163823, test_loss 0.44257630218481564\n",
            "Step 15123, train_loss 0.45528218863063125, test_loss 0.4425757500179452\n",
            "Step 15124, train_loss 0.45528144850724805, test_loss 0.4425751979648009\n",
            "Step 15125, train_loss 0.4552807084714683, test_loss 0.44257464602535623\n",
            "Step 15126, train_loss 0.45527996852327185, test_loss 0.442574094199585\n",
            "Step 15127, train_loss 0.4552792286626383, test_loss 0.4425735424874608\n",
            "Step 15128, train_loss 0.45527848888954736, test_loss 0.4425729908889572\n",
            "Step 15129, train_loss 0.45527774920397895, test_loss 0.44257243940404795\n",
            "Step 15130, train_loss 0.4552770096059127, test_loss 0.44257188803270675\n",
            "Step 15131, train_loss 0.45527627009532845, test_loss 0.4425713367749071\n",
            "Step 15132, train_loss 0.4552755306722059, test_loss 0.44257078563062296\n",
            "Step 15133, train_loss 0.4552747913365248, test_loss 0.44257023459982797\n",
            "Step 15134, train_loss 0.4552740520882651, test_loss 0.4425696836824957\n",
            "Step 15135, train_loss 0.4552733129274064, test_loss 0.44256913287860006\n",
            "Step 15136, train_loss 0.4552725738539286, test_loss 0.44256858218811457\n",
            "Step 15137, train_loss 0.4552718348678115, test_loss 0.44256803161101316\n",
            "Step 15138, train_loss 0.45527109596903487, test_loss 0.4425674811472693\n",
            "Step 15139, train_loss 0.45527035715757846, test_loss 0.4425669307968571\n",
            "Step 15140, train_loss 0.4552696184334223, test_loss 0.44256638055975006\n",
            "Step 15141, train_loss 0.4552688797965461, test_loss 0.442565830435922\n",
            "Step 15142, train_loss 0.45526814124692955, test_loss 0.4425652804253467\n",
            "Step 15143, train_loss 0.45526740278455285, test_loss 0.442564730527998\n",
            "Step 15144, train_loss 0.45526666440939556, test_loss 0.4425641807438496\n",
            "Step 15145, train_loss 0.45526592612143757, test_loss 0.4425636310728754\n",
            "Step 15146, train_loss 0.45526518792065895, test_loss 0.44256308151504903\n",
            "Step 15147, train_loss 0.4552644498070393, test_loss 0.4425625320703444\n",
            "Step 15148, train_loss 0.4552637117805586, test_loss 0.4425619827387355\n",
            "Step 15149, train_loss 0.45526297384119685, test_loss 0.4425614335201958\n",
            "Step 15150, train_loss 0.4552622359889338, test_loss 0.44256088441469943\n",
            "Step 15151, train_loss 0.45526149822374934, test_loss 0.44256033542222006\n",
            "Step 15152, train_loss 0.4552607605456235, test_loss 0.4425597865427317\n",
            "Step 15153, train_loss 0.4552600229545361, test_loss 0.44255923777620804\n",
            "Step 15154, train_loss 0.455259285450467, test_loss 0.44255868912262303\n",
            "Step 15155, train_loss 0.4552585480333961, test_loss 0.4425581405819505\n",
            "Step 15156, train_loss 0.4552578107033036, test_loss 0.4425575921541645\n",
            "Step 15157, train_loss 0.4552570734601693, test_loss 0.4425570438392386\n",
            "Step 15158, train_loss 0.45525633630397305, test_loss 0.442556495637147\n",
            "Step 15159, train_loss 0.4552555992346949, test_loss 0.4425559475478634\n",
            "Step 15160, train_loss 0.45525486225231465, test_loss 0.4425553995713619\n",
            "Step 15161, train_loss 0.45525412535681253, test_loss 0.4425548517076162\n",
            "Step 15162, train_loss 0.4552533885481684, test_loss 0.44255430395660056\n",
            "Step 15163, train_loss 0.455252651826362, test_loss 0.4425537563182885\n",
            "Step 15164, train_loss 0.4552519151913736, test_loss 0.4425532087926543\n",
            "Step 15165, train_loss 0.45525117864318315, test_loss 0.4425526613796717\n",
            "Step 15166, train_loss 0.4552504421817705, test_loss 0.4425521140793148\n",
            "Step 15167, train_loss 0.455249705807116, test_loss 0.4425515668915574\n",
            "Step 15168, train_loss 0.4552489695191992, test_loss 0.44255101981637357\n",
            "Step 15169, train_loss 0.45524823331800035, test_loss 0.4425504728537374\n",
            "Step 15170, train_loss 0.4552474972034995, test_loss 0.44254992600362275\n",
            "Step 15171, train_loss 0.4552467611756767, test_loss 0.4425493792660035\n",
            "Step 15172, train_loss 0.45524602523451185, test_loss 0.442548832640854\n",
            "Step 15173, train_loss 0.45524528937998515, test_loss 0.442548286128148\n",
            "Step 15174, train_loss 0.4552445536120765, test_loss 0.44254773972785955\n",
            "Step 15175, train_loss 0.45524381793076607, test_loss 0.44254719343996274\n",
            "Step 15176, train_loss 0.45524308233603383, test_loss 0.4425466472644314\n",
            "Step 15177, train_loss 0.45524234682785997, test_loss 0.44254610120124\n",
            "Step 15178, train_loss 0.45524161140622454, test_loss 0.44254555525036215\n",
            "Step 15179, train_loss 0.4552408760711076, test_loss 0.44254500941177227\n",
            "Step 15180, train_loss 0.45524014082248915, test_loss 0.44254446368544414\n",
            "Step 15181, train_loss 0.4552394056603494, test_loss 0.442543918071352\n",
            "Step 15182, train_loss 0.4552386705846684, test_loss 0.4425433725694699\n",
            "Step 15183, train_loss 0.4552379355954262, test_loss 0.44254282717977195\n",
            "Step 15184, train_loss 0.45523720069260315, test_loss 0.44254228190223205\n",
            "Step 15185, train_loss 0.45523646587617916, test_loss 0.44254173673682456\n",
            "Step 15186, train_loss 0.45523573114613436, test_loss 0.44254119168352357\n",
            "Step 15187, train_loss 0.4552349965024489, test_loss 0.442540646742303\n",
            "Step 15188, train_loss 0.4552342619451031, test_loss 0.44254010191313736\n",
            "Step 15189, train_loss 0.45523352747407697, test_loss 0.4425395571960002\n",
            "Step 15190, train_loss 0.4552327930893506, test_loss 0.4425390125908662\n",
            "Step 15191, train_loss 0.4552320587909042, test_loss 0.4425384680977093\n",
            "Step 15192, train_loss 0.45523132457871807, test_loss 0.44253792371650363\n",
            "Step 15193, train_loss 0.4552305904527723, test_loss 0.4425373794472234\n",
            "Step 15194, train_loss 0.45522985641304703, test_loss 0.4425368352898428\n",
            "Step 15195, train_loss 0.45522912245952235, test_loss 0.44253629124433597\n",
            "Step 15196, train_loss 0.4552283885921786, test_loss 0.44253574731067713\n",
            "Step 15197, train_loss 0.45522765481099614, test_loss 0.4425352034888404\n",
            "Step 15198, train_loss 0.45522692111595486, test_loss 0.4425346597788002\n",
            "Step 15199, train_loss 0.4552261875070352, test_loss 0.4425341161805305\n",
            "Step 15201, train_loss 0.45522472054748125, test_loss 0.4425330293191998\n",
            "Step 15202, train_loss 0.45522398719680746, test_loss 0.4425324860560873\n",
            "Step 15203, train_loss 0.4552232539321761, test_loss 0.4425319429046423\n",
            "Step 15204, train_loss 0.4552225207535676, test_loss 0.442531399864839\n",
            "Step 15205, train_loss 0.45522178766096194, test_loss 0.4425308569366518\n",
            "Step 15206, train_loss 0.4552210546543395, test_loss 0.44253031412005484\n",
            "Step 15207, train_loss 0.45522032173368066, test_loss 0.4425297714150225\n",
            "Step 15208, train_loss 0.4552195888989656, test_loss 0.4425292288215291\n",
            "Step 15209, train_loss 0.45521885615017443, test_loss 0.4425286863395487\n",
            "Step 15210, train_loss 0.4552181234872877, test_loss 0.44252814396905576\n",
            "Step 15211, train_loss 0.45521739091028557, test_loss 0.4425276017100248\n",
            "Step 15212, train_loss 0.45521665841914843, test_loss 0.44252705956242966\n",
            "Step 15213, train_loss 0.4552159260138564, test_loss 0.4425265175262449\n",
            "Step 15214, train_loss 0.45521519369439, test_loss 0.4425259756014448\n",
            "Step 15215, train_loss 0.4552144614607295, test_loss 0.44252543378800385\n",
            "Step 15216, train_loss 0.4552137293128552, test_loss 0.4425248920858963\n",
            "Step 15217, train_loss 0.4552129972507473, test_loss 0.4425243504950965\n",
            "Step 15218, train_loss 0.4552122652743865, test_loss 0.4425238090155787\n",
            "Step 15219, train_loss 0.4552115333837527, test_loss 0.44252326764731736\n",
            "Step 15220, train_loss 0.45521080157882665, test_loss 0.44252272639028695\n",
            "Step 15221, train_loss 0.4552100698595884, test_loss 0.44252218524446163\n",
            "Step 15222, train_loss 0.4552093382260184, test_loss 0.4425216442098159\n",
            "Step 15223, train_loss 0.4552086066780972, test_loss 0.44252110328632416\n",
            "Step 15224, train_loss 0.45520787521580497, test_loss 0.4425205624739609\n",
            "Step 15225, train_loss 0.45520714383912225, test_loss 0.44252002177270033\n",
            "Step 15226, train_loss 0.4552064125480292, test_loss 0.4425194811825171\n",
            "Step 15227, train_loss 0.4552056813425065, test_loss 0.4425189407033855\n",
            "Step 15228, train_loss 0.4552049502225343, test_loss 0.4425184003352799\n",
            "Step 15229, train_loss 0.4552042191880933, test_loss 0.4425178600781749\n",
            "Step 15230, train_loss 0.4552034882391636, test_loss 0.4425173199320448\n",
            "Step 15231, train_loss 0.4552027573757258, test_loss 0.4425167798968641\n",
            "Step 15232, train_loss 0.4552020265977603, test_loss 0.44251623997260753\n",
            "Step 15233, train_loss 0.4552012959052476, test_loss 0.44251570015924907\n",
            "Step 15234, train_loss 0.45520056529816805, test_loss 0.4425151604567634\n",
            "Step 15235, train_loss 0.45519983477650217, test_loss 0.4425146208651253\n",
            "Step 15236, train_loss 0.4551991043402303, test_loss 0.4425140813843088\n",
            "Step 15237, train_loss 0.4551983739893331, test_loss 0.4425135420142888\n",
            "Step 15238, train_loss 0.4551976437237908, test_loss 0.4425130027550395\n",
            "Step 15239, train_loss 0.4551969135435841, test_loss 0.4425124636065356\n",
            "Step 15240, train_loss 0.45519618344869334, test_loss 0.44251192456875166\n",
            "Step 15241, train_loss 0.45519545343909895, test_loss 0.442511385641662\n",
            "Step 15242, train_loss 0.4551947235147817, test_loss 0.4425108468252412\n",
            "Step 15243, train_loss 0.45519399367572183, test_loss 0.44251030811946407\n",
            "Step 15244, train_loss 0.4551932639218999, test_loss 0.44250976952430493\n",
            "Step 15245, train_loss 0.45519253425329653, test_loss 0.4425092310397383\n",
            "Step 15246, train_loss 0.45519180466989206, test_loss 0.44250869266573906\n",
            "Step 15247, train_loss 0.45519107517166724, test_loss 0.44250815440228136\n",
            "Step 15248, train_loss 0.4551903457586025, test_loss 0.44250761624934015\n",
            "Step 15249, train_loss 0.45518961643067835, test_loss 0.4425070782068898\n",
            "Step 15250, train_loss 0.4551888871878753, test_loss 0.44250654027490494\n",
            "Step 15251, train_loss 0.455188158030174, test_loss 0.4425060024533602\n",
            "Step 15252, train_loss 0.4551874289575549, test_loss 0.44250546474223035\n",
            "Step 15253, train_loss 0.45518669996999866, test_loss 0.4425049271414897\n",
            "Step 15254, train_loss 0.45518597106748593, test_loss 0.44250438965111316\n",
            "Step 15255, train_loss 0.455185242249997, test_loss 0.4425038522710752\n",
            "Step 15256, train_loss 0.4551845135175129, test_loss 0.4425033150013505\n",
            "Step 15257, train_loss 0.45518378487001365, test_loss 0.44250277784191383\n",
            "Step 15258, train_loss 0.4551830563074804, test_loss 0.4425022407927397\n",
            "Step 15259, train_loss 0.45518232782989343, test_loss 0.4425017038538027\n",
            "Step 15260, train_loss 0.4551815994372334, test_loss 0.4425011670250779\n",
            "Step 15261, train_loss 0.455180871129481, test_loss 0.4425006303065396\n",
            "Step 15262, train_loss 0.4551801429066168, test_loss 0.4425000936981625\n",
            "Step 15263, train_loss 0.45517941476862156, test_loss 0.44249955719992146\n",
            "Step 15264, train_loss 0.4551786867154756, test_loss 0.4424990208117912\n",
            "Step 15265, train_loss 0.4551779587471599, test_loss 0.4424984845337462\n",
            "Step 15266, train_loss 0.455177230863655, test_loss 0.44249794836576145\n",
            "Step 15267, train_loss 0.4551765030649415, test_loss 0.4424974123078116\n",
            "Step 15268, train_loss 0.45517577535099996, test_loss 0.4424968763598712\n",
            "Step 15269, train_loss 0.4551750477218113, test_loss 0.4424963405219153\n",
            "Step 15270, train_loss 0.455174320177356, test_loss 0.4424958047939184\n",
            "Step 15271, train_loss 0.45517359271761487, test_loss 0.4424952691758555\n",
            "Step 15272, train_loss 0.4551728653425686, test_loss 0.4424947336677009\n",
            "Step 15273, train_loss 0.4551721380521977, test_loss 0.4424941982694299\n",
            "Step 15274, train_loss 0.45517141084648305, test_loss 0.44249366298101706\n",
            "Step 15275, train_loss 0.45517068372540537, test_loss 0.4424931278024371\n",
            "Step 15276, train_loss 0.45516995668894517, test_loss 0.4424925927336648\n",
            "Step 15277, train_loss 0.4551692297370834, test_loss 0.4424920577746753\n",
            "Step 15278, train_loss 0.45516850286980076, test_loss 0.44249152292544297\n",
            "Step 15279, train_loss 0.45516777608707787, test_loss 0.4424909881859428\n",
            "Step 15280, train_loss 0.4551670493888954, test_loss 0.44249045355614963\n",
            "Step 15281, train_loss 0.45516632277523433, test_loss 0.44248991903603835\n",
            "Step 15282, train_loss 0.45516559624607517, test_loss 0.4424893846255836\n",
            "Step 15283, train_loss 0.4551648698013989, test_loss 0.44248885032476054\n",
            "Step 15284, train_loss 0.4551641434411861, test_loss 0.44248831613354367\n",
            "Step 15285, train_loss 0.45516341716541775, test_loss 0.4424877820519082\n",
            "Step 15286, train_loss 0.45516269097407436, test_loss 0.44248724807982853\n",
            "Step 15287, train_loss 0.4551619648671369, test_loss 0.44248671421728014\n",
            "Step 15288, train_loss 0.45516123884458615, test_loss 0.4424861804642374\n",
            "Step 15289, train_loss 0.45516051290640275, test_loss 0.4424856468206755\n",
            "Step 15290, train_loss 0.45515978705256765, test_loss 0.44248511328656914\n",
            "Step 15291, train_loss 0.4551590612830617, test_loss 0.44248457986189327\n",
            "Step 15292, train_loss 0.45515833559786556, test_loss 0.442484046546623\n",
            "Step 15293, train_loss 0.4551576099969601, test_loss 0.4424835133407331\n",
            "Step 15294, train_loss 0.4551568844803262, test_loss 0.44248298024419835\n",
            "Step 15295, train_loss 0.45515615904794476, test_loss 0.442482447256994\n",
            "Step 15296, train_loss 0.45515543369979644, test_loss 0.44248191437909473\n",
            "Step 15297, train_loss 0.4551547084358622, test_loss 0.4424813816104757\n",
            "Step 15298, train_loss 0.4551539832561229, test_loss 0.4424808489511116\n",
            "Step 15299, train_loss 0.45515325816055924, test_loss 0.4424803164009775\n",
            "Step 15301, train_loss 0.4551518082218829, test_loss 0.44247925162829965\n",
            "Step 15302, train_loss 0.45515108337873195, test_loss 0.4424787194057057\n",
            "Step 15303, train_loss 0.4551503586196801, test_loss 0.44247818729224175\n",
            "Step 15304, train_loss 0.4551496339447085, test_loss 0.4424776552878828\n",
            "Step 15305, train_loss 0.45514890935379787, test_loss 0.44247712339260364\n",
            "Step 15306, train_loss 0.45514818484692926, test_loss 0.4424765916063797\n",
            "Step 15307, train_loss 0.4551474604240835, test_loss 0.4424760599291857\n",
            "Step 15308, train_loss 0.45514673608524153, test_loss 0.4424755283609967\n",
            "Step 15309, train_loss 0.4551460118303843, test_loss 0.4424749969017879\n",
            "Step 15310, train_loss 0.4551452876594926, test_loss 0.44247446555153425\n",
            "Step 15311, train_loss 0.4551445635725475, test_loss 0.44247393431021076\n",
            "Step 15312, train_loss 0.4551438395695299, test_loss 0.44247340317779243\n",
            "Step 15313, train_loss 0.45514311565042076, test_loss 0.4424728721542546\n",
            "Step 15314, train_loss 0.45514239181520094, test_loss 0.4424723412395721\n",
            "Step 15315, train_loss 0.4551416680638516, test_loss 0.44247181043372\n",
            "Step 15316, train_loss 0.45514094439635366, test_loss 0.44247127973667355\n",
            "Step 15317, train_loss 0.4551402208126879, test_loss 0.44247074914840784\n",
            "Step 15318, train_loss 0.4551394973128354, test_loss 0.4424702186688978\n",
            "Step 15319, train_loss 0.4551387738967772, test_loss 0.4424696882981186\n",
            "Step 15320, train_loss 0.4551380505644942, test_loss 0.44246915803604553\n",
            "Step 15321, train_loss 0.45513732731596757, test_loss 0.4424686278826535\n",
            "Step 15322, train_loss 0.45513660415117807, test_loss 0.4424680978379178\n",
            "Step 15323, train_loss 0.45513588107010694, test_loss 0.44246756790181346\n",
            "Step 15324, train_loss 0.4551351580727351, test_loss 0.44246703807431564\n",
            "Step 15325, train_loss 0.4551344351590435, test_loss 0.44246650835539963\n",
            "Step 15326, train_loss 0.4551337123290133, test_loss 0.4424659787450404\n",
            "Step 15327, train_loss 0.4551329895826254, test_loss 0.44246544924321324\n",
            "Step 15328, train_loss 0.4551322669198608, test_loss 0.4424649198498934\n",
            "Step 15329, train_loss 0.4551315443407008, test_loss 0.44246439056505577\n",
            "Step 15330, train_loss 0.4551308218451262, test_loss 0.4424638613886759\n",
            "Step 15331, train_loss 0.4551300994331184, test_loss 0.4424633323207287\n",
            "Step 15332, train_loss 0.455129377104658, test_loss 0.44246280336118954\n",
            "Step 15333, train_loss 0.4551286548597264, test_loss 0.4424622745100337\n",
            "Step 15334, train_loss 0.4551279326983045, test_loss 0.4424617457672362\n",
            "Step 15335, train_loss 0.4551272106203736, test_loss 0.4424612171327725\n",
            "Step 15336, train_loss 0.45512648862591454, test_loss 0.4424606886066175\n",
            "Step 15337, train_loss 0.4551257667149086, test_loss 0.4424601601887468\n",
            "Step 15338, train_loss 0.45512504488733674, test_loss 0.4424596318791353\n",
            "Step 15339, train_loss 0.45512432314318024, test_loss 0.4424591036777587\n",
            "Step 15340, train_loss 0.4551236014824202, test_loss 0.44245857558459184\n",
            "Step 15341, train_loss 0.4551228799050375, test_loss 0.4424580475996103\n",
            "Step 15342, train_loss 0.45512215841101344, test_loss 0.4424575197227891\n",
            "Step 15343, train_loss 0.4551214370003292, test_loss 0.44245699195410376\n",
            "Step 15344, train_loss 0.4551207156729659, test_loss 0.4424564642935295\n",
            "Step 15345, train_loss 0.45511999442890455, test_loss 0.44245593674104156\n",
            "Step 15346, train_loss 0.45511927326812657, test_loss 0.4424554092966152\n",
            "Step 15347, train_loss 0.4551185521906128, test_loss 0.4424548819602258\n",
            "Step 15348, train_loss 0.45511783119634464, test_loss 0.44245435473184885\n",
            "Step 15349, train_loss 0.45511711028530316, test_loss 0.44245382761145946\n",
            "Step 15350, train_loss 0.4551163894574696, test_loss 0.44245330059903304\n",
            "Step 15351, train_loss 0.4551156687128251, test_loss 0.44245277369454494\n",
            "Step 15352, train_loss 0.4551149480513508, test_loss 0.4424522468979706\n",
            "Step 15353, train_loss 0.45511422747302793, test_loss 0.44245172020928514\n",
            "Step 15354, train_loss 0.45511350697783776, test_loss 0.4424511936284642\n",
            "Step 15355, train_loss 0.45511278656576143, test_loss 0.44245066715548287\n",
            "Step 15356, train_loss 0.4551120662367803, test_loss 0.44245014079031675\n",
            "Step 15357, train_loss 0.4551113459908753, test_loss 0.4424496145329413\n",
            "Step 15358, train_loss 0.45511062582802786, test_loss 0.44244908838333163\n",
            "Step 15359, train_loss 0.4551099057482193, test_loss 0.4424485623414633\n",
            "Step 15360, train_loss 0.4551091857514306, test_loss 0.44244803640731173\n",
            "Step 15361, train_loss 0.45510846583764325, test_loss 0.4424475105808525\n",
            "Step 15362, train_loss 0.4551077460068383, test_loss 0.44244698486206074\n",
            "Step 15363, train_loss 0.4551070262589972, test_loss 0.4424464592509119\n",
            "Step 15364, train_loss 0.45510630659410095, test_loss 0.4424459337473816\n",
            "Step 15365, train_loss 0.4551055870121311, test_loss 0.4424454083514453\n",
            "Step 15366, train_loss 0.45510486751306883, test_loss 0.4424448830630783\n",
            "Step 15367, train_loss 0.4551041480968954, test_loss 0.4424443578822561\n",
            "Step 15368, train_loss 0.4551034287635921, test_loss 0.44244383280895416\n",
            "Step 15369, train_loss 0.45510270951314025, test_loss 0.442443307843148\n",
            "Step 15370, train_loss 0.45510199034552107, test_loss 0.4424427829848131\n",
            "Step 15371, train_loss 0.45510127126071603, test_loss 0.4424422582339249\n",
            "Step 15372, train_loss 0.4551005522587062, test_loss 0.4424417335904591\n",
            "Step 15373, train_loss 0.4550998333394732, test_loss 0.44244120905439077\n",
            "Step 15374, train_loss 0.45509911450299806, test_loss 0.4424406846256959\n",
            "Step 15375, train_loss 0.4550983957492623, test_loss 0.44244016030434963\n",
            "Step 15376, train_loss 0.4550976770782472, test_loss 0.4424396360903279\n",
            "Step 15377, train_loss 0.4550969584899341, test_loss 0.44243911198360586\n",
            "Step 15378, train_loss 0.4550962399843045, test_loss 0.44243858798415914\n",
            "Step 15379, train_loss 0.45509552156133937, test_loss 0.4424380640919634\n",
            "Step 15380, train_loss 0.4550948032210204, test_loss 0.442437540306994\n",
            "Step 15381, train_loss 0.45509408496332904, test_loss 0.4424370166292267\n",
            "Step 15382, train_loss 0.4550933667882464, test_loss 0.442436493058637\n",
            "Step 15383, train_loss 0.45509264869575394, test_loss 0.4424359695952006\n",
            "Step 15384, train_loss 0.4550919306858331, test_loss 0.4424354462388928\n",
            "Step 15385, train_loss 0.45509121275846515, test_loss 0.4424349229896894\n",
            "Step 15386, train_loss 0.4550904949136317, test_loss 0.442434399847566\n",
            "Step 15387, train_loss 0.455089777151314, test_loss 0.4424338768124982\n",
            "Step 15388, train_loss 0.45508905947149353, test_loss 0.44243335388446137\n",
            "Step 15389, train_loss 0.45508834187415165, test_loss 0.44243283106343134\n",
            "Step 15390, train_loss 0.45508762435926975, test_loss 0.44243230834938374\n",
            "Step 15391, train_loss 0.4550869069268294, test_loss 0.44243178574229436\n",
            "Step 15392, train_loss 0.4550861895768119, test_loss 0.4424312632421385\n",
            "Step 15393, train_loss 0.4550854723091988, test_loss 0.44243074084889195\n",
            "Step 15394, train_loss 0.4550847551239715, test_loss 0.44243021856253045\n",
            "Step 15395, train_loss 0.4550840380211114, test_loss 0.44242969638302954\n",
            "Step 15396, train_loss 0.45508332100060006, test_loss 0.44242917431036494\n",
            "Step 15397, train_loss 0.4550826040624188, test_loss 0.4424286523445124\n",
            "Step 15398, train_loss 0.4550818872065492, test_loss 0.44242813048544744\n",
            "Step 15399, train_loss 0.4550811704329728, test_loss 0.4424276087331459\n",
            "Step 15401, train_loss 0.45507973713262534, test_loss 0.4424265655487357\n",
            "Step 15402, train_loss 0.4550790206058173, test_loss 0.4424260441165785\n",
            "Step 15403, train_loss 0.45507830416122813, test_loss 0.4424255227910874\n",
            "Step 15404, train_loss 0.45507758779883983, test_loss 0.4424250015722383\n",
            "Step 15405, train_loss 0.45507687151863346, test_loss 0.44242448046000676\n",
            "Step 15406, train_loss 0.45507615532059087, test_loss 0.4424239594543686\n",
            "Step 15407, train_loss 0.45507543920469334, test_loss 0.44242343855529975\n",
            "Step 15408, train_loss 0.45507472317092257, test_loss 0.44242291776277576\n",
            "Step 15409, train_loss 0.45507400721926, test_loss 0.4424223970767723\n",
            "Step 15410, train_loss 0.4550732913496873, test_loss 0.4424218764972653\n",
            "Step 15411, train_loss 0.455072575562186, test_loss 0.4424213560242305\n",
            "Step 15412, train_loss 0.4550718598567375, test_loss 0.4424208356576437\n",
            "Step 15413, train_loss 0.45507114423332345, test_loss 0.44242031539748067\n",
            "Step 15414, train_loss 0.4550704286919254, test_loss 0.44241979524371733\n",
            "Step 15415, train_loss 0.45506971323252493, test_loss 0.4424192751963293\n",
            "Step 15416, train_loss 0.4550689978551037, test_loss 0.44241875525529223\n",
            "Step 15417, train_loss 0.4550682825596434, test_loss 0.44241823542058245\n",
            "Step 15418, train_loss 0.45506756734612525, test_loss 0.44241771569217553\n",
            "Step 15419, train_loss 0.4550668522145311, test_loss 0.4424171960700471\n",
            "Step 15420, train_loss 0.45506613716484257, test_loss 0.4424166765541731\n",
            "Step 15421, train_loss 0.4550654221970412, test_loss 0.44241615714452953\n",
            "Step 15422, train_loss 0.4550647073111086, test_loss 0.4424156378410923\n",
            "Step 15423, train_loss 0.45506399250702656, test_loss 0.4424151186438371\n",
            "Step 15424, train_loss 0.4550632777847764, test_loss 0.4424145995527397\n",
            "Step 15425, train_loss 0.45506256314434007, test_loss 0.44241408056777615\n",
            "Step 15426, train_loss 0.45506184858569887, test_loss 0.4424135616889224\n",
            "Step 15427, train_loss 0.4550611341088348, test_loss 0.442413042916154\n",
            "Step 15428, train_loss 0.4550604197137293, test_loss 0.4424125242494472\n",
            "Step 15429, train_loss 0.45505970540036417, test_loss 0.44241200568877787\n",
            "Step 15430, train_loss 0.455058991168721, test_loss 0.44241148723412177\n",
            "Step 15431, train_loss 0.4550582770187815, test_loss 0.44241096888545484\n",
            "Step 15432, train_loss 0.45505756295052713, test_loss 0.4424104506427531\n",
            "Step 15433, train_loss 0.4550568489639399, test_loss 0.4424099325059924\n",
            "Step 15434, train_loss 0.45505613505900133, test_loss 0.44240941447514864\n",
            "Step 15435, train_loss 0.45505542123569304, test_loss 0.44240889655019794\n",
            "Step 15436, train_loss 0.45505470749399685, test_loss 0.442408378731116\n",
            "Step 15437, train_loss 0.4550539938338945, test_loss 0.44240786101787893\n",
            "Step 15438, train_loss 0.4550532802553677, test_loss 0.4424073434104628\n",
            "Step 15439, train_loss 0.455052566758398, test_loss 0.44240682590884334\n",
            "Step 15440, train_loss 0.4550518533429673, test_loss 0.4424063085129966\n",
            "Step 15441, train_loss 0.4550511400090573, test_loss 0.4424057912228988\n",
            "Step 15442, train_loss 0.4550504267566497, test_loss 0.4424052740385257\n",
            "Step 15443, train_loss 0.4550497135857262, test_loss 0.44240475695985343\n",
            "Step 15444, train_loss 0.4550490004962687, test_loss 0.4424042399868578\n",
            "Step 15445, train_loss 0.45504828748825876, test_loss 0.44240372311951504\n",
            "Step 15446, train_loss 0.4550475745616783, test_loss 0.4424032063578011\n",
            "Step 15447, train_loss 0.45504686171650893, test_loss 0.44240268970169194\n",
            "Step 15448, train_loss 0.4550461489527327, test_loss 0.44240217315116365\n",
            "Step 15449, train_loss 0.4550454362703311, test_loss 0.4424016567061923\n",
            "Step 15450, train_loss 0.45504472366928617, test_loss 0.442401140366754\n",
            "Step 15451, train_loss 0.4550440111495794, test_loss 0.4424006241328246\n",
            "Step 15452, train_loss 0.4550432987111928, test_loss 0.4424001080043803\n",
            "Step 15453, train_loss 0.45504258635410816, test_loss 0.4423995919813973\n",
            "Step 15454, train_loss 0.45504187407830715, test_loss 0.4423990760638514\n",
            "Step 15455, train_loss 0.45504116188377175, test_loss 0.44239856025171903\n",
            "Step 15456, train_loss 0.45504044977048375, test_loss 0.4423980445449759\n",
            "Step 15457, train_loss 0.4550397377384249, test_loss 0.44239752894359835\n",
            "Step 15458, train_loss 0.4550390257875771, test_loss 0.44239701344756244\n",
            "Step 15459, train_loss 0.4550383139179221, test_loss 0.4423964980568443\n",
            "Step 15460, train_loss 0.45503760212944194, test_loss 0.44239598277141995\n",
            "Step 15461, train_loss 0.45503689042211826, test_loss 0.4423954675912655\n",
            "Step 15462, train_loss 0.455036178795933, test_loss 0.44239495251635735\n",
            "Step 15463, train_loss 0.455035467250868, test_loss 0.4423944375466715\n",
            "Step 15464, train_loss 0.4550347557869053, test_loss 0.44239392268218397\n",
            "Step 15465, train_loss 0.45503404440402634, test_loss 0.442393407922871\n",
            "Step 15466, train_loss 0.45503333310221356, test_loss 0.44239289326870873\n",
            "Step 15467, train_loss 0.45503262188144844, test_loss 0.44239237871967346\n",
            "Step 15468, train_loss 0.455031910741713, test_loss 0.44239186427574134\n",
            "Step 15469, train_loss 0.45503119968298916, test_loss 0.4423913499368883\n",
            "Step 15470, train_loss 0.4550304887052588, test_loss 0.4423908357030909\n",
            "Step 15471, train_loss 0.45502977780850384, test_loss 0.44239032157432506\n",
            "Step 15472, train_loss 0.4550290669927062, test_loss 0.4423898075505671\n",
            "Step 15473, train_loss 0.4550283562578478, test_loss 0.44238929363179313\n",
            "Step 15474, train_loss 0.4550276456039106, test_loss 0.44238877981797947\n",
            "Step 15475, train_loss 0.45502693503087643, test_loss 0.4423882661091025\n",
            "Step 15476, train_loss 0.4550262245387273, test_loss 0.4423877525051381\n",
            "Step 15477, train_loss 0.45502551412744524, test_loss 0.4423872390060627\n",
            "Step 15478, train_loss 0.45502480379701205, test_loss 0.4423867256118526\n",
            "Step 15479, train_loss 0.45502409354740975, test_loss 0.4423862123224838\n",
            "Step 15480, train_loss 0.45502338337862025, test_loss 0.4423856991379329\n",
            "Step 15481, train_loss 0.4550226732906256, test_loss 0.44238518605817606\n",
            "Step 15482, train_loss 0.4550219632834078, test_loss 0.44238467308318946\n",
            "Step 15483, train_loss 0.45502125335694876, test_loss 0.4423841602129493\n",
            "Step 15484, train_loss 0.45502054351123045, test_loss 0.4423836474474322\n",
            "Step 15485, train_loss 0.45501983374623495, test_loss 0.4423831347866141\n",
            "Step 15486, train_loss 0.45501912406194417, test_loss 0.4423826222304716\n",
            "Step 15487, train_loss 0.45501841445834024, test_loss 0.44238210977898074\n",
            "Step 15488, train_loss 0.455017704935405, test_loss 0.442381597432118\n",
            "Step 15489, train_loss 0.4550169954931205, test_loss 0.4423810851898596\n",
            "Step 15490, train_loss 0.45501628613146894, test_loss 0.44238057305218204\n",
            "Step 15491, train_loss 0.4550155768504321, test_loss 0.44238006101906135\n",
            "Step 15492, train_loss 0.4550148676499922, test_loss 0.4423795490904742\n",
            "Step 15493, train_loss 0.4550141585301312, test_loss 0.4423790372663968\n",
            "Step 15494, train_loss 0.4550134494908311, test_loss 0.44237852554680557\n",
            "Step 15495, train_loss 0.45501274053207413, test_loss 0.4423780139316767\n",
            "Step 15496, train_loss 0.45501203165384213, test_loss 0.44237750242098683\n",
            "Step 15497, train_loss 0.45501132285611734, test_loss 0.442376991014712\n",
            "Step 15498, train_loss 0.4550106141388817, test_loss 0.44237647971282884\n",
            "Step 15499, train_loss 0.4550099055021173, test_loss 0.4423759685153137\n",
            "Step 15501, train_loss 0.45500848846993075, test_loss 0.44237494643329295\n",
            "Step 15502, train_loss 0.45500778007447273, test_loss 0.44237443554874034\n",
            "Step 15503, train_loss 0.4550070717594143, test_loss 0.4423739247684612\n",
            "Step 15504, train_loss 0.4550063635247376, test_loss 0.44237341409243214\n",
            "Step 15505, train_loss 0.4550056553704248, test_loss 0.4423729035206296\n",
            "Step 15506, train_loss 0.455004947296458, test_loss 0.44237239305302994\n",
            "Step 15507, train_loss 0.4550042393028192, test_loss 0.44237188268960975\n",
            "Step 15508, train_loss 0.45500353138949057, test_loss 0.4423713724303453\n",
            "Step 15509, train_loss 0.45500282355645444, test_loss 0.4423708622752133\n",
            "Step 15510, train_loss 0.45500211580369276, test_loss 0.44237035222418997\n",
            "Step 15511, train_loss 0.45500140813118756, test_loss 0.4423698422772518\n",
            "Step 15512, train_loss 0.4550007005389213, test_loss 0.4423693324343753\n",
            "Step 15513, train_loss 0.45499999302687594, test_loss 0.4423688226955372\n",
            "Step 15514, train_loss 0.4549992855950337, test_loss 0.44236831306071367\n",
            "Step 15515, train_loss 0.4549985782433766, test_loss 0.44236780352988136\n",
            "Step 15516, train_loss 0.4549978709718871, test_loss 0.44236729410301673\n",
            "Step 15517, train_loss 0.45499716378054716, test_loss 0.44236678478009633\n",
            "Step 15518, train_loss 0.4549964566693391, test_loss 0.4423662755610967\n",
            "Step 15519, train_loss 0.4549957496382449, test_loss 0.44236576644599435\n",
            "Step 15520, train_loss 0.45499504268724694, test_loss 0.44236525743476574\n",
            "Step 15521, train_loss 0.4549943358163273, test_loss 0.4423647485273875\n",
            "Step 15522, train_loss 0.4549936290254684, test_loss 0.44236423972383626\n",
            "Step 15523, train_loss 0.45499292231465227, test_loss 0.4423637310240883\n",
            "Step 15524, train_loss 0.4549922156838613, test_loss 0.4423632224281204\n",
            "Step 15525, train_loss 0.4549915091330775, test_loss 0.4423627139359092\n",
            "Step 15526, train_loss 0.45499080266228326, test_loss 0.442362205547431\n",
            "Step 15527, train_loss 0.45499009627146075, test_loss 0.44236169726266267\n",
            "Step 15528, train_loss 0.4549893899605922, test_loss 0.4423611890815807\n",
            "Step 15529, train_loss 0.4549886837296598, test_loss 0.44236068100416154\n",
            "Step 15530, train_loss 0.45498797757864595, test_loss 0.442360173030382\n",
            "Step 15531, train_loss 0.4549872715075329, test_loss 0.44235966516021863\n",
            "Step 15532, train_loss 0.45498656551630284, test_loss 0.4423591573936479\n",
            "Step 15533, train_loss 0.4549858596049381, test_loss 0.44235864973064665\n",
            "Step 15534, train_loss 0.4549851537734208, test_loss 0.44235814217119135\n",
            "Step 15535, train_loss 0.4549844480217335, test_loss 0.4423576347152589\n",
            "Step 15536, train_loss 0.45498374234985833, test_loss 0.4423571273628255\n",
            "Step 15537, train_loss 0.4549830367577775, test_loss 0.4423566201138681\n",
            "Step 15538, train_loss 0.4549823312454735, test_loss 0.4423561129683635\n",
            "Step 15539, train_loss 0.45498162581292856, test_loss 0.4423556059262881\n",
            "Step 15540, train_loss 0.45498092046012484, test_loss 0.4423550989876185\n",
            "Step 15541, train_loss 0.45498021518704496, test_loss 0.44235459215233164\n",
            "Step 15542, train_loss 0.454979509993671, test_loss 0.4423540854204041\n",
            "Step 15543, train_loss 0.45497880487998543, test_loss 0.4423535787918124\n",
            "Step 15544, train_loss 0.4549780998459705, test_loss 0.4423530722665336\n",
            "Step 15545, train_loss 0.4549773948916086, test_loss 0.4423525658445442\n",
            "Step 15546, train_loss 0.454976690016882, test_loss 0.4423520595258209\n",
            "Step 15547, train_loss 0.4549759852217732, test_loss 0.4423515533103404\n",
            "Step 15548, train_loss 0.45497528050626435, test_loss 0.4423510471980795\n",
            "Step 15549, train_loss 0.4549745758703381, test_loss 0.4423505411890148\n",
            "Step 15550, train_loss 0.45497387131397665, test_loss 0.4423500352831232\n",
            "Step 15551, train_loss 0.4549731668371624, test_loss 0.4423495294803815\n",
            "Step 15552, train_loss 0.4549724624398776, test_loss 0.4423490237807662\n",
            "Step 15553, train_loss 0.4549717581221048, test_loss 0.4423485181842542\n",
            "Step 15554, train_loss 0.4549710538838263, test_loss 0.44234801269082236\n",
            "Step 15555, train_loss 0.4549703497250246, test_loss 0.4423475073004473\n",
            "Step 15556, train_loss 0.4549696456456821, test_loss 0.44234700201310573\n",
            "Step 15557, train_loss 0.45496894164578117, test_loss 0.44234649682877486\n",
            "Step 15558, train_loss 0.45496823772530404, test_loss 0.44234599174743094\n",
            "Step 15559, train_loss 0.4549675338842335, test_loss 0.44234548676905106\n",
            "Step 15560, train_loss 0.4549668301225518, test_loss 0.4423449818936121\n",
            "Step 15561, train_loss 0.4549661264402412, test_loss 0.4423444771210906\n",
            "Step 15562, train_loss 0.4549654228372845, test_loss 0.44234397245146373\n",
            "Step 15563, train_loss 0.4549647193136638, test_loss 0.442343467884708\n",
            "Step 15564, train_loss 0.4549640158693618, test_loss 0.44234296342080043\n",
            "Step 15565, train_loss 0.4549633125043608, test_loss 0.44234245905971775\n",
            "Step 15566, train_loss 0.4549626092186434, test_loss 0.44234195480143684\n",
            "Step 15567, train_loss 0.4549619060121919, test_loss 0.4423414506459346\n",
            "Step 15568, train_loss 0.4549612028849888, test_loss 0.4423409465931878\n",
            "Step 15569, train_loss 0.4549604998370168, test_loss 0.4423404426431735\n",
            "Step 15570, train_loss 0.45495979686825816, test_loss 0.44233993879586836\n",
            "Step 15571, train_loss 0.4549590939786955, test_loss 0.4423394350512494\n",
            "Step 15572, train_loss 0.45495839116831116, test_loss 0.44233893140929353\n",
            "Step 15573, train_loss 0.4549576884370878, test_loss 0.44233842786997746\n",
            "Step 15574, train_loss 0.4549569857850078, test_loss 0.4423379244332783\n",
            "Step 15575, train_loss 0.4549562832120538, test_loss 0.44233742109917273\n",
            "Step 15576, train_loss 0.4549555807182083, test_loss 0.44233691786763785\n",
            "Step 15577, train_loss 0.45495487830345366, test_loss 0.44233641473865054\n",
            "Step 15578, train_loss 0.45495417596777254, test_loss 0.44233591171218767\n",
            "Step 15579, train_loss 0.4549534737111475, test_loss 0.44233540878822625\n",
            "Step 15580, train_loss 0.4549527715335612, test_loss 0.44233490596674324\n",
            "Step 15581, train_loss 0.4549520694349958, test_loss 0.44233440324771545\n",
            "Step 15582, train_loss 0.45495136741543424, test_loss 0.44233390063112016\n",
            "Step 15583, train_loss 0.4549506654748588, test_loss 0.44233339811693384\n",
            "Step 15584, train_loss 0.4549499636132523, test_loss 0.44233289570513373\n",
            "Step 15585, train_loss 0.4549492618305972, test_loss 0.44233239339569697\n",
            "Step 15586, train_loss 0.454948560126876, test_loss 0.4423318911886002\n",
            "Step 15587, train_loss 0.4549478585020713, test_loss 0.4423313890838206\n",
            "Step 15588, train_loss 0.4549471569561657, test_loss 0.4423308870813351\n",
            "Step 15589, train_loss 0.45494645548914203, test_loss 0.44233038518112083\n",
            "Step 15590, train_loss 0.4549457541009826, test_loss 0.44232988338315465\n",
            "Step 15591, train_loss 0.45494505279167, test_loss 0.4423293816874137\n",
            "Step 15592, train_loss 0.45494435156118707, test_loss 0.44232888009387483\n",
            "Step 15593, train_loss 0.4549436504095162, test_loss 0.4423283786025152\n",
            "Step 15594, train_loss 0.45494294933664003, test_loss 0.44232787721331185\n",
            "Step 15595, train_loss 0.4549422483425415, test_loss 0.4423273759262417\n",
            "Step 15596, train_loss 0.4549415474272028, test_loss 0.4423268747412821\n",
            "Step 15597, train_loss 0.45494084659060685, test_loss 0.44232637365840966\n",
            "Step 15598, train_loss 0.4549401458327362, test_loss 0.4423258726776019\n",
            "Step 15599, train_loss 0.4549394451535735, test_loss 0.44232537179883546\n",
            "Step 15601, train_loss 0.45493804403130267, test_loss 0.44232437034733557\n",
            "Step 15602, train_loss 0.4549373435881598, test_loss 0.44232386977455623\n",
            "Step 15603, train_loss 0.45493664322365557, test_loss 0.4423233693037268\n",
            "Step 15604, train_loss 0.45493594293777273, test_loss 0.44232286893482414\n",
            "Step 15605, train_loss 0.4549352427304938, test_loss 0.44232236866782576\n",
            "Step 15606, train_loss 0.4549345426018015, test_loss 0.4423218685027085\n",
            "Step 15607, train_loss 0.4549338425516786, test_loss 0.4423213684394495\n",
            "Step 15608, train_loss 0.4549331425801078, test_loss 0.44232086847802604\n",
            "Step 15609, train_loss 0.4549324426870718, test_loss 0.4423203686184152\n",
            "Step 15610, train_loss 0.45493174287255317, test_loss 0.442319868860594\n",
            "Step 15611, train_loss 0.45493104313653476, test_loss 0.44231936920453957\n",
            "Step 15612, train_loss 0.4549303434789993, test_loss 0.44231886965022926\n",
            "Step 15613, train_loss 0.4549296438999295, test_loss 0.4423183701976401\n",
            "Step 15614, train_loss 0.45492894439930814, test_loss 0.44231787084674923\n",
            "Step 15615, train_loss 0.4549282449771178, test_loss 0.442317371597534\n",
            "Step 15616, train_loss 0.4549275456333414, test_loss 0.4423168724499713\n",
            "Step 15617, train_loss 0.45492684636796155, test_loss 0.4423163734040386\n",
            "Step 15618, train_loss 0.45492614718096125, test_loss 0.44231587445971293\n",
            "Step 15619, train_loss 0.4549254480723228, test_loss 0.4423153756169716\n",
            "Step 15620, train_loss 0.45492474904202956, test_loss 0.4423148768757918\n",
            "Step 15621, train_loss 0.4549240500900638, test_loss 0.44231437823615055\n",
            "Step 15622, train_loss 0.4549233512164086, test_loss 0.4423138796980254\n",
            "Step 15623, train_loss 0.45492265242104657, test_loss 0.4423133812613933\n",
            "Step 15624, train_loss 0.4549219537039605, test_loss 0.4423128829262316\n",
            "Step 15625, train_loss 0.4549212550651334, test_loss 0.44231238469251766\n",
            "Step 15626, train_loss 0.45492055650454793, test_loss 0.4423118865602284\n",
            "Step 15627, train_loss 0.4549198580221869, test_loss 0.4423113885293414\n",
            "Step 15628, train_loss 0.454919159618033, test_loss 0.4423108905998337\n",
            "Step 15629, train_loss 0.45491846129206925, test_loss 0.4423103927716827\n",
            "Step 15630, train_loss 0.4549177630442783, test_loss 0.4423098950448656\n",
            "Step 15631, train_loss 0.45491706487464323, test_loss 0.4423093974193597\n",
            "Step 15632, train_loss 0.45491636678314656, test_loss 0.4423088998951424\n",
            "Step 15633, train_loss 0.45491566876977135, test_loss 0.4423084024721908\n",
            "Step 15634, train_loss 0.45491497083450033, test_loss 0.4423079051504823\n",
            "Step 15635, train_loss 0.4549142729773164, test_loss 0.4423074079299943\n",
            "Step 15636, train_loss 0.45491357519820247, test_loss 0.44230691081070395\n",
            "Step 15637, train_loss 0.45491287749714127, test_loss 0.44230641379258856\n",
            "Step 15638, train_loss 0.45491217987411586, test_loss 0.44230591687562554\n",
            "Step 15639, train_loss 0.45491148232910894, test_loss 0.4423054200597923\n",
            "Step 15640, train_loss 0.45491078486210335, test_loss 0.44230492334506594\n",
            "Step 15641, train_loss 0.45491008747308226, test_loss 0.44230442673142406\n",
            "Step 15642, train_loss 0.4549093901620283, test_loss 0.44230393021884384\n",
            "Step 15643, train_loss 0.4549086929289244, test_loss 0.44230343380730275\n",
            "Step 15644, train_loss 0.45490799577375346, test_loss 0.4423029374967781\n",
            "Step 15645, train_loss 0.45490729869649854, test_loss 0.4423024412872473\n",
            "Step 15646, train_loss 0.45490660169714237, test_loss 0.44230194517868776\n",
            "Step 15647, train_loss 0.4549059047756679, test_loss 0.4423014491710767\n",
            "Step 15648, train_loss 0.45490520793205813, test_loss 0.4423009532643916\n",
            "Step 15649, train_loss 0.45490451116629604, test_loss 0.4423004574586099\n",
            "Step 15650, train_loss 0.45490381447836425, test_loss 0.4422999617537091\n",
            "Step 15651, train_loss 0.4549031178682461, test_loss 0.4422994661496664\n",
            "Step 15652, train_loss 0.4549024213359244, test_loss 0.44229897064645934\n",
            "Step 15653, train_loss 0.45490172488138186, test_loss 0.44229847524406524\n",
            "Step 15654, train_loss 0.4549010285046018, test_loss 0.44229797994246167\n",
            "Step 15655, train_loss 0.45490033220556697, test_loss 0.4422974847416259\n",
            "Step 15656, train_loss 0.45489963598426025, test_loss 0.4422969896415356\n",
            "Step 15657, train_loss 0.454898939840665, test_loss 0.4422964946421681\n",
            "Step 15658, train_loss 0.4548982437747638, test_loss 0.44229599974350087\n",
            "Step 15659, train_loss 0.4548975477865398, test_loss 0.4422955049455113\n",
            "Step 15660, train_loss 0.45489685187597595, test_loss 0.4422950102481769\n",
            "Step 15661, train_loss 0.45489615604305533, test_loss 0.4422945156514752\n",
            "Step 15662, train_loss 0.45489546028776084, test_loss 0.4422940211553836\n",
            "Step 15663, train_loss 0.4548947646100755, test_loss 0.4422935267598797\n",
            "Step 15664, train_loss 0.4548940690099823, test_loss 0.4422930324649409\n",
            "Step 15665, train_loss 0.4548933734874644, test_loss 0.4422925382705448\n",
            "Step 15666, train_loss 0.4548926780425047, test_loss 0.44229204417666873\n",
            "Step 15667, train_loss 0.4548919826750862, test_loss 0.44229155018329036\n",
            "Step 15668, train_loss 0.45489128738519197, test_loss 0.4422910562903873\n",
            "Step 15669, train_loss 0.4548905921728052, test_loss 0.4422905624979368\n",
            "Step 15670, train_loss 0.4548898970379087, test_loss 0.44229006880591654\n",
            "Step 15671, train_loss 0.45488920198048566, test_loss 0.4422895752143042\n",
            "Step 15672, train_loss 0.4548885070005191, test_loss 0.442289081723077\n",
            "Step 15673, train_loss 0.45488781209799195, test_loss 0.44228858833221285\n",
            "Step 15674, train_loss 0.4548871172728875, test_loss 0.4422880950416891\n",
            "Step 15675, train_loss 0.4548864225251888, test_loss 0.4422876018514834\n",
            "Step 15676, train_loss 0.4548857278548787, test_loss 0.4422871087615733\n",
            "Step 15677, train_loss 0.4548850332619406, test_loss 0.4422866157719363\n",
            "Step 15678, train_loss 0.4548843387463573, test_loss 0.4422861228825502\n",
            "Step 15679, train_loss 0.454883644308112, test_loss 0.4422856300933922\n",
            "Step 15680, train_loss 0.4548829499471879, test_loss 0.4422851374044404\n",
            "Step 15681, train_loss 0.45488225566356805, test_loss 0.4422846448156721\n",
            "Step 15682, train_loss 0.45488156145723546, test_loss 0.44228415232706497\n",
            "Step 15683, train_loss 0.4548808673281733, test_loss 0.4422836599385966\n",
            "Step 15684, train_loss 0.45488017327636476, test_loss 0.44228316765024467\n",
            "Step 15685, train_loss 0.4548794793017928, test_loss 0.4422826754619868\n",
            "Step 15686, train_loss 0.45487878540444093, test_loss 0.4422821833738006\n",
            "Step 15687, train_loss 0.4548780915842918, test_loss 0.44228169138566376\n",
            "Step 15688, train_loss 0.45487739784132875, test_loss 0.44228119949755385\n",
            "Step 15689, train_loss 0.4548767041755351, test_loss 0.44228070770944866\n",
            "Step 15690, train_loss 0.45487601058689386, test_loss 0.44228021602132583\n",
            "Step 15691, train_loss 0.45487531707538814, test_loss 0.4422797244331628\n",
            "Step 15692, train_loss 0.45487462364100123, test_loss 0.4422792329449376\n",
            "Step 15693, train_loss 0.4548739302837161, test_loss 0.44227874155662766\n",
            "Step 15694, train_loss 0.45487323700351606, test_loss 0.44227825026821066\n",
            "Step 15695, train_loss 0.4548725438003844, test_loss 0.4422777590796646\n",
            "Step 15696, train_loss 0.4548718506743041, test_loss 0.4422772679909667\n",
            "Step 15697, train_loss 0.45487115762525854, test_loss 0.4422767770020953\n",
            "Step 15698, train_loss 0.4548704646532307, test_loss 0.44227628611302744\n",
            "Step 15699, train_loss 0.4548697717582039, test_loss 0.44227579532374134\n",
            "Step 15701, train_loss 0.4548683861990864, test_loss 0.44227481404442465\n",
            "Step 15702, train_loss 0.4548676935349621, test_loss 0.44227432355434954\n",
            "Step 15703, train_loss 0.4548670009477716, test_loss 0.4422738331639671\n",
            "Step 15704, train_loss 0.45486630843749837, test_loss 0.44227334287325487\n",
            "Step 15705, train_loss 0.4548656160041255, test_loss 0.4422728526821907\n",
            "Step 15706, train_loss 0.45486492364763614, test_loss 0.4422723625907524\n",
            "Step 15707, train_loss 0.4548642313680137, test_loss 0.44227187259891765\n",
            "Step 15708, train_loss 0.4548635391652413, test_loss 0.4422713827066642\n",
            "Step 15709, train_loss 0.45486284703930246, test_loss 0.4422708929139701\n",
            "Step 15710, train_loss 0.45486215499018007, test_loss 0.44227040322081274\n",
            "Step 15711, train_loss 0.4548614630178577, test_loss 0.4422699136271703\n",
            "Step 15712, train_loss 0.4548607711223185, test_loss 0.44226942413302034\n",
            "Step 15713, train_loss 0.45486007930354566, test_loss 0.44226893473834067\n",
            "Step 15714, train_loss 0.4548593875615226, test_loss 0.44226844544310934\n",
            "Step 15715, train_loss 0.45485869589623257, test_loss 0.4422679562473038\n",
            "Step 15716, train_loss 0.4548580043076589, test_loss 0.4422674671509023\n",
            "Step 15717, train_loss 0.4548573127957848, test_loss 0.4422669781538824\n",
            "Step 15718, train_loss 0.4548566213605938, test_loss 0.4422664892562221\n",
            "Step 15719, train_loss 0.4548559300020688, test_loss 0.4422660004578991\n",
            "Step 15720, train_loss 0.4548552387201935, test_loss 0.44226551175889134\n",
            "Step 15721, train_loss 0.4548545475149511, test_loss 0.44226502315917676\n",
            "Step 15722, train_loss 0.45485385638632475, test_loss 0.442264534658733\n",
            "Step 15723, train_loss 0.45485316533429815, test_loss 0.4422640462575381\n",
            "Step 15724, train_loss 0.4548524743588543, test_loss 0.44226355795556993\n",
            "Step 15725, train_loss 0.4548517834599767, test_loss 0.44226306975280655\n",
            "Step 15726, train_loss 0.4548510926376486, test_loss 0.4422625816492255\n",
            "Step 15727, train_loss 0.4548504018918535, test_loss 0.44226209364480495\n",
            "Step 15728, train_loss 0.4548497112225747, test_loss 0.44226160573952267\n",
            "Step 15729, train_loss 0.45484902062979543, test_loss 0.44226111793335654\n",
            "Step 15730, train_loss 0.4548483301134994, test_loss 0.44226063022628476\n",
            "Step 15731, train_loss 0.45484763967366965, test_loss 0.44226014261828495\n",
            "Step 15732, train_loss 0.4548469493102896, test_loss 0.4422596551093351\n",
            "Step 15733, train_loss 0.4548462590233428, test_loss 0.44225916769941326\n",
            "Step 15734, train_loss 0.4548455688128126, test_loss 0.4422586803884974\n",
            "Step 15735, train_loss 0.45484487867868223, test_loss 0.44225819317656523\n",
            "Step 15736, train_loss 0.4548441886209354, test_loss 0.442257706063595\n",
            "Step 15737, train_loss 0.4548434986395551, test_loss 0.44225721904956444\n",
            "Step 15738, train_loss 0.4548428087345251, test_loss 0.4422567321344517\n",
            "Step 15739, train_loss 0.45484211890582865, test_loss 0.44225624531823465\n",
            "Step 15740, train_loss 0.4548414291534493, test_loss 0.4422557586008913\n",
            "Step 15741, train_loss 0.45484073947737036, test_loss 0.4422552719823996\n",
            "Step 15742, train_loss 0.45484004987757537, test_loss 0.44225478546273783\n",
            "Step 15743, train_loss 0.45483936035404765, test_loss 0.4422542990418835\n",
            "Step 15744, train_loss 0.4548386709067706, test_loss 0.44225381271981495\n",
            "Step 15745, train_loss 0.45483798153572785, test_loss 0.4422533264965101\n",
            "Step 15746, train_loss 0.4548372922409028, test_loss 0.4422528403719471\n",
            "Step 15747, train_loss 0.4548366030222789, test_loss 0.44225235434610377\n",
            "Step 15748, train_loss 0.4548359138798396, test_loss 0.4422518684189583\n",
            "Step 15749, train_loss 0.4548352248135683, test_loss 0.4422513825904888\n",
            "Step 15750, train_loss 0.4548345358234486, test_loss 0.44225089686067304\n",
            "Step 15751, train_loss 0.454833846909464, test_loss 0.44225041122948916\n",
            "Step 15752, train_loss 0.45483315807159785, test_loss 0.4422499256969155\n",
            "Step 15753, train_loss 0.45483246930983384, test_loss 0.44224944026292984\n",
            "Step 15754, train_loss 0.45483178062415525, test_loss 0.4422489549275103\n",
            "Step 15755, train_loss 0.4548310920145456, test_loss 0.4422484696906349\n",
            "Step 15756, train_loss 0.4548304034809886, test_loss 0.44224798455228204\n",
            "Step 15757, train_loss 0.45482971502346764, test_loss 0.4422474995124294\n",
            "Step 15758, train_loss 0.4548290266419661, test_loss 0.4422470145710553\n",
            "Step 15759, train_loss 0.4548283383364678, test_loss 0.4422465297281379\n",
            "Step 15760, train_loss 0.454827650106956, test_loss 0.44224604498365516\n",
            "Step 15761, train_loss 0.45482696195341454, test_loss 0.4422455603375853\n",
            "Step 15762, train_loss 0.4548262738758267, test_loss 0.4422450757899063\n",
            "Step 15763, train_loss 0.4548255858741761, test_loss 0.4422445913405964\n",
            "Step 15764, train_loss 0.4548248979484462, test_loss 0.4422441069896337\n",
            "Step 15765, train_loss 0.45482421009862073, test_loss 0.4422436227369964\n",
            "Step 15766, train_loss 0.4548235223246832, test_loss 0.44224313858266273\n",
            "Step 15767, train_loss 0.4548228346266171, test_loss 0.4422426545266106\n",
            "Step 15768, train_loss 0.45482214700440615, test_loss 0.4422421705688183\n",
            "Step 15769, train_loss 0.4548214594580337, test_loss 0.44224168670926417\n",
            "Step 15770, train_loss 0.4548207719874835, test_loss 0.44224120294792607\n",
            "Step 15771, train_loss 0.4548200845927392, test_loss 0.44224071928478226\n",
            "Step 15772, train_loss 0.45481939727378423, test_loss 0.442240235719811\n",
            "Step 15773, train_loss 0.45481871003060237, test_loss 0.44223975225299056\n",
            "Step 15774, train_loss 0.45481802286317696, test_loss 0.44223926888429904\n",
            "Step 15775, train_loss 0.45481733577149186, test_loss 0.44223878561371455\n",
            "Step 15776, train_loss 0.4548166487555306, test_loss 0.4422383024412154\n",
            "Step 15777, train_loss 0.4548159618152767, test_loss 0.44223781936678\n",
            "Step 15778, train_loss 0.454815274950714, test_loss 0.4422373363903863\n",
            "Step 15779, train_loss 0.4548145881618259, test_loss 0.44223685351201253\n",
            "Step 15780, train_loss 0.4548139014485962, test_loss 0.442236370731637\n",
            "Step 15781, train_loss 0.45481321481100845, test_loss 0.44223588804923797\n",
            "Step 15782, train_loss 0.45481252824904644, test_loss 0.4422354054647938\n",
            "Step 15783, train_loss 0.45481184176269357, test_loss 0.4422349229782826\n",
            "Step 15784, train_loss 0.4548111553519337, test_loss 0.4422344405896824\n",
            "Step 15785, train_loss 0.4548104690167505, test_loss 0.442233958298972\n",
            "Step 15786, train_loss 0.4548097827571275, test_loss 0.44223347610612934\n",
            "Step 15787, train_loss 0.4548090965730486, test_loss 0.44223299401113275\n",
            "Step 15788, train_loss 0.45480841046449705, test_loss 0.4422325120139604\n",
            "Step 15789, train_loss 0.45480772443145706, test_loss 0.4422320301145909\n",
            "Step 15790, train_loss 0.45480703847391185, test_loss 0.4422315483130022\n",
            "Step 15791, train_loss 0.4548063525918455, test_loss 0.4422310666091728\n",
            "Step 15792, train_loss 0.4548056667852416, test_loss 0.44223058500308104\n",
            "Step 15793, train_loss 0.4548049810540837, test_loss 0.4422301034947051\n",
            "Step 15794, train_loss 0.4548042953983555, test_loss 0.4422296220840233\n",
            "Step 15795, train_loss 0.454803609818041, test_loss 0.4422291407710141\n",
            "Step 15796, train_loss 0.45480292431312364, test_loss 0.44222865955565577\n",
            "Step 15797, train_loss 0.4548022388835872, test_loss 0.44222817843792667\n",
            "Step 15798, train_loss 0.4548015535294156, test_loss 0.44222769741780504\n",
            "Step 15799, train_loss 0.4548008682505924, test_loss 0.4422272164952695\n",
            "Step 15801, train_loss 0.4547994979189262, test_loss 0.4422262549428694\n",
            "Step 15802, train_loss 0.45479881286605073, test_loss 0.44222577431296173\n",
            "Step 15803, train_loss 0.4547981278884587, test_loss 0.44222529378055336\n",
            "Step 15804, train_loss 0.45479744298613395, test_loss 0.44222481334562286\n",
            "Step 15805, train_loss 0.45479675815906007, test_loss 0.44222433300814845\n",
            "Step 15806, train_loss 0.4547960734072209, test_loss 0.44222385276810866\n",
            "Step 15807, train_loss 0.45479538873060027, test_loss 0.44222337262548195\n",
            "Step 15808, train_loss 0.454794704129182, test_loss 0.44222289258024655\n",
            "Step 15809, train_loss 0.45479401960294974, test_loss 0.4422224126323808\n",
            "Step 15810, train_loss 0.45479333515188747, test_loss 0.44222193278186334\n",
            "Step 15811, train_loss 0.45479265077597886, test_loss 0.4422214530286726\n",
            "Step 15812, train_loss 0.45479196647520764, test_loss 0.442220973372787\n",
            "Step 15813, train_loss 0.4547912822495577, test_loss 0.4422204938141849\n",
            "Step 15814, train_loss 0.45479059809901295, test_loss 0.4422200143528447\n",
            "Step 15815, train_loss 0.4547899140235571, test_loss 0.4422195349887449\n",
            "Step 15816, train_loss 0.45478923002317395, test_loss 0.44221905572186415\n",
            "Step 15817, train_loss 0.45478854609784747, test_loss 0.44221857655218055\n",
            "Step 15818, train_loss 0.4547878622475614, test_loss 0.44221809747967294\n",
            "Step 15819, train_loss 0.4547871784722995, test_loss 0.4422176185043196\n",
            "Step 15820, train_loss 0.4547864947720458, test_loss 0.44221713962609904\n",
            "Step 15821, train_loss 0.45478581114678407, test_loss 0.44221666084498973\n",
            "Step 15822, train_loss 0.454785127596498, test_loss 0.4422161821609702\n",
            "Step 15823, train_loss 0.45478444412117164, test_loss 0.4422157035740189\n",
            "Step 15824, train_loss 0.4547837607207888, test_loss 0.4422152250841145\n",
            "Step 15825, train_loss 0.45478307739533347, test_loss 0.44221474669123545\n",
            "Step 15826, train_loss 0.45478239414478944, test_loss 0.44221426839536\n",
            "Step 15827, train_loss 0.45478171096914044, test_loss 0.442213790196467\n",
            "Step 15828, train_loss 0.4547810278683706, test_loss 0.44221331209453496\n",
            "Step 15829, train_loss 0.4547803448424637, test_loss 0.4422128340895422\n",
            "Step 15830, train_loss 0.45477966189140356, test_loss 0.44221235618146765\n",
            "Step 15831, train_loss 0.4547789790151743, test_loss 0.44221187837028936\n",
            "Step 15832, train_loss 0.45477829621375965, test_loss 0.44221140065598635\n",
            "Step 15833, train_loss 0.4547776134871435, test_loss 0.4422109230385369\n",
            "Step 15834, train_loss 0.45477693083530984, test_loss 0.4422104455179196\n",
            "Step 15835, train_loss 0.45477624825824264, test_loss 0.44220996809411317\n",
            "Step 15836, train_loss 0.4547755657559258, test_loss 0.44220949076709615\n",
            "Step 15837, train_loss 0.4547748833283431, test_loss 0.442209013536847\n",
            "Step 15838, train_loss 0.4547742009754787, test_loss 0.4422085364033445\n",
            "Step 15839, train_loss 0.4547735186973165, test_loss 0.44220805936656715\n",
            "Step 15840, train_loss 0.4547728364938404, test_loss 0.44220758242649344\n",
            "Step 15841, train_loss 0.45477215436503426, test_loss 0.44220710558310233\n",
            "Step 15842, train_loss 0.4547714723108822, test_loss 0.442206628836372\n",
            "Step 15843, train_loss 0.45477079033136814, test_loss 0.44220615218628134\n",
            "Step 15844, train_loss 0.45477010842647597, test_loss 0.44220567563280894\n",
            "Step 15845, train_loss 0.4547694265961898, test_loss 0.44220519917593354\n",
            "Step 15846, train_loss 0.45476874484049346, test_loss 0.4422047228156335\n",
            "Step 15847, train_loss 0.45476806315937107, test_loss 0.4422042465518879\n",
            "Step 15848, train_loss 0.4547673815528066, test_loss 0.44220377038467495\n",
            "Step 15849, train_loss 0.45476670002078395, test_loss 0.4422032943139734\n",
            "Step 15850, train_loss 0.4547660185632871, test_loss 0.4422028183397623\n",
            "Step 15851, train_loss 0.4547653371803002, test_loss 0.44220234246202\n",
            "Step 15852, train_loss 0.4547646558718072, test_loss 0.4422018666807251\n",
            "Step 15853, train_loss 0.4547639746377921, test_loss 0.44220139099585654\n",
            "Step 15854, train_loss 0.454763293478239, test_loss 0.4422009154073928\n",
            "Step 15855, train_loss 0.45476261239313176, test_loss 0.44220043991531266\n",
            "Step 15856, train_loss 0.4547619313824546, test_loss 0.44219996451959487\n",
            "Step 15857, train_loss 0.4547612504461913, test_loss 0.4421994892202181\n",
            "Step 15858, train_loss 0.45476056958432615, test_loss 0.4421990140171611\n",
            "Step 15859, train_loss 0.4547598887968432, test_loss 0.44219853891040256\n",
            "Step 15860, train_loss 0.45475920808372633, test_loss 0.4421980638999213\n",
            "Step 15861, train_loss 0.4547585274449597, test_loss 0.4421975889856957\n",
            "Step 15862, train_loss 0.45475784688052734, test_loss 0.4421971141677049\n",
            "Step 15863, train_loss 0.4547571663904134, test_loss 0.44219663944592763\n",
            "Step 15864, train_loss 0.45475648597460183, test_loss 0.4421961648203424\n",
            "Step 15865, train_loss 0.45475580563307677, test_loss 0.4421956902909282\n",
            "Step 15866, train_loss 0.45475512536582224, test_loss 0.4421952158576637\n",
            "Step 15867, train_loss 0.4547544451728223, test_loss 0.44219474152052757\n",
            "Step 15868, train_loss 0.4547537650540614, test_loss 0.44219426727949873\n",
            "Step 15869, train_loss 0.4547530850095232, test_loss 0.44219379313455587\n",
            "Step 15870, train_loss 0.454752405039192, test_loss 0.44219331908567794\n",
            "Step 15871, train_loss 0.4547517251430518, test_loss 0.44219284513284346\n",
            "Step 15872, train_loss 0.4547510453210868, test_loss 0.4421923712760316\n",
            "Step 15873, train_loss 0.4547503655732812, test_loss 0.4421918975152208\n",
            "Step 15874, train_loss 0.45474968589961884, test_loss 0.4421914238503901\n",
            "Step 15875, train_loss 0.4547490063000842, test_loss 0.44219095028151845\n",
            "Step 15876, train_loss 0.4547483267746612, test_loss 0.44219047680858414\n",
            "Step 15877, train_loss 0.45474764732333406, test_loss 0.44219000343156667\n",
            "Step 15878, train_loss 0.4547469679460868, test_loss 0.44218953015044443\n",
            "Step 15879, train_loss 0.45474628864290373, test_loss 0.4421890569651963\n",
            "Step 15880, train_loss 0.45474560941376885, test_loss 0.4421885838758014\n",
            "Step 15881, train_loss 0.4547449302586666, test_loss 0.4421881108822383\n",
            "Step 15882, train_loss 0.45474425117758077, test_loss 0.4421876379844859\n",
            "Step 15883, train_loss 0.45474357217049566, test_loss 0.4421871651825233\n",
            "Step 15884, train_loss 0.45474289323739564, test_loss 0.4421866924763291\n",
            "Step 15885, train_loss 0.4547422143782647, test_loss 0.4421862198658823\n",
            "Step 15886, train_loss 0.454741535593087, test_loss 0.44218574735116184\n",
            "Step 15887, train_loss 0.4547408568818468, test_loss 0.4421852749321466\n",
            "Step 15888, train_loss 0.4547401782445283, test_loss 0.4421848026088154\n",
            "Step 15889, train_loss 0.45473949968111577, test_loss 0.44218433038114713\n",
            "Step 15890, train_loss 0.4547388211915932, test_loss 0.4421838582491207\n",
            "Step 15891, train_loss 0.45473814277594493, test_loss 0.4421833862127152\n",
            "Step 15892, train_loss 0.4547374644341553, test_loss 0.44218291427190937\n",
            "Step 15893, train_loss 0.45473678616620833, test_loss 0.44218244242668225\n",
            "Step 15894, train_loss 0.45473610797208824, test_loss 0.44218197067701276\n",
            "Step 15895, train_loss 0.4547354298517795, test_loss 0.44218149902287956\n",
            "Step 15896, train_loss 0.454734751805266, test_loss 0.442181027464262\n",
            "Step 15897, train_loss 0.4547340738325323, test_loss 0.442180556001139\n",
            "Step 15898, train_loss 0.4547333959335625, test_loss 0.44218008463348923\n",
            "Step 15899, train_loss 0.45473271810834076, test_loss 0.4421796133612919\n",
            "Step 15901, train_loss 0.45473136267907893, test_loss 0.4421786711031702\n",
            "Step 15902, train_loss 0.4547306850750072, test_loss 0.44217820011720377\n",
            "Step 15903, train_loss 0.4547300075446208, test_loss 0.44217772922660553\n",
            "Step 15904, train_loss 0.4547293300879037, test_loss 0.44217725843135464\n",
            "Step 15905, train_loss 0.45472865270484053, test_loss 0.44217678773143\n",
            "Step 15906, train_loss 0.45472797539541526, test_loss 0.4421763171268107\n",
            "Step 15907, train_loss 0.4547272981596125, test_loss 0.44217584661747567\n",
            "Step 15908, train_loss 0.45472662099741634, test_loss 0.44217537620340386\n",
            "Step 15909, train_loss 0.454725943908811, test_loss 0.44217490588457453\n",
            "Step 15910, train_loss 0.454725266893781, test_loss 0.4421744356609664\n",
            "Step 15911, train_loss 0.4547245899523106, test_loss 0.4421739655325589\n",
            "Step 15912, train_loss 0.4547239130843838, test_loss 0.4421734954993307\n",
            "Step 15913, train_loss 0.45472323628998534, test_loss 0.4421730255612611\n",
            "Step 15914, train_loss 0.45472255956909946, test_loss 0.442172555718329\n",
            "Step 15915, train_loss 0.4547218829217103, test_loss 0.4421720859705135\n",
            "Step 15916, train_loss 0.4547212063478024, test_loss 0.44217161631779356\n",
            "Step 15917, train_loss 0.45472052984736, test_loss 0.44217114676014857\n",
            "Step 15918, train_loss 0.4547198534203674, test_loss 0.4421706772975573\n",
            "Step 15919, train_loss 0.45471917706680903, test_loss 0.44217020792999895\n",
            "Step 15920, train_loss 0.4547185007866693, test_loss 0.44216973865745274\n",
            "Step 15921, train_loss 0.4547178245799324, test_loss 0.44216926947989743\n",
            "Step 15922, train_loss 0.45471714844658284, test_loss 0.44216880039731243\n",
            "Step 15923, train_loss 0.45471647238660495, test_loss 0.4421683314096767\n",
            "Step 15924, train_loss 0.45471579639998305, test_loss 0.44216786251696943\n",
            "Step 15925, train_loss 0.45471512048670165, test_loss 0.4421673937191697\n",
            "Step 15926, train_loss 0.454714444646745, test_loss 0.4421669250162567\n",
            "Step 15927, train_loss 0.4547137688800975, test_loss 0.4421664564082094\n",
            "Step 15928, train_loss 0.4547130931867437, test_loss 0.44216598789500694\n",
            "Step 15929, train_loss 0.45471241756666775, test_loss 0.4421655194766288\n",
            "Step 15930, train_loss 0.45471174201985426, test_loss 0.4421650511530538\n",
            "Step 15931, train_loss 0.45471106654628757, test_loss 0.44216458292426125\n",
            "Step 15932, train_loss 0.4547103911459521, test_loss 0.44216411479023027\n",
            "Step 15933, train_loss 0.4547097158188323, test_loss 0.44216364675093983\n",
            "Step 15934, train_loss 0.4547090405649125, test_loss 0.44216317880636946\n",
            "Step 15935, train_loss 0.4547083653841772, test_loss 0.442162710956498\n",
            "Step 15936, train_loss 0.45470769027661084, test_loss 0.4421622432013049\n",
            "Step 15937, train_loss 0.454707015242198, test_loss 0.44216177554076935\n",
            "Step 15938, train_loss 0.45470634028092277, test_loss 0.4421613079748704\n",
            "Step 15939, train_loss 0.45470566539276985, test_loss 0.44216084050358734\n",
            "Step 15940, train_loss 0.4547049905777236, test_loss 0.4421603731268992\n",
            "Step 15941, train_loss 0.45470431583576876, test_loss 0.4421599058447855\n",
            "Step 15942, train_loss 0.4547036411668893, test_loss 0.44215943865722523\n",
            "Step 15943, train_loss 0.4547029665710701, test_loss 0.44215897156419764\n",
            "Step 15944, train_loss 0.45470229204829543, test_loss 0.442158504565682\n",
            "Step 15945, train_loss 0.4547016175985499, test_loss 0.44215803766165757\n",
            "Step 15946, train_loss 0.45470094322181787, test_loss 0.44215757085210367\n",
            "Step 15947, train_loss 0.45470026891808385, test_loss 0.4421571041369994\n",
            "Step 15948, train_loss 0.45469959468733245, test_loss 0.4421566375163241\n",
            "Step 15949, train_loss 0.45469892052954797, test_loss 0.442156170990057\n",
            "Step 15950, train_loss 0.45469824644471524, test_loss 0.44215570455817743\n",
            "Step 15951, train_loss 0.4546975724328184, test_loss 0.44215523822066455\n",
            "Step 15952, train_loss 0.45469689849384204, test_loss 0.4421547719774977\n",
            "Step 15953, train_loss 0.45469622462777093, test_loss 0.4421543058286562\n",
            "Step 15954, train_loss 0.45469555083458946, test_loss 0.4421538397741193\n",
            "Step 15955, train_loss 0.454694877114282, test_loss 0.4421533738138663\n",
            "Step 15956, train_loss 0.4546942034668333, test_loss 0.4421529079478766\n",
            "Step 15957, train_loss 0.4546935298922278, test_loss 0.4421524421761293\n",
            "Step 15958, train_loss 0.45469285639044993, test_loss 0.4421519764986039\n",
            "Step 15959, train_loss 0.4546921829614845, test_loss 0.44215151091527977\n",
            "Step 15960, train_loss 0.45469150960531596, test_loss 0.442151045426136\n",
            "Step 15961, train_loss 0.45469083632192875, test_loss 0.442150580031152\n",
            "Step 15962, train_loss 0.4546901631113076, test_loss 0.4421501147303073\n",
            "Step 15963, train_loss 0.45468948997343694, test_loss 0.44214964952358105\n",
            "Step 15964, train_loss 0.4546888169083014, test_loss 0.4421491844109526\n",
            "Step 15965, train_loss 0.4546881439158857, test_loss 0.4421487193924014\n",
            "Step 15966, train_loss 0.4546874709961741, test_loss 0.44214825446790684\n",
            "Step 15967, train_loss 0.4546867981491514, test_loss 0.4421477896374483\n",
            "Step 15968, train_loss 0.45468612537480224, test_loss 0.442147324901005\n",
            "Step 15969, train_loss 0.45468545267311117, test_loss 0.44214686025855626\n",
            "Step 15970, train_loss 0.45468478004406276, test_loss 0.44214639571008174\n",
            "Step 15971, train_loss 0.4546841074876416, test_loss 0.4421459312555607\n",
            "Step 15972, train_loss 0.4546834350038323, test_loss 0.4421454668949724\n",
            "Step 15973, train_loss 0.45468276259261964, test_loss 0.4421450026282965\n",
            "Step 15974, train_loss 0.4546820902539879, test_loss 0.4421445384555122\n",
            "Step 15975, train_loss 0.4546814179879221, test_loss 0.44214407437659914\n",
            "Step 15976, train_loss 0.45468074579440665, test_loss 0.4421436103915366\n",
            "Step 15977, train_loss 0.4546800736734263, test_loss 0.44214314650030384\n",
            "Step 15978, train_loss 0.45467940162496545, test_loss 0.44214268270288054\n",
            "Step 15979, train_loss 0.45467872964900896, test_loss 0.4421422189992462\n",
            "Step 15980, train_loss 0.45467805774554143, test_loss 0.44214175538937994\n",
            "Step 15981, train_loss 0.4546773859145476, test_loss 0.4421412918732614\n",
            "Step 15982, train_loss 0.4546767141560121, test_loss 0.4421408284508701\n",
            "Step 15983, train_loss 0.45467604246991955, test_loss 0.44214036512218546\n",
            "Step 15984, train_loss 0.4546753708562546, test_loss 0.4421399018871868\n",
            "Step 15985, train_loss 0.454674699315002, test_loss 0.4421394387458539\n",
            "Step 15986, train_loss 0.4546740278461462, test_loss 0.4421389756981659\n",
            "Step 15987, train_loss 0.4546733564496722, test_loss 0.44213851274410265\n",
            "Step 15988, train_loss 0.45467268512556464, test_loss 0.44213804988364325\n",
            "Step 15989, train_loss 0.454672013873808, test_loss 0.4421375871167673\n",
            "Step 15990, train_loss 0.45467134269438725, test_loss 0.44213712444345454\n",
            "Step 15991, train_loss 0.4546706715872869, test_loss 0.44213666186368433\n",
            "Step 15992, train_loss 0.45467000055249174, test_loss 0.4421361993774361\n",
            "Step 15993, train_loss 0.4546693295899865, test_loss 0.44213573698468944\n",
            "Step 15994, train_loss 0.4546686586997558, test_loss 0.44213527468542396\n",
            "Step 15995, train_loss 0.4546679878817846, test_loss 0.442134812479619\n",
            "Step 15996, train_loss 0.4546673171360574, test_loss 0.44213435036725435\n",
            "Step 15997, train_loss 0.454666646462559, test_loss 0.4421338883483093\n",
            "Step 15998, train_loss 0.4546659758612741, test_loss 0.44213342642276354\n",
            "Step 15999, train_loss 0.45466530533218746, test_loss 0.4421329645905965\n",
            "Step 16001, train_loss 0.45466396449054824, test_loss 0.4421320412063173\n",
            "Step 16002, train_loss 0.45466329417796497, test_loss 0.4421315796541643\n",
            "Step 16003, train_loss 0.4546626239375192, test_loss 0.4421311181953082\n",
            "Step 16004, train_loss 0.4546619537691954, test_loss 0.44213065682972896\n",
            "Step 16005, train_loss 0.4546612836729785, test_loss 0.44213019555740585\n",
            "Step 16006, train_loss 0.45466061364885313, test_loss 0.4421297343783186\n",
            "Step 16007, train_loss 0.4546599436968043, test_loss 0.4421292732924469\n",
            "Step 16008, train_loss 0.45465927381681676, test_loss 0.44212881229977025\n",
            "Step 16009, train_loss 0.4546586040088751, test_loss 0.44212835140026824\n",
            "Step 16010, train_loss 0.4546579342729643, test_loss 0.44212789059392066\n",
            "Step 16011, train_loss 0.454657264609069, test_loss 0.4421274298807069\n",
            "Step 16012, train_loss 0.45465659501717426, test_loss 0.4421269692606067\n",
            "Step 16013, train_loss 0.45465592549726463, test_loss 0.44212650873359965\n",
            "Step 16014, train_loss 0.4546552560493253, test_loss 0.4421260482996655\n",
            "Step 16015, train_loss 0.45465458667334047, test_loss 0.4421255879587839\n",
            "Step 16016, train_loss 0.4546539173692955, test_loss 0.4421251277109343\n",
            "Step 16017, train_loss 0.45465324813717517, test_loss 0.4421246675560966\n",
            "Step 16018, train_loss 0.45465257897696415, test_loss 0.44212420749425035\n",
            "Step 16019, train_loss 0.45465190988864723, test_loss 0.44212374752537525\n",
            "Step 16020, train_loss 0.45465124087220943, test_loss 0.44212328764945086\n",
            "Step 16021, train_loss 0.45465057192763547, test_loss 0.442122827866457\n",
            "Step 16022, train_loss 0.45464990305491043, test_loss 0.44212236817637335\n",
            "Step 16023, train_loss 0.4546492342540189, test_loss 0.4421219085791796\n",
            "Step 16024, train_loss 0.4546485655249458, test_loss 0.4421214490748553\n",
            "Step 16025, train_loss 0.4546478968676761, test_loss 0.44212098966338037\n",
            "Step 16026, train_loss 0.4546472282821947, test_loss 0.4421205303447345\n",
            "Step 16027, train_loss 0.45464655976848634, test_loss 0.4421200711188972\n",
            "Step 16028, train_loss 0.4546458913265361, test_loss 0.44211961198584837\n",
            "Step 16029, train_loss 0.4546452229563286, test_loss 0.44211915294556775\n",
            "Step 16030, train_loss 0.45464455465784887, test_loss 0.442118693998035\n",
            "Step 16031, train_loss 0.45464388643108194, test_loss 0.4421182351432298\n",
            "Step 16032, train_loss 0.4546432182760125, test_loss 0.4421177763811321\n",
            "Step 16033, train_loss 0.4546425501926256, test_loss 0.44211731771172136\n",
            "Step 16034, train_loss 0.45464188218090607, test_loss 0.44211685913497767\n",
            "Step 16035, train_loss 0.4546412142408389, test_loss 0.4421164006508804\n",
            "Step 16036, train_loss 0.454640546372409, test_loss 0.44211594225940976\n",
            "Step 16037, train_loss 0.45463987857560123, test_loss 0.44211548396054523\n",
            "Step 16038, train_loss 0.45463921085040065, test_loss 0.44211502575426664\n",
            "Step 16039, train_loss 0.45463854319679214, test_loss 0.4421145676405539\n",
            "Step 16040, train_loss 0.4546378756147605, test_loss 0.4421141096193867\n",
            "Step 16041, train_loss 0.45463720810429087, test_loss 0.4421136516907447\n",
            "Step 16042, train_loss 0.4546365406653681, test_loss 0.442113193854608\n",
            "Step 16043, train_loss 0.4546358732979773, test_loss 0.44211273611095614\n",
            "Step 16044, train_loss 0.45463520600210305, test_loss 0.44211227845976897\n",
            "Step 16045, train_loss 0.45463453877773086, test_loss 0.4421118209010266\n",
            "Step 16046, train_loss 0.4546338716248452, test_loss 0.44211136343470847\n",
            "Step 16047, train_loss 0.4546332045434313, test_loss 0.4421109060607946\n",
            "Step 16048, train_loss 0.45463253753347416, test_loss 0.44211044877926486\n",
            "Step 16049, train_loss 0.4546318705949588, test_loss 0.442109991590099\n",
            "Step 16050, train_loss 0.45463120372786997, test_loss 0.4421095344932769\n",
            "Step 16051, train_loss 0.45463053693219285, test_loss 0.4421090774887785\n",
            "Step 16052, train_loss 0.45462987020791246, test_loss 0.44210862057658346\n",
            "Step 16053, train_loss 0.45462920355501374, test_loss 0.4421081637566719\n",
            "Step 16054, train_loss 0.45462853697348166, test_loss 0.4421077070290236\n",
            "Step 16055, train_loss 0.4546278704633013, test_loss 0.4421072503936182\n",
            "Step 16056, train_loss 0.4546272040244578, test_loss 0.4421067938504359\n",
            "Step 16057, train_loss 0.45462653765693584, test_loss 0.4421063373994565\n",
            "Step 16058, train_loss 0.45462587136072075, test_loss 0.44210588104065973\n",
            "Step 16059, train_loss 0.4546252051357974, test_loss 0.44210542477402565\n",
            "Step 16060, train_loss 0.45462453898215094, test_loss 0.4421049685995343\n",
            "Step 16061, train_loss 0.4546238728997664, test_loss 0.4421045125171652\n",
            "Step 16062, train_loss 0.45462320688862873, test_loss 0.4421040565268987\n",
            "Step 16063, train_loss 0.45462254094872306, test_loss 0.44210360062871434\n",
            "Step 16064, train_loss 0.4546218750800344, test_loss 0.4421031448225924\n",
            "Step 16065, train_loss 0.45462120928254796, test_loss 0.44210268910851247\n",
            "Step 16066, train_loss 0.45462054355624865, test_loss 0.4421022334864547\n",
            "Step 16067, train_loss 0.4546198779011215, test_loss 0.44210177795639904\n",
            "Step 16068, train_loss 0.45461921231715174, test_loss 0.44210132251832535\n",
            "Step 16069, train_loss 0.45461854680432445, test_loss 0.4421008671722136\n",
            "Step 16070, train_loss 0.4546178813626245, test_loss 0.44210041191804383\n",
            "Step 16071, train_loss 0.45461721599203725, test_loss 0.44209995675579594\n",
            "Step 16072, train_loss 0.4546165506925475, test_loss 0.44209950168544987\n",
            "Step 16073, train_loss 0.4546158854641407, test_loss 0.4420990467069857\n",
            "Step 16074, train_loss 0.4546152203068016, test_loss 0.44209859182038347\n",
            "Step 16075, train_loss 0.4546145552205157, test_loss 0.44209813702562284\n",
            "Step 16076, train_loss 0.45461389020526777, test_loss 0.44209768232268415\n",
            "Step 16077, train_loss 0.4546132252610431, test_loss 0.4420972277115473\n",
            "Step 16078, train_loss 0.4546125603878267, test_loss 0.44209677319219226\n",
            "Step 16079, train_loss 0.4546118955856037, test_loss 0.44209631876459904\n",
            "Step 16080, train_loss 0.45461123085435945, test_loss 0.44209586442874776\n",
            "Step 16081, train_loss 0.4546105661940789, test_loss 0.44209541018461823\n",
            "Step 16082, train_loss 0.45460990160474724, test_loss 0.4420949560321907\n",
            "Step 16083, train_loss 0.45460923708634954, test_loss 0.44209450197144506\n",
            "Step 16084, train_loss 0.454608572638871, test_loss 0.4420940480023614\n",
            "Step 16085, train_loss 0.45460790826229686, test_loss 0.4420935941249199\n",
            "Step 16086, train_loss 0.4546072439566122, test_loss 0.44209314033910047\n",
            "Step 16087, train_loss 0.45460657972180224, test_loss 0.44209268664488316\n",
            "Step 16088, train_loss 0.45460591555785196, test_loss 0.4420922330422479\n",
            "Step 16089, train_loss 0.45460525146474684, test_loss 0.4420917795311751\n",
            "Step 16090, train_loss 0.45460458744247184, test_loss 0.4420913261116446\n",
            "Step 16091, train_loss 0.45460392349101214, test_loss 0.44209087278363657\n",
            "Step 16092, train_loss 0.45460325961035314, test_loss 0.442090419547131\n",
            "Step 16093, train_loss 0.45460259580047974, test_loss 0.4420899664021082\n",
            "Step 16094, train_loss 0.4546019320613774, test_loss 0.4420895133485478\n",
            "Step 16095, train_loss 0.45460126839303105, test_loss 0.44208906038643053\n",
            "Step 16096, train_loss 0.45460060479542624, test_loss 0.44208860751573587\n",
            "Step 16097, train_loss 0.4545999412685479, test_loss 0.4420881547364445\n",
            "Step 16098, train_loss 0.45459927781238135, test_loss 0.4420877020485362\n",
            "Step 16099, train_loss 0.45459861442691185, test_loss 0.4420872494519912\n",
            "Step 16101, train_loss 0.4545972878680049, test_loss 0.44208634453291157\n",
            "Step 16102, train_loss 0.45459662469453777, test_loss 0.4420858922103371\n",
            "Step 16103, train_loss 0.4545959615917086, test_loss 0.4420854399790466\n",
            "Step 16104, train_loss 0.4545952985595026, test_loss 0.44208498783902006\n",
            "Step 16105, train_loss 0.4545946355979051, test_loss 0.4420845357902377\n",
            "Step 16106, train_loss 0.45459397270690116, test_loss 0.44208408383267955\n",
            "Step 16107, train_loss 0.45459330988647634, test_loss 0.44208363196632605\n",
            "Step 16108, train_loss 0.4545926471366157, test_loss 0.4420831801911571\n",
            "Step 16109, train_loss 0.4545919844573044, test_loss 0.4420827285071529\n",
            "Step 16110, train_loss 0.4545913218485281, test_loss 0.44208227691429386\n",
            "Step 16111, train_loss 0.45459065931027176, test_loss 0.4420818254125601\n",
            "Step 16112, train_loss 0.45458999684252055, test_loss 0.44208137400193154\n",
            "Step 16113, train_loss 0.4545893344452602, test_loss 0.44208092268238885\n",
            "Step 16114, train_loss 0.45458867211847553, test_loss 0.4420804714539117\n",
            "Step 16115, train_loss 0.45458800986215214, test_loss 0.44208002031648086\n",
            "Step 16116, train_loss 0.4545873476762753, test_loss 0.44207956927007624\n",
            "Step 16117, train_loss 0.4545866855608302, test_loss 0.442079118314678\n",
            "Step 16118, train_loss 0.45458602351580224, test_loss 0.4420786674502666\n",
            "Step 16119, train_loss 0.45458536154117674, test_loss 0.4420782166768221\n",
            "Step 16120, train_loss 0.4545846996369389, test_loss 0.44207776599432486\n",
            "Step 16121, train_loss 0.45458403780307416, test_loss 0.442077315402755\n",
            "Step 16122, train_loss 0.45458337603956783, test_loss 0.4420768649020929\n",
            "Step 16123, train_loss 0.4545827143464053, test_loss 0.44207641449231866\n",
            "Step 16124, train_loss 0.45458205272357183, test_loss 0.44207596417341277\n",
            "Step 16125, train_loss 0.4545813911710527, test_loss 0.4420755139453553\n",
            "Step 16126, train_loss 0.45458072968883323, test_loss 0.4420750638081267\n",
            "Step 16127, train_loss 0.45458006827689906, test_loss 0.44207461376170704\n",
            "Step 16128, train_loss 0.4545794069352353, test_loss 0.44207416380607695\n",
            "Step 16129, train_loss 0.4545787456638274, test_loss 0.44207371394121636\n",
            "Step 16130, train_loss 0.4545780844626606, test_loss 0.4420732641671055\n",
            "Step 16131, train_loss 0.4545774233317204, test_loss 0.44207281448372515\n",
            "Step 16132, train_loss 0.4545767622709923, test_loss 0.4420723648910553\n",
            "Step 16133, train_loss 0.4545761012804614, test_loss 0.44207191538907625\n",
            "Step 16134, train_loss 0.45457544036011327, test_loss 0.4420714659777685\n",
            "Step 16135, train_loss 0.45457477950993314, test_loss 0.4420710166571122\n",
            "Step 16136, train_loss 0.4545741187299066, test_loss 0.44207056742708767\n",
            "Step 16137, train_loss 0.4545734580200189, test_loss 0.44207011828767534\n",
            "Step 16138, train_loss 0.4545727973802555, test_loss 0.44206966923885566\n",
            "Step 16139, train_loss 0.4545721368106018, test_loss 0.44206922028060863\n",
            "Step 16140, train_loss 0.4545714763110433, test_loss 0.4420687714129149\n",
            "Step 16141, train_loss 0.4545708158815651, test_loss 0.44206832263575474\n",
            "Step 16142, train_loss 0.454570155522153, test_loss 0.44206787394910874\n",
            "Step 16143, train_loss 0.45456949523279233, test_loss 0.4420674253529568\n",
            "Step 16144, train_loss 0.45456883501346845, test_loss 0.44206697684727975\n",
            "Step 16145, train_loss 0.45456817486416684, test_loss 0.4420665284320577\n",
            "Step 16146, train_loss 0.45456751478487284, test_loss 0.442066080107271\n",
            "Step 16147, train_loss 0.4545668547755719, test_loss 0.44206563187290027\n",
            "Step 16148, train_loss 0.4545661948362497, test_loss 0.44206518372892584\n",
            "Step 16149, train_loss 0.4545655349668915, test_loss 0.44206473567532795\n",
            "Step 16150, train_loss 0.4545648751674827, test_loss 0.44206428771208717\n",
            "Step 16151, train_loss 0.45456421543800896, test_loss 0.4420638398391839\n",
            "Step 16152, train_loss 0.45456355577845553, test_loss 0.44206339205659845\n",
            "Step 16153, train_loss 0.4545628961888082, test_loss 0.44206294436431137\n",
            "Step 16154, train_loss 0.45456223666905204, test_loss 0.442062496762303\n",
            "Step 16155, train_loss 0.45456157721917273, test_loss 0.4420620492505539\n",
            "Step 16156, train_loss 0.45456091783915586, test_loss 0.4420616018290444\n",
            "Step 16157, train_loss 0.45456025852898674, test_loss 0.44206115449775496\n",
            "Step 16158, train_loss 0.4545595992886511, test_loss 0.44206070725666613\n",
            "Step 16159, train_loss 0.4545589401181341, test_loss 0.4420602601057583\n",
            "Step 16160, train_loss 0.4545582810174214, test_loss 0.44205981304501174\n",
            "Step 16161, train_loss 0.45455762198649863, test_loss 0.4420593660744072\n",
            "Step 16162, train_loss 0.4545569630253513, test_loss 0.4420589191939251\n",
            "Step 16163, train_loss 0.45455630413396475, test_loss 0.4420584724035459\n",
            "Step 16164, train_loss 0.4545556453123246, test_loss 0.44205802570325003\n",
            "Step 16165, train_loss 0.4545549865604164, test_loss 0.44205757909301785\n",
            "Step 16166, train_loss 0.45455432787822553, test_loss 0.44205713257283025\n",
            "Step 16167, train_loss 0.45455366926573776, test_loss 0.44205668614266747\n",
            "Step 16168, train_loss 0.4545530107229384, test_loss 0.44205623980251\n",
            "Step 16169, train_loss 0.45455235224981316, test_loss 0.44205579355233854\n",
            "Step 16170, train_loss 0.45455169384634764, test_loss 0.4420553473921333\n",
            "Step 16171, train_loss 0.45455103551252724, test_loss 0.44205490132187497\n",
            "Step 16172, train_loss 0.45455037724833763, test_loss 0.4420544553415442\n",
            "Step 16173, train_loss 0.4545497190537641, test_loss 0.4420540094511214\n",
            "Step 16174, train_loss 0.45454906092879266, test_loss 0.44205356365058707\n",
            "Step 16175, train_loss 0.4545484028734086, test_loss 0.44205311793992186\n",
            "Step 16176, train_loss 0.45454774488759747, test_loss 0.4420526723191063\n",
            "Step 16177, train_loss 0.454547086971345, test_loss 0.4420522267881209\n",
            "Step 16178, train_loss 0.4545464291246368, test_loss 0.44205178134694617\n",
            "Step 16179, train_loss 0.4545457713474582, test_loss 0.4420513359955628\n",
            "Step 16180, train_loss 0.4545451136397951, test_loss 0.4420508907339513\n",
            "Step 16181, train_loss 0.45454445600163296, test_loss 0.4420504455620923\n",
            "Step 16182, train_loss 0.4545437984329574, test_loss 0.4420500004799663\n",
            "Step 16183, train_loss 0.4545431409337541, test_loss 0.44204955548755404\n",
            "Step 16184, train_loss 0.4545424835040084, test_loss 0.44204911058483587\n",
            "Step 16185, train_loss 0.45454182614370614, test_loss 0.4420486657717925\n",
            "Step 16186, train_loss 0.454541168852833, test_loss 0.44204822104840474\n",
            "Step 16187, train_loss 0.4545405116313746, test_loss 0.44204777641465287\n",
            "Step 16188, train_loss 0.4545398544793164, test_loss 0.4420473318705178\n",
            "Step 16189, train_loss 0.4545391973966441, test_loss 0.4420468874159799\n",
            "Step 16190, train_loss 0.45453854038334346, test_loss 0.44204644305102014\n",
            "Step 16191, train_loss 0.45453788343940005, test_loss 0.4420459987756186\n",
            "Step 16192, train_loss 0.4545372265647994, test_loss 0.4420455545897565\n",
            "Step 16193, train_loss 0.45453656975952744, test_loss 0.4420451104934142\n",
            "Step 16194, train_loss 0.4545359130235695, test_loss 0.4420446664865722\n",
            "Step 16195, train_loss 0.45453525635691144, test_loss 0.44204422256921144\n",
            "Step 16196, train_loss 0.45453459975953897, test_loss 0.4420437787413125\n",
            "Step 16197, train_loss 0.45453394323143764, test_loss 0.4420433350028561\n",
            "Step 16198, train_loss 0.4545332867725932, test_loss 0.44204289135382274\n",
            "Step 16199, train_loss 0.4545326303829913, test_loss 0.4420424477941932\n",
            "Step 16201, train_loss 0.4545313178114579, test_loss 0.4420415609430683\n",
            "Step 16202, train_loss 0.45453066162949773, test_loss 0.44204111765153437\n",
            "Step 16203, train_loss 0.45453000551672296, test_loss 0.4420406744493269\n",
            "Step 16204, train_loss 0.45452934947311907, test_loss 0.4420402313364267\n",
            "Step 16205, train_loss 0.45452869349867214, test_loss 0.4420397883128147\n",
            "Step 16206, train_loss 0.45452803759336746, test_loss 0.44203934537847117\n",
            "Step 16207, train_loss 0.454527381757191, test_loss 0.4420389025333772\n",
            "Step 16208, train_loss 0.4545267259901284, test_loss 0.4420384597775132\n",
            "Step 16209, train_loss 0.45452607029216546, test_loss 0.44203801711086027\n",
            "Step 16210, train_loss 0.45452541466328783, test_loss 0.44203757453339876\n",
            "Step 16211, train_loss 0.4545247591034813, test_loss 0.4420371320451098\n",
            "Step 16212, train_loss 0.4545241036127315, test_loss 0.4420366896459739\n",
            "Step 16213, train_loss 0.45452344819102436, test_loss 0.4420362473359718\n",
            "Step 16214, train_loss 0.4545227928383454, test_loss 0.4420358051150843\n",
            "Step 16215, train_loss 0.45452213755468057, test_loss 0.44203536298329216\n",
            "Step 16216, train_loss 0.4545214823400157, test_loss 0.4420349209405762\n",
            "Step 16217, train_loss 0.4545208271943363, test_loss 0.4420344789869171\n",
            "Step 16218, train_loss 0.45452017211762824, test_loss 0.4420340371222959\n",
            "Step 16219, train_loss 0.45451951710987737, test_loss 0.44203359534669295\n",
            "Step 16220, train_loss 0.4545188621710693, test_loss 0.44203315366008933\n",
            "Step 16221, train_loss 0.45451820730119, test_loss 0.44203271206246586\n",
            "Step 16222, train_loss 0.4545175525002251, test_loss 0.4420322705538031\n",
            "Step 16223, train_loss 0.45451689776816045, test_loss 0.44203182913408207\n",
            "Step 16224, train_loss 0.45451624310498184, test_loss 0.44203138780328366\n",
            "Step 16225, train_loss 0.4545155885106751, test_loss 0.44203094656138836\n",
            "Step 16226, train_loss 0.4545149339852261, test_loss 0.4420305054083773\n",
            "Step 16227, train_loss 0.4545142795286204, test_loss 0.4420300643442312\n",
            "Step 16228, train_loss 0.4545136251408439, test_loss 0.44202962336893076\n",
            "Step 16229, train_loss 0.4545129708218827, test_loss 0.44202918248245704\n",
            "Step 16230, train_loss 0.4545123165717223, test_loss 0.4420287416847908\n",
            "Step 16231, train_loss 0.4545116623903487, test_loss 0.44202830097591295\n",
            "Step 16232, train_loss 0.4545110082777476, test_loss 0.44202786035580416\n",
            "Step 16233, train_loss 0.4545103542339048, test_loss 0.4420274198244454\n",
            "Step 16234, train_loss 0.45450970025880627, test_loss 0.44202697938181756\n",
            "Step 16235, train_loss 0.4545090463524379, test_loss 0.44202653902790146\n",
            "Step 16236, train_loss 0.4545083925147854, test_loss 0.442026098762678\n",
            "Step 16237, train_loss 0.45450773874583467, test_loss 0.44202565858612813\n",
            "Step 16238, train_loss 0.45450708504557163, test_loss 0.44202521849823256\n",
            "Step 16239, train_loss 0.454506431413982, test_loss 0.44202477849897237\n",
            "Step 16240, train_loss 0.45450577785105173, test_loss 0.44202433858832846\n",
            "Step 16241, train_loss 0.4545051243567667, test_loss 0.4420238987662815\n",
            "Step 16242, train_loss 0.4545044709311128, test_loss 0.4420234590328126\n",
            "Step 16243, train_loss 0.4545038175740758, test_loss 0.4420230193879028\n",
            "Step 16244, train_loss 0.45450316428564175, test_loss 0.44202257983153265\n",
            "Step 16245, train_loss 0.45450251106579637, test_loss 0.44202214036368326\n",
            "Step 16246, train_loss 0.45450185791452563, test_loss 0.4420217009843356\n",
            "Step 16247, train_loss 0.4545012048318155, test_loss 0.4420212616934706\n",
            "Step 16248, train_loss 0.4545005518176518, test_loss 0.4420208224910691\n",
            "Step 16249, train_loss 0.45449989887202036, test_loss 0.4420203833771122\n",
            "Step 16250, train_loss 0.45449924599490726, test_loss 0.4420199443515807\n",
            "Step 16251, train_loss 0.45449859318629826, test_loss 0.44201950541445567\n",
            "Step 16252, train_loss 0.4544979404461793, test_loss 0.44201906656571804\n",
            "Step 16253, train_loss 0.4544972877745365, test_loss 0.44201862780534873\n",
            "Step 16254, train_loss 0.4544966351713555, test_loss 0.4420181891333287\n",
            "Step 16255, train_loss 0.45449598263662244, test_loss 0.4420177505496391\n",
            "Step 16256, train_loss 0.45449533017032306, test_loss 0.4420173120542607\n",
            "Step 16257, train_loss 0.4544946777724435, test_loss 0.44201687364717457\n",
            "Step 16258, train_loss 0.4544940254429697, test_loss 0.44201643532836177\n",
            "Step 16259, train_loss 0.4544933731818874, test_loss 0.4420159970978032\n",
            "Step 16260, train_loss 0.4544927209891828, test_loss 0.44201555895547984\n",
            "Step 16261, train_loss 0.45449206886484167, test_loss 0.44201512090137274\n",
            "Step 16262, train_loss 0.45449141680885, test_loss 0.442014682935463\n",
            "Step 16263, train_loss 0.45449076482119394, test_loss 0.4420142450577316\n",
            "Step 16264, train_loss 0.45449011290185926, test_loss 0.44201380726815953\n",
            "Step 16265, train_loss 0.454489461050832, test_loss 0.4420133695667278\n",
            "Step 16266, train_loss 0.4544888092680981, test_loss 0.4420129319534174\n",
            "Step 16267, train_loss 0.4544881575536437, test_loss 0.4420124944282096\n",
            "Step 16268, train_loss 0.45448750590745457, test_loss 0.4420120569910852\n",
            "Step 16269, train_loss 0.45448685432951685, test_loss 0.4420116196420253\n",
            "Step 16270, train_loss 0.45448620281981655, test_loss 0.4420111823810111\n",
            "Step 16271, train_loss 0.45448555137833957, test_loss 0.4420107452080236\n",
            "Step 16272, train_loss 0.45448490000507197, test_loss 0.44201030812304376\n",
            "Step 16273, train_loss 0.4544842486999997, test_loss 0.4420098711260526\n",
            "Step 16274, train_loss 0.4544835974631089, test_loss 0.4420094342170314\n",
            "Step 16275, train_loss 0.45448294629438546, test_loss 0.4420089973959613\n",
            "Step 16276, train_loss 0.45448229519381556, test_loss 0.44200856066282307\n",
            "Step 16277, train_loss 0.4544816441613851, test_loss 0.4420081240175981\n",
            "Step 16278, train_loss 0.45448099319708013, test_loss 0.4420076874602673\n",
            "Step 16279, train_loss 0.45448034230088674, test_loss 0.4420072509908119\n",
            "Step 16280, train_loss 0.4544796914727909, test_loss 0.442006814609213\n",
            "Step 16281, train_loss 0.45447904071277867, test_loss 0.44200637831545153\n",
            "Step 16282, train_loss 0.4544783900208361, test_loss 0.44200594210950894\n",
            "Step 16283, train_loss 0.45447773939694935, test_loss 0.442005505991366\n",
            "Step 16284, train_loss 0.45447708884110444, test_loss 0.44200506996100414\n",
            "Step 16285, train_loss 0.4544764383532873, test_loss 0.4420046340184043\n",
            "Step 16286, train_loss 0.4544757879334842, test_loss 0.4420041981635478\n",
            "Step 16287, train_loss 0.4544751375816809, test_loss 0.44200376239641553\n",
            "Step 16288, train_loss 0.4544744872978638, test_loss 0.4420033267169889\n",
            "Step 16289, train_loss 0.4544738370820189, test_loss 0.4420028911252488\n",
            "Step 16290, train_loss 0.4544731869341322, test_loss 0.44200245562117674\n",
            "Step 16291, train_loss 0.45447253685418976, test_loss 0.44200202020475365\n",
            "Step 16292, train_loss 0.4544718868421778, test_loss 0.44200158487596064\n",
            "Step 16293, train_loss 0.45447123689808244, test_loss 0.44200114963477904\n",
            "Step 16294, train_loss 0.45447058702188964, test_loss 0.4420007144811901\n",
            "Step 16295, train_loss 0.4544699372135857, test_loss 0.4420002794151748\n",
            "Step 16296, train_loss 0.4544692874731563, test_loss 0.4419998444367144\n",
            "Step 16297, train_loss 0.45446863780058816, test_loss 0.4419994095457902\n",
            "Step 16298, train_loss 0.454467988195867, test_loss 0.44199897474238337\n",
            "Step 16299, train_loss 0.45446733865897904, test_loss 0.441998540026475\n",
            "Step 16301, train_loss 0.45446603978864736, test_loss 0.44199767085707886\n",
            "Step 16302, train_loss 0.4544653904551758, test_loss 0.4419972364035535\n",
            "Step 16303, train_loss 0.4544647411894821, test_loss 0.44199680203745156\n",
            "Step 16304, train_loss 0.4544640919915521, test_loss 0.44199636775875417\n",
            "Step 16305, train_loss 0.45446344286137225, test_loss 0.4419959335674429\n",
            "Step 16306, train_loss 0.45446279379892873, test_loss 0.4419954994634987\n",
            "Step 16307, train_loss 0.45446214480420744, test_loss 0.441995065446903\n",
            "Step 16308, train_loss 0.4544614958771947, test_loss 0.44199463151763685\n",
            "Step 16309, train_loss 0.4544608470178766, test_loss 0.4419941976756817\n",
            "Step 16310, train_loss 0.45446019822623945, test_loss 0.44199376392101875\n",
            "Step 16311, train_loss 0.4544595495022693, test_loss 0.4419933302536293\n",
            "Step 16312, train_loss 0.45445890084595236, test_loss 0.44199289667349445\n",
            "Step 16313, train_loss 0.45445825225727493, test_loss 0.44199246318059576\n",
            "Step 16314, train_loss 0.45445760373622296, test_loss 0.44199202977491436\n",
            "Step 16315, train_loss 0.45445695528278285, test_loss 0.44199159645643155\n",
            "Step 16316, train_loss 0.4544563068969407, test_loss 0.44199116322512877\n",
            "Step 16317, train_loss 0.45445565857868275, test_loss 0.44199073008098705\n",
            "Step 16318, train_loss 0.45445501032799523, test_loss 0.441990297023988\n",
            "Step 16319, train_loss 0.4544543621448643, test_loss 0.4419898640541127\n",
            "Step 16320, train_loss 0.45445371402927626, test_loss 0.4419894311713425\n",
            "Step 16321, train_loss 0.4544530659812172, test_loss 0.4419889983756589\n",
            "Step 16322, train_loss 0.4544524180006735, test_loss 0.44198856566704314\n",
            "Step 16323, train_loss 0.4544517700876311, test_loss 0.44198813304547635\n",
            "Step 16324, train_loss 0.4544511222420766, test_loss 0.4419877005109402\n",
            "Step 16325, train_loss 0.45445047446399606, test_loss 0.44198726806341593\n",
            "Step 16326, train_loss 0.45444982675337575, test_loss 0.4419868357028848\n",
            "Step 16327, train_loss 0.4544491791102018, test_loss 0.44198640342932805\n",
            "Step 16328, train_loss 0.4544485315344606, test_loss 0.44198597124272737\n",
            "Step 16329, train_loss 0.4544478840261384, test_loss 0.4419855391430639\n",
            "Step 16330, train_loss 0.4544472365852213, test_loss 0.4419851071303191\n",
            "Step 16331, train_loss 0.45444658921169584, test_loss 0.44198467520447426\n",
            "Step 16332, train_loss 0.45444594190554805, test_loss 0.4419842433655109\n",
            "Step 16333, train_loss 0.45444529466676437, test_loss 0.44198381161341027\n",
            "Step 16334, train_loss 0.4544446474953309, test_loss 0.44198337994815384\n",
            "Step 16335, train_loss 0.454444000391234, test_loss 0.44198294836972296\n",
            "Step 16336, train_loss 0.45444335335445996, test_loss 0.4419825168780991\n",
            "Step 16337, train_loss 0.4544427063849952, test_loss 0.44198208547326356\n",
            "Step 16338, train_loss 0.45444205948282573, test_loss 0.4419816541551978\n",
            "Step 16339, train_loss 0.4544414126479381, test_loss 0.44198122292388337\n",
            "Step 16340, train_loss 0.4544407658803184, test_loss 0.44198079177930155\n",
            "Step 16341, train_loss 0.4544401191799531, test_loss 0.4419803607214337\n",
            "Step 16342, train_loss 0.4544394725468285, test_loss 0.44197992975026146\n",
            "Step 16343, train_loss 0.4544388259809309, test_loss 0.4419794988657661\n",
            "Step 16344, train_loss 0.4544381794822465, test_loss 0.4419790680679291\n",
            "Step 16345, train_loss 0.4544375330507618, test_loss 0.4419786373567318\n",
            "Step 16346, train_loss 0.454436886686463, test_loss 0.44197820673215615\n",
            "Step 16347, train_loss 0.4544362403893365, test_loss 0.441977776194183\n",
            "Step 16348, train_loss 0.45443559415936857, test_loss 0.4419773457427941\n",
            "Step 16349, train_loss 0.45443494799654566, test_loss 0.44197691537797085\n",
            "Step 16350, train_loss 0.454434301900854, test_loss 0.4419764850996947\n",
            "Step 16351, train_loss 0.45443365587227996, test_loss 0.44197605490794734\n",
            "Step 16352, train_loss 0.45443300991081, test_loss 0.4419756248027101\n",
            "Step 16353, train_loss 0.4544323640164303, test_loss 0.4419751947839643\n",
            "Step 16354, train_loss 0.4544317181891273, test_loss 0.44197476485169174\n",
            "Step 16355, train_loss 0.45443107242888753, test_loss 0.44197433500587374\n",
            "Step 16356, train_loss 0.4544304267356971, test_loss 0.4419739052464918\n",
            "Step 16357, train_loss 0.4544297811095425, test_loss 0.4419734755735276\n",
            "Step 16358, train_loss 0.45442913555041015, test_loss 0.4419730459869624\n",
            "Step 16359, train_loss 0.45442849005828634, test_loss 0.441972616486778\n",
            "Step 16360, train_loss 0.45442784463315744, test_loss 0.4419721870729557\n",
            "Step 16361, train_loss 0.45442719927500996, test_loss 0.4419717577454771\n",
            "Step 16362, train_loss 0.4544265539838303, test_loss 0.44197132850432375\n",
            "Step 16363, train_loss 0.4544259087596048, test_loss 0.4419708993494773\n",
            "Step 16364, train_loss 0.4544252636023198, test_loss 0.44197047028091907\n",
            "Step 16365, train_loss 0.4544246185119618, test_loss 0.4419700412986308\n",
            "Step 16366, train_loss 0.4544239734885172, test_loss 0.4419696124025939\n",
            "Step 16367, train_loss 0.45442332853197237, test_loss 0.44196918359279014\n",
            "Step 16368, train_loss 0.45442268364231375, test_loss 0.44196875486920084\n",
            "Step 16369, train_loss 0.4544220388195279, test_loss 0.44196832623180776\n",
            "Step 16370, train_loss 0.45442139406360094, test_loss 0.4419678976805924\n",
            "Step 16371, train_loss 0.4544207493745197, test_loss 0.4419674692155363\n",
            "Step 16372, train_loss 0.45442010475227024, test_loss 0.4419670408366212\n",
            "Step 16373, train_loss 0.45441946019683926, test_loss 0.4419666125438284\n",
            "Step 16374, train_loss 0.45441881570821313, test_loss 0.44196618433713997\n",
            "Step 16375, train_loss 0.4544181712863782, test_loss 0.44196575621653705\n",
            "Step 16376, train_loss 0.45441752693132104, test_loss 0.4419653281820014\n",
            "Step 16377, train_loss 0.4544168826430281, test_loss 0.4419649002335148\n",
            "Step 16378, train_loss 0.4544162384214858, test_loss 0.4419644723710586\n",
            "Step 16379, train_loss 0.4544155942666807, test_loss 0.4419640445946145\n",
            "Step 16380, train_loss 0.4544149501785991, test_loss 0.44196361690416447\n",
            "Step 16381, train_loss 0.45441430615722767, test_loss 0.44196318929968964\n",
            "Step 16382, train_loss 0.4544136622025528, test_loss 0.4419627617811719\n",
            "Step 16383, train_loss 0.45441301831456093, test_loss 0.44196233434859294\n",
            "Step 16384, train_loss 0.4544123744932387, test_loss 0.4419619070019343\n",
            "Step 16385, train_loss 0.4544117307385723, test_loss 0.44196147974117755\n",
            "Step 16386, train_loss 0.45441108705054856, test_loss 0.44196105256630464\n",
            "Step 16387, train_loss 0.4544104434291539, test_loss 0.44196062547729686\n",
            "Step 16388, train_loss 0.4544097998743746, test_loss 0.44196019847413626\n",
            "Step 16389, train_loss 0.4544091563861975, test_loss 0.44195977155680416\n",
            "Step 16390, train_loss 0.4544085129646088, test_loss 0.4419593447252825\n",
            "Step 16391, train_loss 0.4544078696095953, test_loss 0.44195891797955295\n",
            "Step 16392, train_loss 0.4544072263211434, test_loss 0.4419584913195969\n",
            "Step 16393, train_loss 0.45440658309923954, test_loss 0.4419580647453965\n",
            "Step 16394, train_loss 0.4544059399438704, test_loss 0.44195763825693307\n",
            "Step 16395, train_loss 0.4544052968550224, test_loss 0.4419572118541885\n",
            "Step 16396, train_loss 0.45440465383268214, test_loss 0.4419567855371445\n",
            "Step 16397, train_loss 0.45440401087683624, test_loss 0.44195635930578275\n",
            "Step 16398, train_loss 0.45440336798747105, test_loss 0.441955933160085\n",
            "Step 16399, train_loss 0.45440272516457325, test_loss 0.44195550710003295\n",
            "Step 16401, train_loss 0.45440143971812597, test_loss 0.44195465523679267\n",
            "Step 16402, train_loss 0.45440079709454967, test_loss 0.4419542294335681\n",
            "Step 16403, train_loss 0.45440015453738697, test_loss 0.44195380371591614\n",
            "Step 16404, train_loss 0.4543995120466243, test_loss 0.4419533780838185\n",
            "Step 16405, train_loss 0.4543988696222487, test_loss 0.4419529525372572\n",
            "Step 16406, train_loss 0.4543982272642463, test_loss 0.44195252707621363\n",
            "Step 16407, train_loss 0.4543975849726038, test_loss 0.4419521017006698\n",
            "Step 16408, train_loss 0.4543969427473079, test_loss 0.44195167641060745\n",
            "Step 16409, train_loss 0.4543963005883449, test_loss 0.44195125120600826\n",
            "Step 16410, train_loss 0.45439565849570185, test_loss 0.44195082608685404\n",
            "Step 16411, train_loss 0.45439501646936503, test_loss 0.4419504010531268\n",
            "Step 16412, train_loss 0.45439437450932113, test_loss 0.4419499761048081\n",
            "Step 16413, train_loss 0.4543937326155568, test_loss 0.44194955124187957\n",
            "Step 16414, train_loss 0.4543930907880586, test_loss 0.44194912646432327\n",
            "Step 16415, train_loss 0.45439244902681325, test_loss 0.44194870177212114\n",
            "Step 16416, train_loss 0.4543918073318071, test_loss 0.4419482771652547\n",
            "Step 16417, train_loss 0.4543911657030271, test_loss 0.4419478526437059\n",
            "Step 16418, train_loss 0.4543905241404597, test_loss 0.4419474282074564\n",
            "Step 16419, train_loss 0.4543898826440917, test_loss 0.44194700385648833\n",
            "Step 16420, train_loss 0.4543892412139094, test_loss 0.4419465795907833\n",
            "Step 16421, train_loss 0.4543885998498999, test_loss 0.44194615541032306\n",
            "Step 16422, train_loss 0.4543879585520494, test_loss 0.44194573131508974\n",
            "Step 16423, train_loss 0.45438731732034493, test_loss 0.441945307305065\n",
            "Step 16424, train_loss 0.4543866761547729, test_loss 0.4419448833802308\n",
            "Step 16425, train_loss 0.4543860350553201, test_loss 0.4419444595405689\n",
            "Step 16426, train_loss 0.4543853940219731, test_loss 0.441944035786061\n",
            "Step 16427, train_loss 0.45438475305471865, test_loss 0.4419436121166893\n",
            "Step 16428, train_loss 0.4543841121535434, test_loss 0.4419431885324356\n",
            "Step 16429, train_loss 0.454383471318434, test_loss 0.4419427650332816\n",
            "Step 16430, train_loss 0.45438283054937706, test_loss 0.44194234161920926\n",
            "Step 16431, train_loss 0.4543821898463595, test_loss 0.4419419182902005\n",
            "Step 16432, train_loss 0.4543815492093677, test_loss 0.4419414950462372\n",
            "Step 16433, train_loss 0.4543809086383887, test_loss 0.4419410718873013\n",
            "Step 16434, train_loss 0.4543802681334089, test_loss 0.44194064881337464\n",
            "Step 16435, train_loss 0.45437962769441503, test_loss 0.44194022582443915\n",
            "Step 16436, train_loss 0.4543789873213941, test_loss 0.4419398029204767\n",
            "Step 16437, train_loss 0.4543783470143324, test_loss 0.4419393801014692\n",
            "Step 16438, train_loss 0.45437770677321687, test_loss 0.4419389573673987\n",
            "Step 16439, train_loss 0.4543770665980342, test_loss 0.44193853471824696\n",
            "Step 16440, train_loss 0.45437642648877125, test_loss 0.44193811215399614\n",
            "Step 16441, train_loss 0.45437578644541443, test_loss 0.4419376896746279\n",
            "Step 16442, train_loss 0.4543751464679507, test_loss 0.4419372672801243\n",
            "Step 16443, train_loss 0.45437450655636685, test_loss 0.4419368449704673\n",
            "Step 16444, train_loss 0.45437386671064933, test_loss 0.4419364227456389\n",
            "Step 16445, train_loss 0.4543732269307852, test_loss 0.441936000605621\n",
            "Step 16446, train_loss 0.4543725872167611, test_loss 0.4419355785503955\n",
            "Step 16447, train_loss 0.4543719475685638, test_loss 0.44193515657994453\n",
            "Step 16448, train_loss 0.4543713079861798, test_loss 0.4419347346942499\n",
            "Step 16449, train_loss 0.4543706684695962, test_loss 0.44193431289329366\n",
            "Step 16450, train_loss 0.45437002901879964, test_loss 0.44193389117705767\n",
            "Step 16451, train_loss 0.45436938963377693, test_loss 0.4419334695455242\n",
            "Step 16452, train_loss 0.45436875031451457, test_loss 0.441933047998675\n",
            "Step 16453, train_loss 0.4543681110609997, test_loss 0.441932626536492\n",
            "Step 16454, train_loss 0.4543674718732191, test_loss 0.4419322051589575\n",
            "Step 16455, train_loss 0.45436683275115924, test_loss 0.4419317838660533\n",
            "Step 16456, train_loss 0.4543661936948071, test_loss 0.44193136265776156\n",
            "Step 16457, train_loss 0.4543655547041495, test_loss 0.44193094153406404\n",
            "Step 16458, train_loss 0.4543649157791732, test_loss 0.4419305204949431\n",
            "Step 16459, train_loss 0.4543642769198649, test_loss 0.4419300995403803\n",
            "Step 16460, train_loss 0.4543636381262116, test_loss 0.44192967867035815\n",
            "Step 16461, train_loss 0.45436299939819996, test_loss 0.44192925788485843\n",
            "Step 16462, train_loss 0.4543623607358169, test_loss 0.4419288371838633\n",
            "Step 16463, train_loss 0.45436172213904913, test_loss 0.44192841656735454\n",
            "Step 16464, train_loss 0.4543610836078835, test_loss 0.44192799603531463\n",
            "Step 16465, train_loss 0.45436044514230683, test_loss 0.4419275755877253\n",
            "Step 16466, train_loss 0.45435980674230597, test_loss 0.44192715522456866\n",
            "Step 16467, train_loss 0.45435916840786783, test_loss 0.44192673494582696\n",
            "Step 16468, train_loss 0.4543585301389791, test_loss 0.44192631475148203\n",
            "Step 16469, train_loss 0.4543578919356267, test_loss 0.44192589464151616\n",
            "Step 16470, train_loss 0.45435725379779746, test_loss 0.4419254746159112\n",
            "Step 16471, train_loss 0.4543566157254783, test_loss 0.44192505467464943\n",
            "Step 16472, train_loss 0.45435597771865593, test_loss 0.441924634817713\n",
            "Step 16473, train_loss 0.4543553397773174, test_loss 0.44192421504508356\n",
            "Step 16474, train_loss 0.45435470190144944, test_loss 0.44192379535674375\n",
            "Step 16475, train_loss 0.4543540640910389, test_loss 0.44192337575267543\n",
            "Step 16476, train_loss 0.4543534263460726, test_loss 0.44192295623286076\n",
            "Step 16477, train_loss 0.45435278866653767, test_loss 0.4419225367972817\n",
            "Step 16478, train_loss 0.4543521510524207, test_loss 0.44192211744592064\n",
            "Step 16479, train_loss 0.4543515135037088, test_loss 0.44192169817875954\n",
            "Step 16480, train_loss 0.4543508760203886, test_loss 0.44192127899578043\n",
            "Step 16481, train_loss 0.45435023860244733, test_loss 0.44192085989696567\n",
            "Step 16482, train_loss 0.4543496012498715, test_loss 0.4419204408822973\n",
            "Step 16483, train_loss 0.45434896396264834, test_loss 0.44192002195175745\n",
            "Step 16484, train_loss 0.45434832674076453, test_loss 0.44191960310532824\n",
            "Step 16485, train_loss 0.4543476895842071, test_loss 0.44191918434299193\n",
            "Step 16486, train_loss 0.454347052492963, test_loss 0.4419187656647307\n",
            "Step 16487, train_loss 0.4543464154670189, test_loss 0.4419183470705266\n",
            "Step 16488, train_loss 0.45434577850636193, test_loss 0.44191792856036166\n",
            "Step 16489, train_loss 0.45434514161097916, test_loss 0.44191751013421837\n",
            "Step 16490, train_loss 0.45434450478085714, test_loss 0.4419170917920788\n",
            "Step 16491, train_loss 0.4543438680159831, test_loss 0.441916673533925\n",
            "Step 16492, train_loss 0.45434323131634374, test_loss 0.4419162553597392\n",
            "Step 16493, train_loss 0.4543425946819262, test_loss 0.4419158372695038\n",
            "Step 16494, train_loss 0.45434195811271727, test_loss 0.4419154192632008\n",
            "Step 16495, train_loss 0.4543413216087041, test_loss 0.44191500134081246\n",
            "Step 16496, train_loss 0.45434068516987347, test_loss 0.44191458350232105\n",
            "Step 16497, train_loss 0.45434004879621237, test_loss 0.44191416574770875\n",
            "Step 16498, train_loss 0.45433941248770776, test_loss 0.44191374807695766\n",
            "Step 16499, train_loss 0.45433877624434654, test_loss 0.44191333049004994\n",
            "Step 16501, train_loss 0.4543375039530026, test_loss 0.44191249556769446\n",
            "Step 16502, train_loss 0.4543368679049937, test_loss 0.44191207823221085\n",
            "Step 16503, train_loss 0.4543362319220761, test_loss 0.44191166098049983\n",
            "Step 16504, train_loss 0.45433559600423695, test_loss 0.4419112438125434\n",
            "Step 16505, train_loss 0.4543349601514631, test_loss 0.44191082672832394\n",
            "Step 16506, train_loss 0.45433432436374144, test_loss 0.44191040972782375\n",
            "Step 16507, train_loss 0.45433368864105916, test_loss 0.44190999281102505\n",
            "Step 16508, train_loss 0.45433305298340326, test_loss 0.44190957597791\n",
            "Step 16509, train_loss 0.4543324173907606, test_loss 0.44190915922846113\n",
            "Step 16510, train_loss 0.4543317818631184, test_loss 0.44190874256266055\n",
            "Step 16511, train_loss 0.45433114640046346, test_loss 0.44190832598049046\n",
            "Step 16512, train_loss 0.45433051100278277, test_loss 0.4419079094819332\n",
            "Step 16513, train_loss 0.45432987567006355, test_loss 0.44190749306697136\n",
            "Step 16514, train_loss 0.4543292404022928, test_loss 0.4419070767355868\n",
            "Step 16515, train_loss 0.4543286051994574, test_loss 0.4419066604877621\n",
            "Step 16516, train_loss 0.4543279700615445, test_loss 0.4419062443234794\n",
            "Step 16517, train_loss 0.4543273349885411, test_loss 0.4419058282427211\n",
            "Step 16518, train_loss 0.4543266999804341, test_loss 0.44190541224546953\n",
            "Step 16519, train_loss 0.4543260650372109, test_loss 0.4419049963317069\n",
            "Step 16520, train_loss 0.45432543015885823, test_loss 0.4419045805014158\n",
            "Step 16521, train_loss 0.4543247953453632, test_loss 0.44190416475457833\n",
            "Step 16522, train_loss 0.45432416059671293, test_loss 0.4419037490911768\n",
            "Step 16523, train_loss 0.4543235259128945, test_loss 0.44190333351119354\n",
            "Step 16524, train_loss 0.4543228912938949, test_loss 0.4419029180146112\n",
            "Step 16525, train_loss 0.4543222567397013, test_loss 0.4419025026014118\n",
            "Step 16526, train_loss 0.45432162225030065, test_loss 0.44190208727157776\n",
            "Step 16527, train_loss 0.4543209878256801, test_loss 0.4419016720250916\n",
            "Step 16528, train_loss 0.45432035346582667, test_loss 0.44190125686193554\n",
            "Step 16529, train_loss 0.4543197191707275, test_loss 0.44190084178209194\n",
            "Step 16530, train_loss 0.4543190849403696, test_loss 0.4419004267855433\n",
            "Step 16531, train_loss 0.45431845077474015, test_loss 0.44190001187227185\n",
            "Step 16532, train_loss 0.4543178166738263, test_loss 0.4418995970422601\n",
            "Step 16533, train_loss 0.454317182637615, test_loss 0.4418991822954904\n",
            "Step 16534, train_loss 0.45431654866609344, test_loss 0.441898767631945\n",
            "Step 16535, train_loss 0.4543159147592487, test_loss 0.44189835305160663\n",
            "Step 16536, train_loss 0.4543152809170679, test_loss 0.4418979385544573\n",
            "Step 16537, train_loss 0.45431464713953823, test_loss 0.4418975241404797\n",
            "Step 16538, train_loss 0.4543140134266466, test_loss 0.4418971098096562\n",
            "Step 16539, train_loss 0.4543133797783805, test_loss 0.4418966955619691\n",
            "Step 16540, train_loss 0.45431274619472667, test_loss 0.4418962813974009\n",
            "Step 16541, train_loss 0.4543121126756724, test_loss 0.44189586731593394\n",
            "Step 16542, train_loss 0.45431147922120485, test_loss 0.441895453317551\n",
            "Step 16543, train_loss 0.45431084583131115, test_loss 0.441895039402234\n",
            "Step 16544, train_loss 0.45431021250597853, test_loss 0.4418946255699658\n",
            "Step 16545, train_loss 0.4543095792451939, test_loss 0.44189421182072863\n",
            "Step 16546, train_loss 0.45430894604894456, test_loss 0.4418937981545049\n",
            "Step 16547, train_loss 0.45430831291721774, test_loss 0.4418933845712772\n",
            "Step 16548, train_loss 0.45430767985000053, test_loss 0.44189297107102804\n",
            "Step 16549, train_loss 0.45430704684727996, test_loss 0.44189255765373975\n",
            "Step 16550, train_loss 0.45430641390904347, test_loss 0.44189214431939494\n",
            "Step 16551, train_loss 0.454305781035278, test_loss 0.4418917310679759\n",
            "Step 16552, train_loss 0.45430514822597085, test_loss 0.4418913178994652\n",
            "Step 16553, train_loss 0.4543045154811092, test_loss 0.44189090481384546\n",
            "Step 16554, train_loss 0.45430388280068007, test_loss 0.44189049181109896\n",
            "Step 16555, train_loss 0.4543032501846708, test_loss 0.44189007889120824\n",
            "Step 16556, train_loss 0.4543026176330687, test_loss 0.4418896660541559\n",
            "Step 16557, train_loss 0.4543019851458606, test_loss 0.44188925329992446\n",
            "Step 16558, train_loss 0.4543013527230341, test_loss 0.4418888406284963\n",
            "Step 16559, train_loss 0.4543007203645761, test_loss 0.44188842803985395\n",
            "Step 16560, train_loss 0.454300088070474, test_loss 0.44188801553398005\n",
            "Step 16561, train_loss 0.45429945584071496, test_loss 0.44188760311085695\n",
            "Step 16562, train_loss 0.454298823675286, test_loss 0.44188719077046734\n",
            "Step 16563, train_loss 0.4542981915741748, test_loss 0.44188677851279373\n",
            "Step 16564, train_loss 0.4542975595373681, test_loss 0.44188636633781864\n",
            "Step 16565, train_loss 0.4542969275648533, test_loss 0.4418859542455244\n",
            "Step 16566, train_loss 0.4542962956566178, test_loss 0.44188554223589394\n",
            "Step 16567, train_loss 0.4542956638126486, test_loss 0.4418851303089097\n",
            "Step 16568, train_loss 0.45429503203293303, test_loss 0.441884718464554\n",
            "Step 16569, train_loss 0.4542944003174583, test_loss 0.4418843067028096\n",
            "Step 16570, train_loss 0.4542937686662118, test_loss 0.44188389502365905\n",
            "Step 16571, train_loss 0.4542931370791806, test_loss 0.44188348342708494\n",
            "Step 16572, train_loss 0.45429250555635214, test_loss 0.4418830719130698\n",
            "Step 16573, train_loss 0.45429187409771343, test_loss 0.44188266048159625\n",
            "Step 16574, train_loss 0.45429124270325194, test_loss 0.4418822491326468\n",
            "Step 16575, train_loss 0.45429061137295473, test_loss 0.4418818378662041\n",
            "Step 16576, train_loss 0.4542899801068094, test_loss 0.4418814266822508\n",
            "Step 16577, train_loss 0.4542893489048029, test_loss 0.4418810155807694\n",
            "Step 16578, train_loss 0.4542887177669227, test_loss 0.44188060456174255\n",
            "Step 16579, train_loss 0.45428808669315596, test_loss 0.44188019362515285\n",
            "Step 16580, train_loss 0.45428745568349005, test_loss 0.44187978277098294\n",
            "Step 16581, train_loss 0.4542868247379122, test_loss 0.4418793719992155\n",
            "Step 16582, train_loss 0.4542861938564098, test_loss 0.4418789613098329\n",
            "Step 16583, train_loss 0.45428556303897005, test_loss 0.4418785507028182\n",
            "Step 16584, train_loss 0.4542849322855803, test_loss 0.4418781401781536\n",
            "Step 16585, train_loss 0.4542843015962279, test_loss 0.4418777297358219\n",
            "Step 16586, train_loss 0.4542836709709, test_loss 0.4418773193758058\n",
            "Step 16587, train_loss 0.45428304040958417, test_loss 0.441876909098088\n",
            "Step 16588, train_loss 0.45428240991226754, test_loss 0.44187649890265107\n",
            "Step 16589, train_loss 0.4542817794789374, test_loss 0.4418760887894776\n",
            "Step 16590, train_loss 0.4542811491095811, test_loss 0.4418756787585503\n",
            "Step 16591, train_loss 0.4542805188041862, test_loss 0.441875268809852\n",
            "Step 16592, train_loss 0.4542798885627397, test_loss 0.44187485894336515\n",
            "Step 16593, train_loss 0.45427925838522903, test_loss 0.44187444915907237\n",
            "Step 16594, train_loss 0.45427862827164167, test_loss 0.4418740394569566\n",
            "Step 16595, train_loss 0.45427799822196496, test_loss 0.4418736298370005\n",
            "Step 16596, train_loss 0.4542773682361861, test_loss 0.4418732202991867\n",
            "Step 16597, train_loss 0.4542767383142924, test_loss 0.4418728108434977\n",
            "Step 16598, train_loss 0.4542761084562715, test_loss 0.44187240146991646\n",
            "Step 16599, train_loss 0.4542754786621106, test_loss 0.44187199217842554\n",
            "Step 16601, train_loss 0.45427421926531797, test_loss 0.44187117384164565\n",
            "Step 16602, train_loss 0.4542735896626612, test_loss 0.44187076479632215\n",
            "Step 16603, train_loss 0.4542729601238139, test_loss 0.4418703558330201\n",
            "Step 16604, train_loss 0.4542723306487634, test_loss 0.44186994695172166\n",
            "Step 16605, train_loss 0.4542717012374971, test_loss 0.44186953815241004\n",
            "Step 16606, train_loss 0.4542710718900025, test_loss 0.441869129435068\n",
            "Step 16607, train_loss 0.45427044260626687, test_loss 0.44186872079967815\n",
            "Step 16608, train_loss 0.45426981338627764, test_loss 0.441868312246223\n",
            "Step 16609, train_loss 0.4542691842300222, test_loss 0.4418679037746858\n",
            "Step 16610, train_loss 0.4542685551374881, test_loss 0.4418674953850489\n",
            "Step 16611, train_loss 0.4542679261086625, test_loss 0.4418670870772955\n",
            "Step 16612, train_loss 0.45426729714353303, test_loss 0.44186667885140773\n",
            "Step 16613, train_loss 0.45426666824208695, test_loss 0.4418662707073688\n",
            "Step 16614, train_loss 0.4542660394043117, test_loss 0.4418658626451614\n",
            "Step 16615, train_loss 0.45426541063019477, test_loss 0.44186545466476834\n",
            "Step 16616, train_loss 0.4542647819197236, test_loss 0.4418650467661724\n",
            "Step 16617, train_loss 0.45426415327288555, test_loss 0.44186463894935624\n",
            "Step 16618, train_loss 0.454263524689668, test_loss 0.441864231214303\n",
            "Step 16619, train_loss 0.45426289617005855, test_loss 0.441863823560995\n",
            "Step 16620, train_loss 0.4542622677140445, test_loss 0.44186341598941553\n",
            "Step 16621, train_loss 0.4542616393216135, test_loss 0.44186300849954696\n",
            "Step 16622, train_loss 0.4542610109927527, test_loss 0.4418626010913723\n",
            "Step 16623, train_loss 0.4542603827274498, test_loss 0.4418621937648744\n",
            "Step 16624, train_loss 0.4542597545256921, test_loss 0.4418617865200361\n",
            "Step 16625, train_loss 0.4542591263874672, test_loss 0.4418613793568403\n",
            "Step 16626, train_loss 0.4542584983127624, test_loss 0.44186097227526955\n",
            "Step 16627, train_loss 0.45425787030156534, test_loss 0.4418605652753069\n",
            "Step 16628, train_loss 0.45425724235386333, test_loss 0.44186015835693515\n",
            "Step 16629, train_loss 0.45425661446964405, test_loss 0.4418597515201372\n",
            "Step 16630, train_loss 0.4542559866488948, test_loss 0.4418593447648957\n",
            "Step 16631, train_loss 0.4542553588916031, test_loss 0.4418589380911937\n",
            "Step 16632, train_loss 0.4542547311977565, test_loss 0.44185853149901416\n",
            "Step 16633, train_loss 0.4542541035673425, test_loss 0.4418581249883397\n",
            "Step 16634, train_loss 0.4542534760003486, test_loss 0.44185771855915335\n",
            "Step 16635, train_loss 0.4542528484967622, test_loss 0.44185731221143787\n",
            "Step 16636, train_loss 0.45425222105657076, test_loss 0.4418569059451762\n",
            "Step 16637, train_loss 0.45425159367976203, test_loss 0.44185649976035124\n",
            "Step 16638, train_loss 0.45425096636632323, test_loss 0.44185609365694595\n",
            "Step 16639, train_loss 0.4542503391162422, test_loss 0.441855687634943\n",
            "Step 16640, train_loss 0.4542497119295062, test_loss 0.4418552816943254\n",
            "Step 16641, train_loss 0.4542490848061029, test_loss 0.44185487583507627\n",
            "Step 16642, train_loss 0.4542484577460196, test_loss 0.4418544700571782\n",
            "Step 16643, train_loss 0.4542478307492441, test_loss 0.4418540643606143\n",
            "Step 16644, train_loss 0.45424720381576383, test_loss 0.44185365874536725\n",
            "Step 16645, train_loss 0.45424657694556636, test_loss 0.44185325321142027\n",
            "Step 16646, train_loss 0.45424595013863917, test_loss 0.4418528477587562\n",
            "Step 16647, train_loss 0.4542453233949697, test_loss 0.4418524423873578\n",
            "Step 16648, train_loss 0.45424469671454587, test_loss 0.4418520370972083\n",
            "Step 16649, train_loss 0.4542440700973549, test_loss 0.44185163188829035\n",
            "Step 16650, train_loss 0.45424344354338436, test_loss 0.44185122676058713\n",
            "Step 16651, train_loss 0.4542428170526221, test_loss 0.4418508217140814\n",
            "Step 16652, train_loss 0.45424219062505533, test_loss 0.44185041674875625\n",
            "Step 16653, train_loss 0.4542415642606718, test_loss 0.44185001186459455\n",
            "Step 16654, train_loss 0.454240937959459, test_loss 0.4418496070615794\n",
            "Step 16655, train_loss 0.45424031172140467, test_loss 0.44184920233969355\n",
            "Step 16656, train_loss 0.45423968554649624, test_loss 0.44184879769892016\n",
            "Step 16657, train_loss 0.4542390594347213, test_loss 0.4418483931392421\n",
            "Step 16658, train_loss 0.4542384333860676, test_loss 0.44184798866064245\n",
            "Step 16659, train_loss 0.45423780740052255, test_loss 0.4418475842631041\n",
            "Step 16660, train_loss 0.45423718147807374, test_loss 0.44184717994661005\n",
            "Step 16661, train_loss 0.45423655561870885, test_loss 0.4418467757111435\n",
            "Step 16662, train_loss 0.4542359298224155, test_loss 0.44184637155668705\n",
            "Step 16663, train_loss 0.4542353040891813, test_loss 0.4418459674832241\n",
            "Step 16664, train_loss 0.4542346784189937, test_loss 0.4418455634907374\n",
            "Step 16665, train_loss 0.45423405281184065, test_loss 0.44184515957921006\n",
            "Step 16666, train_loss 0.4542334272677094, test_loss 0.44184475574862525\n",
            "Step 16667, train_loss 0.4542328017865877, test_loss 0.4418443519989658\n",
            "Step 16668, train_loss 0.4542321763684633, test_loss 0.44184394833021484\n",
            "Step 16669, train_loss 0.45423155101332374, test_loss 0.4418435447423553\n",
            "Step 16670, train_loss 0.4542309257211566, test_loss 0.4418431412353702\n",
            "Step 16671, train_loss 0.45423030049194957, test_loss 0.4418427378092427\n",
            "Step 16672, train_loss 0.45422967532569036, test_loss 0.44184233446395577\n",
            "Step 16673, train_loss 0.4542290502223665, test_loss 0.4418419311994926\n",
            "Step 16674, train_loss 0.45422842518196554, test_loss 0.4418415280158361\n",
            "Step 16675, train_loss 0.45422780020447545, test_loss 0.44184112491296934\n",
            "Step 16676, train_loss 0.4542271752898837, test_loss 0.4418407218908754\n",
            "Step 16677, train_loss 0.45422655043817795, test_loss 0.44184031894953746\n",
            "Step 16678, train_loss 0.4542259256493459, test_loss 0.44183991608893847\n",
            "Step 16679, train_loss 0.4542253009233752, test_loss 0.4418395133090615\n",
            "Step 16680, train_loss 0.4542246762602534, test_loss 0.4418391106098898\n",
            "Step 16681, train_loss 0.4542240516599685, test_loss 0.4418387079914063\n",
            "Step 16682, train_loss 0.45422342712250785, test_loss 0.44183830545359404\n",
            "Step 16683, train_loss 0.4542228026478592, test_loss 0.4418379029964363\n",
            "Step 16684, train_loss 0.45422217823601035, test_loss 0.4418375006199161\n",
            "Step 16685, train_loss 0.45422155388694896, test_loss 0.4418370983240165\n",
            "Step 16686, train_loss 0.45422092960066274, test_loss 0.44183669610872073\n",
            "Step 16687, train_loss 0.45422030537713926, test_loss 0.44183629397401175\n",
            "Step 16688, train_loss 0.4542196812163663, test_loss 0.4418358919198729\n",
            "Step 16689, train_loss 0.4542190571183316, test_loss 0.441835489946287\n",
            "Step 16690, train_loss 0.45421843308302284, test_loss 0.4418350880532375\n",
            "Step 16691, train_loss 0.45421780911042786, test_loss 0.4418346862407074\n",
            "Step 16692, train_loss 0.4542171852005341, test_loss 0.4418342845086797\n",
            "Step 16693, train_loss 0.4542165613533295, test_loss 0.4418338828571377\n",
            "Step 16694, train_loss 0.4542159375688018, test_loss 0.44183348128606464\n",
            "Step 16695, train_loss 0.4542153138469386, test_loss 0.44183307979544356\n",
            "Step 16696, train_loss 0.4542146901877276, test_loss 0.4418326783852575\n",
            "Step 16697, train_loss 0.4542140665911566, test_loss 0.4418322770554898\n",
            "Step 16698, train_loss 0.4542134430572135, test_loss 0.44183187580612354\n",
            "Step 16699, train_loss 0.45421281958588583, test_loss 0.44183147463714195\n",
            "Step 16701, train_loss 0.45421157283102787, test_loss 0.44183067254026537\n",
            "Step 16702, train_loss 0.45421094954747315, test_loss 0.4418302716123369\n",
            "Step 16703, train_loss 0.454210326326485, test_loss 0.4418298707647256\n",
            "Step 16704, train_loss 0.4542097031680512, test_loss 0.4418294699974149\n",
            "Step 16705, train_loss 0.4542090800721593, test_loss 0.4418290693103881\n",
            "Step 16706, train_loss 0.4542084570387973, test_loss 0.44182866870362825\n",
            "Step 16707, train_loss 0.4542078340679528, test_loss 0.44182826817711834\n",
            "Step 16708, train_loss 0.45420721115961366, test_loss 0.4418278677308422\n",
            "Step 16709, train_loss 0.4542065883137678, test_loss 0.4418274673647825\n",
            "Step 16710, train_loss 0.4542059655304027, test_loss 0.4418270670789226\n",
            "Step 16711, train_loss 0.4542053428095064, test_loss 0.4418266668732458\n",
            "Step 16712, train_loss 0.4542047201510666, test_loss 0.4418262667477353\n",
            "Step 16713, train_loss 0.4542040975550712, test_loss 0.44182586670237434\n",
            "Step 16714, train_loss 0.4542034750215077, test_loss 0.44182546673714623\n",
            "Step 16715, train_loss 0.4542028525503642, test_loss 0.441825066852034\n",
            "Step 16716, train_loss 0.4542022301416284, test_loss 0.4418246670470211\n",
            "Step 16717, train_loss 0.45420160779528823, test_loss 0.44182426732209074\n",
            "Step 16718, train_loss 0.45420098551133126, test_loss 0.4418238676772261\n",
            "Step 16719, train_loss 0.45420036328974556, test_loss 0.4418234681124105\n",
            "Step 16720, train_loss 0.4541997411305187, test_loss 0.44182306862762727\n",
            "Step 16721, train_loss 0.4541991190336389, test_loss 0.44182266922285973\n",
            "Step 16722, train_loss 0.45419849699909354, test_loss 0.44182226989809087\n",
            "Step 16723, train_loss 0.45419787502687076, test_loss 0.4418218706533043\n",
            "Step 16724, train_loss 0.4541972531169582, test_loss 0.44182147148848316\n",
            "Step 16725, train_loss 0.4541966312693437, test_loss 0.4418210724036106\n",
            "Step 16726, train_loss 0.45419600948401534, test_loss 0.44182067339867015\n",
            "Step 16727, train_loss 0.4541953877609607, test_loss 0.4418202744736452\n",
            "Step 16728, train_loss 0.4541947661001679, test_loss 0.44181987562851865\n",
            "Step 16729, train_loss 0.45419414450162454, test_loss 0.441819476863274\n",
            "Step 16730, train_loss 0.45419352296531856, test_loss 0.4418190781778946\n",
            "Step 16731, train_loss 0.45419290149123787, test_loss 0.441818679572364\n",
            "Step 16732, train_loss 0.45419228007937035, test_loss 0.44181828104666504\n",
            "Step 16733, train_loss 0.4541916587297039, test_loss 0.4418178826007814\n",
            "Step 16734, train_loss 0.4541910374422263, test_loss 0.44181748423469636\n",
            "Step 16735, train_loss 0.45419041621692535, test_loss 0.44181708594839314\n",
            "Step 16736, train_loss 0.45418979505378915, test_loss 0.4418166877418552\n",
            "Step 16737, train_loss 0.45418917395280545, test_loss 0.4418162896150658\n",
            "Step 16738, train_loss 0.45418855291396215, test_loss 0.4418158915680083\n",
            "Step 16739, train_loss 0.45418793193724716, test_loss 0.44181549360066613\n",
            "Step 16740, train_loss 0.45418731102264837, test_loss 0.4418150957130226\n",
            "Step 16741, train_loss 0.4541866901701537, test_loss 0.441814697905061\n",
            "Step 16742, train_loss 0.45418606937975103, test_loss 0.4418143001767648\n",
            "Step 16743, train_loss 0.45418544865142835, test_loss 0.44181390252811736\n",
            "Step 16744, train_loss 0.4541848279851734, test_loss 0.441813504959102\n",
            "Step 16745, train_loss 0.45418420738097415, test_loss 0.4418131074697021\n",
            "Step 16746, train_loss 0.4541835868388187, test_loss 0.4418127100599012\n",
            "Step 16747, train_loss 0.45418296635869476, test_loss 0.4418123127296824\n",
            "Step 16748, train_loss 0.4541823459405903, test_loss 0.4418119154790296\n",
            "Step 16749, train_loss 0.4541817255844934, test_loss 0.4418115183079256\n",
            "Step 16750, train_loss 0.4541811052903918, test_loss 0.44181112121635413\n",
            "Step 16751, train_loss 0.4541804850582734, test_loss 0.44181072420429857\n",
            "Step 16752, train_loss 0.4541798648881265, test_loss 0.44181032727174224\n",
            "Step 16753, train_loss 0.4541792447799386, test_loss 0.4418099304186687\n",
            "Step 16754, train_loss 0.4541786247336979, test_loss 0.44180953364506126\n",
            "Step 16755, train_loss 0.45417800474939224, test_loss 0.44180913695090335\n",
            "Step 16756, train_loss 0.45417738482700976, test_loss 0.44180874033617845\n",
            "Step 16757, train_loss 0.4541767649665382, test_loss 0.44180834380087003\n",
            "Step 16758, train_loss 0.45417614516796556, test_loss 0.44180794734496154\n",
            "Step 16759, train_loss 0.45417552543127987, test_loss 0.4418075509684363\n",
            "Step 16760, train_loss 0.45417490575646924, test_loss 0.4418071546712778\n",
            "Step 16761, train_loss 0.45417428614352146, test_loss 0.4418067584534696\n",
            "Step 16762, train_loss 0.4541736665924244, test_loss 0.4418063623149949\n",
            "Step 16763, train_loss 0.45417304710316636, test_loss 0.4418059662558376\n",
            "Step 16764, train_loss 0.45417242767573507, test_loss 0.44180557027598066\n",
            "Step 16765, train_loss 0.45417180831011855, test_loss 0.441805174375408\n",
            "Step 16766, train_loss 0.45417118900630493, test_loss 0.44180477855410283\n",
            "Step 16767, train_loss 0.45417056976428205, test_loss 0.4418043828120487\n",
            "Step 16768, train_loss 0.4541699505840379, test_loss 0.441803987149229\n",
            "Step 16769, train_loss 0.4541693314655607, test_loss 0.4418035915656273\n",
            "Step 16770, train_loss 0.45416871240883827, test_loss 0.44180319606122725\n",
            "Step 16771, train_loss 0.45416809341385866, test_loss 0.4418028006360121\n",
            "Step 16772, train_loss 0.45416747448060996, test_loss 0.4418024052899656\n",
            "Step 16773, train_loss 0.45416685560907993, test_loss 0.441802010023071\n",
            "Step 16774, train_loss 0.45416623679925694, test_loss 0.4418016148353119\n",
            "Step 16775, train_loss 0.4541656180511289, test_loss 0.44180121972667197\n",
            "Step 16776, train_loss 0.4541649993646837, test_loss 0.4418008246971345\n",
            "Step 16777, train_loss 0.4541643807399094, test_loss 0.4418004297466831\n",
            "Step 16778, train_loss 0.4541637621767943, test_loss 0.4418000348753014\n",
            "Step 16779, train_loss 0.45416314367532606, test_loss 0.4417996400829728\n",
            "Step 16780, train_loss 0.45416252523549305, test_loss 0.44179924536968085\n",
            "Step 16781, train_loss 0.454161906857283, test_loss 0.44179885073540937\n",
            "Step 16782, train_loss 0.4541612885406844, test_loss 0.44179845618014146\n",
            "Step 16783, train_loss 0.45416067028568485, test_loss 0.44179806170386093\n",
            "Step 16784, train_loss 0.45416005209227256, test_loss 0.4417976673065513\n",
            "Step 16785, train_loss 0.4541594339604357, test_loss 0.4417972729881962\n",
            "Step 16786, train_loss 0.4541588158901622, test_loss 0.44179687874877904\n",
            "Step 16787, train_loss 0.4541581978814403, test_loss 0.44179648458828347\n",
            "Step 16788, train_loss 0.4541575799342579, test_loss 0.44179609050669305\n",
            "Step 16789, train_loss 0.45415696204860323, test_loss 0.4417956965039915\n",
            "Step 16790, train_loss 0.45415634422446427, test_loss 0.4417953025801622\n",
            "Step 16791, train_loss 0.454155726461829, test_loss 0.44179490873518873\n",
            "Step 16792, train_loss 0.4541551087606857, test_loss 0.44179451496905486\n",
            "Step 16793, train_loss 0.45415449112102246, test_loss 0.44179412128174417\n",
            "Step 16794, train_loss 0.4541538735428272, test_loss 0.44179372767324\n",
            "Step 16795, train_loss 0.45415325602608814, test_loss 0.44179333414352634\n",
            "Step 16796, train_loss 0.45415263857079335, test_loss 0.4417929406925865\n",
            "Step 16797, train_loss 0.45415202117693104, test_loss 0.4417925473204042\n",
            "Step 16798, train_loss 0.4541514038444892, test_loss 0.4417921540269631\n",
            "Step 16799, train_loss 0.45415078657345587, test_loss 0.44179176081224675\n",
            "Step 16801, train_loss 0.45414955221556774, test_loss 0.44179097461892286\n",
            "Step 16802, train_loss 0.45414893512868887, test_loss 0.4417905816402828\n",
            "Step 16803, train_loss 0.4541483181031713, test_loss 0.4417901887403019\n",
            "Step 16804, train_loss 0.4541477011390028, test_loss 0.4417897959189639\n",
            "Step 16805, train_loss 0.4541470842361718, test_loss 0.44178940317625265\n",
            "Step 16806, train_loss 0.45414646739466624, test_loss 0.4417890105121516\n",
            "Step 16807, train_loss 0.4541458506144743, test_loss 0.4417886179266445\n",
            "Step 16808, train_loss 0.4541452338955842, test_loss 0.44178822541971496\n",
            "Step 16809, train_loss 0.454144617237984, test_loss 0.4417878329913467\n",
            "Step 16810, train_loss 0.45414400064166194, test_loss 0.4417874406415234\n",
            "Step 16811, train_loss 0.4541433841066061, test_loss 0.44178704837022875\n",
            "Step 16812, train_loss 0.45414276763280464, test_loss 0.44178665617744634\n",
            "Step 16813, train_loss 0.45414215122024576, test_loss 0.4417862640631599\n",
            "Step 16814, train_loss 0.4541415348689176, test_loss 0.4417858720273531\n",
            "Step 16815, train_loss 0.4541409185788084, test_loss 0.4417854800700096\n",
            "Step 16816, train_loss 0.45414030234990616, test_loss 0.4417850881911132\n",
            "Step 16817, train_loss 0.4541396861821993, test_loss 0.44178469639064766\n",
            "Step 16818, train_loss 0.4541390700756758, test_loss 0.44178430466859653\n",
            "Step 16819, train_loss 0.4541384540303239, test_loss 0.44178391302494363\n",
            "Step 16820, train_loss 0.45413783804613195, test_loss 0.44178352145967265\n",
            "Step 16821, train_loss 0.45413722212308794, test_loss 0.44178312997276714\n",
            "Step 16822, train_loss 0.4541366062611801, test_loss 0.44178273856421113\n",
            "Step 16823, train_loss 0.4541359904603966, test_loss 0.4417823472339881\n",
            "Step 16824, train_loss 0.45413537472072585, test_loss 0.4417819559820819\n",
            "Step 16825, train_loss 0.45413475904215583, test_loss 0.4417815648084764\n",
            "Step 16826, train_loss 0.4541341434246747, test_loss 0.4417811737131551\n",
            "Step 16827, train_loss 0.45413352786827105, test_loss 0.4417807826961016\n",
            "Step 16828, train_loss 0.45413291237293274, test_loss 0.44178039175730016\n",
            "Step 16829, train_loss 0.45413229693864815, test_loss 0.44178000089673414\n",
            "Step 16830, train_loss 0.4541316815654053, test_loss 0.44177961011438754\n",
            "Step 16831, train_loss 0.45413106625319266, test_loss 0.4417792194102439\n",
            "Step 16832, train_loss 0.45413045100199834, test_loss 0.4417788287842872\n",
            "Step 16833, train_loss 0.4541298358118107, test_loss 0.441778438236501\n",
            "Step 16834, train_loss 0.45412922068261785, test_loss 0.4417780477668693\n",
            "Step 16835, train_loss 0.4541286056144081, test_loss 0.4417776573753758\n",
            "Step 16836, train_loss 0.4541279906071696, test_loss 0.44177726706200404\n",
            "Step 16837, train_loss 0.45412737566089073, test_loss 0.4417768768267382\n",
            "Step 16838, train_loss 0.4541267607755597, test_loss 0.441776486669562\n",
            "Step 16839, train_loss 0.4541261459511647, test_loss 0.4417760965904591\n",
            "Step 16840, train_loss 0.45412553118769405, test_loss 0.44177570658941306\n",
            "Step 16841, train_loss 0.454124916485136, test_loss 0.44177531666640824\n",
            "Step 16842, train_loss 0.4541243018434788, test_loss 0.44177492682142827\n",
            "Step 16843, train_loss 0.45412368726271085, test_loss 0.4417745370544568\n",
            "Step 16844, train_loss 0.4541230727428201, test_loss 0.44177414736547765\n",
            "Step 16845, train_loss 0.45412245828379516, test_loss 0.44177375775447497\n",
            "Step 16846, train_loss 0.45412184388562427, test_loss 0.44177336822143204\n",
            "Step 16847, train_loss 0.4541212295482955, test_loss 0.4417729787663332\n",
            "Step 16848, train_loss 0.45412061527179737, test_loss 0.44177258938916214\n",
            "Step 16849, train_loss 0.45412000105611805, test_loss 0.44177220008990253\n",
            "Step 16850, train_loss 0.4541193869012459, test_loss 0.44177181086853845\n",
            "Step 16851, train_loss 0.45411877280716917, test_loss 0.4417714217250537\n",
            "Step 16852, train_loss 0.4541181587738762, test_loss 0.4417710326594321\n",
            "Step 16853, train_loss 0.45411754480135524, test_loss 0.44177064367165736\n",
            "Step 16854, train_loss 0.45411693088959465, test_loss 0.44177025476171367\n",
            "Step 16855, train_loss 0.45411631703858274, test_loss 0.44176986592958456\n",
            "Step 16856, train_loss 0.45411570324830786, test_loss 0.4417694771752541\n",
            "Step 16857, train_loss 0.45411508951875823, test_loss 0.44176908849870633\n",
            "Step 16858, train_loss 0.45411447584992226, test_loss 0.4417686998999248\n",
            "Step 16859, train_loss 0.45411386224178824, test_loss 0.44176831137889344\n",
            "Step 16860, train_loss 0.45411324869434444, test_loss 0.44176792293559636\n",
            "Step 16861, train_loss 0.45411263520757944, test_loss 0.4417675345700173\n",
            "Step 16862, train_loss 0.45411202178148136, test_loss 0.4417671462821402\n",
            "Step 16863, train_loss 0.4541114084160385, test_loss 0.44176675807194893\n",
            "Step 16864, train_loss 0.45411079511123936, test_loss 0.4417663699394275\n",
            "Step 16865, train_loss 0.45411018186707214, test_loss 0.4417659818845597\n",
            "Step 16866, train_loss 0.4541095686835253, test_loss 0.4417655939073294\n",
            "Step 16867, train_loss 0.4541089555605873, test_loss 0.4417652060077206\n",
            "Step 16868, train_loss 0.4541083424982462, test_loss 0.4417648181857175\n",
            "Step 16869, train_loss 0.45410772949649064, test_loss 0.4417644304413037\n",
            "Step 16870, train_loss 0.4541071165553089, test_loss 0.441764042774463\n",
            "Step 16871, train_loss 0.45410650367468935, test_loss 0.4417636551851797\n",
            "Step 16872, train_loss 0.45410589085462016, test_loss 0.44176326767343765\n",
            "Step 16873, train_loss 0.45410527809509005, test_loss 0.44176288023922056\n",
            "Step 16874, train_loss 0.4541046653960873, test_loss 0.44176249288251274\n",
            "Step 16875, train_loss 0.45410405275760013, test_loss 0.441762105603298\n",
            "Step 16876, train_loss 0.454103440179617, test_loss 0.4417617184015601\n",
            "Step 16877, train_loss 0.4541028276621265, test_loss 0.44176133127728334\n",
            "Step 16878, train_loss 0.4541022152051168, test_loss 0.4417609442304515\n",
            "Step 16879, train_loss 0.4541016028085763, test_loss 0.4417605572610484\n",
            "Step 16880, train_loss 0.45410099047249364, test_loss 0.4417601703690585\n",
            "Step 16881, train_loss 0.45410037819685684, test_loss 0.4417597835544653\n",
            "Step 16882, train_loss 0.4540997659816546, test_loss 0.4417593968172529\n",
            "Step 16883, train_loss 0.45409915382687527, test_loss 0.44175901015740554\n",
            "Step 16884, train_loss 0.4540985417325073, test_loss 0.441758623574907\n",
            "Step 16885, train_loss 0.4540979296985389, test_loss 0.44175823706974127\n",
            "Step 16886, train_loss 0.45409731772495887, test_loss 0.4417578506418923\n",
            "Step 16887, train_loss 0.4540967058117553, test_loss 0.4417574642913444\n",
            "Step 16888, train_loss 0.45409609395891676, test_loss 0.44175707801808123\n",
            "Step 16889, train_loss 0.4540954821664317, test_loss 0.44175669182208704\n",
            "Step 16890, train_loss 0.4540948704342885, test_loss 0.4417563057033458\n",
            "Step 16891, train_loss 0.4540942587624756, test_loss 0.4417559196618414\n",
            "Step 16892, train_loss 0.4540936471509814, test_loss 0.4417555336975581\n",
            "Step 16893, train_loss 0.45409303559979447, test_loss 0.44175514781047964\n",
            "Step 16894, train_loss 0.4540924241089034, test_loss 0.4417547620005904\n",
            "Step 16895, train_loss 0.4540918126782962, test_loss 0.4417543762678743\n",
            "Step 16896, train_loss 0.45409120130796166, test_loss 0.4417539906123152\n",
            "Step 16897, train_loss 0.4540905899978882, test_loss 0.44175360503389743\n",
            "Step 16898, train_loss 0.45408997874806417, test_loss 0.4417532195326046\n",
            "Step 16899, train_loss 0.4540893675584781, test_loss 0.44175283410842137\n",
            "Step 16901, train_loss 0.454088145359974, test_loss 0.44175206349131896\n",
            "Step 16902, train_loss 0.4540875343510328, test_loss 0.44175167829836803\n",
            "Step 16903, train_loss 0.4540869234022835, test_loss 0.4417512931824627\n",
            "Step 16904, train_loss 0.4540863125137145, test_loss 0.4417509081435871\n",
            "Step 16905, train_loss 0.4540857016853145, test_loss 0.441750523181725\n",
            "Step 16906, train_loss 0.4540850909170717, test_loss 0.4417501382968609\n",
            "Step 16907, train_loss 0.45408448020897485, test_loss 0.44174975348897877\n",
            "Step 16908, train_loss 0.4540838695610122, test_loss 0.4417493687580627\n",
            "Step 16909, train_loss 0.45408325897317264, test_loss 0.44174898410409674\n",
            "Step 16910, train_loss 0.45408264844544427, test_loss 0.4417485995270651\n",
            "Step 16911, train_loss 0.4540820379778159, test_loss 0.4417482150269517\n",
            "Step 16912, train_loss 0.45408142757027586, test_loss 0.44174783060374084\n",
            "Step 16913, train_loss 0.4540808172228126, test_loss 0.4417474462574166\n",
            "Step 16914, train_loss 0.45408020693541506, test_loss 0.4417470619879632\n",
            "Step 16915, train_loss 0.45407959670807124, test_loss 0.4417466777953644\n",
            "Step 16916, train_loss 0.45407898654077006, test_loss 0.4417462936796048\n",
            "Step 16917, train_loss 0.4540783764334998, test_loss 0.4417459096406683\n",
            "Step 16918, train_loss 0.4540777663862491, test_loss 0.44174552567853903\n",
            "Step 16919, train_loss 0.45407715639900653, test_loss 0.4417451417932011\n",
            "Step 16920, train_loss 0.4540765464717605, test_loss 0.4417447579846389\n",
            "Step 16921, train_loss 0.45407593660449974, test_loss 0.4417443742528363\n",
            "Step 16922, train_loss 0.45407532679721263, test_loss 0.4417439905977777\n",
            "Step 16923, train_loss 0.4540747170498879, test_loss 0.4417436070194469\n",
            "Step 16924, train_loss 0.45407410736251386, test_loss 0.44174322351782846\n",
            "Step 16925, train_loss 0.45407349773507927, test_loss 0.4417428400929064\n",
            "Step 16926, train_loss 0.45407288816757274, test_loss 0.44174245674466495\n",
            "Step 16927, train_loss 0.45407227865998256, test_loss 0.4417420734730882\n",
            "Step 16928, train_loss 0.4540716692122977, test_loss 0.44174169027816035\n",
            "Step 16929, train_loss 0.4540710598245063, test_loss 0.4417413071598655\n",
            "Step 16930, train_loss 0.4540704504965972, test_loss 0.4417409241181881\n",
            "Step 16931, train_loss 0.4540698412285589, test_loss 0.4417405411531122\n",
            "Step 16932, train_loss 0.4540692320203801, test_loss 0.44174015826462193\n",
            "Step 16933, train_loss 0.4540686228720491, test_loss 0.44173977545270154\n",
            "Step 16934, train_loss 0.4540680137835549, test_loss 0.44173939271733526\n",
            "Step 16935, train_loss 0.4540674047548858, test_loss 0.4417390100585074\n",
            "Step 16936, train_loss 0.4540667957860305, test_loss 0.4417386274762021\n",
            "Step 16937, train_loss 0.45406618687697753, test_loss 0.4417382449704035\n",
            "Step 16938, train_loss 0.4540655780277156, test_loss 0.44173786254109587\n",
            "Step 16939, train_loss 0.45406496923823314, test_loss 0.4417374801882636\n",
            "Step 16940, train_loss 0.454064360508519, test_loss 0.44173709791189064\n",
            "Step 16941, train_loss 0.4540637518385616, test_loss 0.44173671571196144\n",
            "Step 16942, train_loss 0.4540631432283496, test_loss 0.44173633358846015\n",
            "Step 16943, train_loss 0.4540625346778717, test_loss 0.441735951541371\n",
            "Step 16944, train_loss 0.4540619261871166, test_loss 0.44173556957067844\n",
            "Step 16945, train_loss 0.4540613177560726, test_loss 0.44173518767636644\n",
            "Step 16946, train_loss 0.45406070938472876, test_loss 0.4417348058584195\n",
            "Step 16947, train_loss 0.4540601010730732, test_loss 0.4417344241168218\n",
            "Step 16948, train_loss 0.4540594928210951, test_loss 0.4417340424515576\n",
            "Step 16949, train_loss 0.4540588846287828, test_loss 0.4417336608626112\n",
            "Step 16950, train_loss 0.4540582764961251, test_loss 0.44173327934996665\n",
            "Step 16951, train_loss 0.45405766842311035, test_loss 0.44173289791360865\n",
            "Step 16952, train_loss 0.4540570604097275, test_loss 0.44173251655352114\n",
            "Step 16953, train_loss 0.45405645245596504, test_loss 0.4417321352696886\n",
            "Step 16954, train_loss 0.4540558445618118, test_loss 0.4417317540620952\n",
            "Step 16955, train_loss 0.4540552367272563, test_loss 0.44173137293072534\n",
            "Step 16956, train_loss 0.45405462895228726, test_loss 0.4417309918755633\n",
            "Step 16957, train_loss 0.4540540212368933, test_loss 0.44173061089659327\n",
            "Step 16958, train_loss 0.45405341358106327, test_loss 0.44173022999379963\n",
            "Step 16959, train_loss 0.4540528059847855, test_loss 0.44172984916716695\n",
            "Step 16960, train_loss 0.45405219844804895, test_loss 0.44172946841667915\n",
            "Step 16961, train_loss 0.45405159097084236, test_loss 0.44172908774232067\n",
            "Step 16962, train_loss 0.45405098355315415, test_loss 0.4417287071440761\n",
            "Step 16963, train_loss 0.4540503761949732, test_loss 0.4417283266219294\n",
            "Step 16964, train_loss 0.45404976889628795, test_loss 0.44172794617586514\n",
            "Step 16965, train_loss 0.45404916165708753, test_loss 0.4417275658058675\n",
            "Step 16966, train_loss 0.4540485544773604, test_loss 0.44172718551192114\n",
            "Step 16967, train_loss 0.45404794735709525, test_loss 0.44172680529401\n",
            "Step 16968, train_loss 0.4540473402962808, test_loss 0.4417264251521187\n",
            "Step 16969, train_loss 0.45404673329490575, test_loss 0.44172604508623137\n",
            "Step 16970, train_loss 0.4540461263529588, test_loss 0.44172566509633276\n",
            "Step 16971, train_loss 0.4540455194704287, test_loss 0.44172528518240695\n",
            "Step 16972, train_loss 0.4540449126473043, test_loss 0.44172490534443826\n",
            "Step 16973, train_loss 0.4540443058835741, test_loss 0.44172452558241115\n",
            "Step 16974, train_loss 0.4540436991792269, test_loss 0.44172414589631004\n",
            "Step 16975, train_loss 0.4540430925342516, test_loss 0.4417237662861193\n",
            "Step 16976, train_loss 0.45404248594863666, test_loss 0.4417233867518233\n",
            "Step 16977, train_loss 0.45404187942237095, test_loss 0.44172300729340636\n",
            "Step 16978, train_loss 0.4540412729554433, test_loss 0.44172262791085304\n",
            "Step 16979, train_loss 0.45404066654784225, test_loss 0.44172224860414766\n",
            "Step 16980, train_loss 0.4540400601995567, test_loss 0.4417218693732746\n",
            "Step 16981, train_loss 0.45403945391057543, test_loss 0.4417214902182182\n",
            "Step 16982, train_loss 0.45403884768088704, test_loss 0.44172111113896295\n",
            "Step 16983, train_loss 0.45403824151048044, test_loss 0.44172073213549334\n",
            "Step 16984, train_loss 0.4540376353993443, test_loss 0.4417203532077938\n",
            "Step 16985, train_loss 0.45403702934746737, test_loss 0.4417199743558485\n",
            "Step 16986, train_loss 0.45403642335483857, test_loss 0.44171959557964213\n",
            "Step 16987, train_loss 0.4540358174214466, test_loss 0.441719216879159\n",
            "Step 16988, train_loss 0.45403521154728005, test_loss 0.44171883825438346\n",
            "Step 16989, train_loss 0.4540346057323279, test_loss 0.4417184597053003\n",
            "Step 16990, train_loss 0.4540339999765789, test_loss 0.44171808123189366\n",
            "Step 16991, train_loss 0.4540333942800218, test_loss 0.441717702834148\n",
            "Step 16992, train_loss 0.45403278864264546, test_loss 0.4417173245120478\n",
            "Step 16993, train_loss 0.45403218306443854, test_loss 0.4417169462655776\n",
            "Step 16994, train_loss 0.45403157754539003, test_loss 0.4417165680947219\n",
            "Step 16995, train_loss 0.4540309720854885, test_loss 0.44171618999946505\n",
            "Step 16996, train_loss 0.4540303666847229, test_loss 0.44171581197979154\n",
            "Step 16997, train_loss 0.45402976134308204, test_loss 0.44171543403568586\n",
            "Step 16998, train_loss 0.45402915606055466, test_loss 0.4417150561671324\n",
            "Step 16999, train_loss 0.45402855083712945, test_loss 0.44171467837411593\n",
            "Step 17001, train_loss 0.45402734056754157, test_loss 0.44171392301463097\n",
            "Step 17002, train_loss 0.4540267355213563, test_loss 0.4417135454481318\n",
            "Step 17003, train_loss 0.45402613053422886, test_loss 0.44171316795710736\n",
            "Step 17004, train_loss 0.4540255256061476, test_loss 0.44171279054154206\n",
            "Step 17005, train_loss 0.4540249207371018, test_loss 0.4417124132014205\n",
            "Step 17006, train_loss 0.45402431592708, test_loss 0.4417120359367274\n",
            "Step 17007, train_loss 0.4540237111760712, test_loss 0.441711658747447\n",
            "Step 17008, train_loss 0.45402310648406413, test_loss 0.441711281633564\n",
            "Step 17009, train_loss 0.45402250185104776, test_loss 0.4417109045950628\n",
            "Step 17010, train_loss 0.45402189727701076, test_loss 0.4417105276319279\n",
            "Step 17011, train_loss 0.45402129276194214, test_loss 0.44171015074414405\n",
            "Step 17012, train_loss 0.4540206883058308, test_loss 0.4417097739316956\n",
            "Step 17013, train_loss 0.45402008390866533, test_loss 0.44170939719456714\n",
            "Step 17014, train_loss 0.45401947957043504, test_loss 0.44170902053274325\n",
            "Step 17015, train_loss 0.4540188752911283, test_loss 0.44170864394620835\n",
            "Step 17016, train_loss 0.4540182710707343, test_loss 0.4417082674349471\n",
            "Step 17017, train_loss 0.45401766690924183, test_loss 0.4417078909989441\n",
            "Step 17018, train_loss 0.45401706280663967, test_loss 0.4417075146381838\n",
            "Step 17019, train_loss 0.4540164587629169, test_loss 0.44170713835265085\n",
            "Step 17020, train_loss 0.4540158547780622, test_loss 0.44170676214232973\n",
            "Step 17021, train_loss 0.4540152508520646, test_loss 0.44170638600720524\n",
            "Step 17022, train_loss 0.4540146469849129, test_loss 0.4417060099472616\n",
            "Step 17023, train_loss 0.45401404317659605, test_loss 0.44170563396248363\n",
            "Step 17024, train_loss 0.45401343942710304, test_loss 0.44170525805285593\n",
            "Step 17025, train_loss 0.4540128357364225, test_loss 0.441704882218363\n",
            "Step 17026, train_loss 0.4540122321045435, test_loss 0.44170450645898945\n",
            "Step 17027, train_loss 0.45401162853145516, test_loss 0.44170413077472004\n",
            "Step 17028, train_loss 0.4540110250171459, test_loss 0.44170375516553906\n",
            "Step 17029, train_loss 0.4540104215616049, test_loss 0.4417033796314313\n",
            "Step 17030, train_loss 0.4540098181648212, test_loss 0.44170300417238145\n",
            "Step 17031, train_loss 0.45400921482678364, test_loss 0.4417026287883739\n",
            "Step 17032, train_loss 0.45400861154748107, test_loss 0.4417022534793935\n",
            "Step 17033, train_loss 0.45400800832690236, test_loss 0.4417018782454246\n",
            "Step 17034, train_loss 0.4540074051650365, test_loss 0.44170150308645223\n",
            "Step 17035, train_loss 0.45400680206187255, test_loss 0.4417011280024607\n",
            "Step 17036, train_loss 0.4540061990173994, test_loss 0.4417007529934348\n",
            "Step 17037, train_loss 0.4540055960316058, test_loss 0.441700378059359\n",
            "Step 17038, train_loss 0.4540049931044809, test_loss 0.4417000032002181\n",
            "Step 17039, train_loss 0.4540043902360135, test_loss 0.44169962841599675\n",
            "Step 17040, train_loss 0.45400378742619274, test_loss 0.4416992537066795\n",
            "Step 17041, train_loss 0.45400318467500733, test_loss 0.441698879072251\n",
            "Step 17042, train_loss 0.45400258198244636, test_loss 0.44169850451269604\n",
            "Step 17043, train_loss 0.4540019793484989, test_loss 0.4416981300279992\n",
            "Step 17044, train_loss 0.4540013767731538, test_loss 0.44169775561814517\n",
            "Step 17045, train_loss 0.4540007742564, test_loss 0.4416973812831186\n",
            "Step 17046, train_loss 0.4540001717982264, test_loss 0.44169700702290415\n",
            "Step 17047, train_loss 0.4539995693986221, test_loss 0.4416966328374866\n",
            "Step 17048, train_loss 0.45399896705757614, test_loss 0.4416962587268506\n",
            "Step 17049, train_loss 0.4539983647750774, test_loss 0.44169588469098076\n",
            "Step 17050, train_loss 0.4539977625511147, test_loss 0.4416955107298618\n",
            "Step 17051, train_loss 0.4539971603856773, test_loss 0.44169513684347844\n",
            "Step 17052, train_loss 0.4539965582787541, test_loss 0.44169476303181543\n",
            "Step 17053, train_loss 0.453995956230334, test_loss 0.4416943892948574\n",
            "Step 17054, train_loss 0.45399535424040616, test_loss 0.44169401563258903\n",
            "Step 17055, train_loss 0.45399475230895947, test_loss 0.44169364204499517\n",
            "Step 17056, train_loss 0.4539941504359829, test_loss 0.4416932685320604\n",
            "Step 17057, train_loss 0.4539935486214655, test_loss 0.44169289509376947\n",
            "Step 17058, train_loss 0.45399294686539643, test_loss 0.4416925217301073\n",
            "Step 17059, train_loss 0.4539923451677644, test_loss 0.44169214844105814\n",
            "Step 17060, train_loss 0.4539917435285587, test_loss 0.4416917752266073\n",
            "Step 17061, train_loss 0.45399114194776824, test_loss 0.4416914020867392\n",
            "Step 17062, train_loss 0.4539905404253819, test_loss 0.44169102902143864\n",
            "Step 17063, train_loss 0.45398993896138895, test_loss 0.44169065603069035\n",
            "Step 17064, train_loss 0.45398933755577825, test_loss 0.44169028311447905\n",
            "Step 17065, train_loss 0.453988736208539, test_loss 0.4416899102727896\n",
            "Step 17066, train_loss 0.4539881349196601, test_loss 0.44168953750560663\n",
            "Step 17067, train_loss 0.45398753368913064, test_loss 0.44168916481291504\n",
            "Step 17068, train_loss 0.4539869325169396, test_loss 0.4416887921946994\n",
            "Step 17069, train_loss 0.45398633140307615, test_loss 0.4416884196509448\n",
            "Step 17070, train_loss 0.45398573034752915, test_loss 0.4416880471816356\n",
            "Step 17071, train_loss 0.4539851293502878, test_loss 0.44168767478675697\n",
            "Step 17072, train_loss 0.45398452841134107, test_loss 0.44168730246629345\n",
            "Step 17073, train_loss 0.45398392753067823, test_loss 0.4416869302202299\n",
            "Step 17074, train_loss 0.453983326708288, test_loss 0.44168655804855106\n",
            "Step 17075, train_loss 0.45398272594415967, test_loss 0.4416861859512419\n",
            "Step 17076, train_loss 0.45398212523828235, test_loss 0.4416858139282871\n",
            "Step 17077, train_loss 0.45398152459064484, test_loss 0.44168544197967147\n",
            "Step 17078, train_loss 0.4539809240012367, test_loss 0.4416850701053795\n",
            "Step 17079, train_loss 0.4539803234700465, test_loss 0.44168469830539653\n",
            "Step 17080, train_loss 0.4539797229970635, test_loss 0.44168432657970713\n",
            "Step 17081, train_loss 0.45397912258227696, test_loss 0.44168395492829626\n",
            "Step 17082, train_loss 0.45397852222567575, test_loss 0.4416835833511484\n",
            "Step 17083, train_loss 0.45397792192724895, test_loss 0.4416832118482489\n",
            "Step 17084, train_loss 0.4539773216869859, test_loss 0.441682840419582\n",
            "Step 17085, train_loss 0.4539767215048754, test_loss 0.44168246906513287\n",
            "Step 17086, train_loss 0.45397612138090676, test_loss 0.44168209778488643\n",
            "Step 17087, train_loss 0.453975521315069, test_loss 0.4416817265788273\n",
            "Step 17088, train_loss 0.45397492130735134, test_loss 0.4416813554469404\n",
            "Step 17089, train_loss 0.4539743213577426, test_loss 0.4416809843892108\n",
            "Step 17090, train_loss 0.4539737214662323, test_loss 0.441680613405623\n",
            "Step 17091, train_loss 0.45397312163280923, test_loss 0.44168024249616206\n",
            "Step 17092, train_loss 0.4539725218574626, test_loss 0.44167987166081285\n",
            "Step 17093, train_loss 0.45397192214018167, test_loss 0.44167950089956015\n",
            "Step 17094, train_loss 0.45397132248095534, test_loss 0.44167913021238886\n",
            "Step 17095, train_loss 0.453970722879773, test_loss 0.4416787595992839\n",
            "Step 17096, train_loss 0.45397012333662357, test_loss 0.44167838906023005\n",
            "Step 17097, train_loss 0.45396952385149625, test_loss 0.4416780185952124\n",
            "Step 17098, train_loss 0.45396892442438025, test_loss 0.4416776482042156\n",
            "Step 17099, train_loss 0.4539683250552646, test_loss 0.4416772778872247\n",
            "Step 17101, train_loss 0.4539671264909911, test_loss 0.4416765374752\n",
            "Step 17102, train_loss 0.4539665272958116, test_loss 0.441676167380136\n",
            "Step 17103, train_loss 0.45396592815858916, test_loss 0.4416757973590175\n",
            "Step 17104, train_loss 0.45396532907931286, test_loss 0.44167542741182936\n",
            "Step 17105, train_loss 0.45396473005797183, test_loss 0.4416750575385565\n",
            "Step 17106, train_loss 0.4539641310945553, test_loss 0.44167468773918384\n",
            "Step 17107, train_loss 0.4539635321890526, test_loss 0.44167431801369633\n",
            "Step 17108, train_loss 0.45396293334145266, test_loss 0.44167394836207885\n",
            "Step 17109, train_loss 0.45396233455174473, test_loss 0.4416735787843163\n",
            "Step 17110, train_loss 0.45396173581991806, test_loss 0.4416732092803937\n",
            "Step 17111, train_loss 0.45396113714596165, test_loss 0.44167283985029604\n",
            "Step 17112, train_loss 0.4539605385298649, test_loss 0.44167247049400815\n",
            "Step 17113, train_loss 0.4539599399716169, test_loss 0.44167210121151496\n",
            "Step 17114, train_loss 0.453959341471207, test_loss 0.4416717320028016\n",
            "Step 17115, train_loss 0.45395874302862405, test_loss 0.4416713628678528\n",
            "Step 17116, train_loss 0.4539581446438576, test_loss 0.4416709938066536\n",
            "Step 17117, train_loss 0.4539575463168966, test_loss 0.4416706248191891\n",
            "Step 17118, train_loss 0.45395694804773035, test_loss 0.44167025590544406\n",
            "Step 17119, train_loss 0.45395634983634814, test_loss 0.44166988706540367\n",
            "Step 17120, train_loss 0.4539557516827391, test_loss 0.4416695182990526\n",
            "Step 17121, train_loss 0.4539551535868924, test_loss 0.44166914960637615\n",
            "Step 17122, train_loss 0.4539545555487974, test_loss 0.44166878098735907\n",
            "Step 17123, train_loss 0.45395395756844314, test_loss 0.44166841244198657\n",
            "Step 17124, train_loss 0.45395335964581907, test_loss 0.4416680439702435\n",
            "Step 17125, train_loss 0.45395276178091415, test_loss 0.4416676755721147\n",
            "Step 17126, train_loss 0.4539521639737178, test_loss 0.44166730724758557\n",
            "Step 17127, train_loss 0.4539515662242193, test_loss 0.4416669389966408\n",
            "Step 17128, train_loss 0.4539509685324077, test_loss 0.4416665708192655\n",
            "Step 17129, train_loss 0.45395037089827234, test_loss 0.44166620271544466\n",
            "Step 17130, train_loss 0.45394977332180253, test_loss 0.44166583468516324\n",
            "Step 17131, train_loss 0.45394917580298727, test_loss 0.44166546672840645\n",
            "Step 17132, train_loss 0.4539485783418162, test_loss 0.44166509884515914\n",
            "Step 17133, train_loss 0.45394798093827826, test_loss 0.4416647310354064\n",
            "Step 17134, train_loss 0.4539473835923628, test_loss 0.4416643632991333\n",
            "Step 17135, train_loss 0.45394678630405905, test_loss 0.4416639956363247\n",
            "Step 17136, train_loss 0.45394618907335643, test_loss 0.44166362804696585\n",
            "Step 17137, train_loss 0.4539455919002439, test_loss 0.4416632605310417\n",
            "Step 17138, train_loss 0.4539449947847112, test_loss 0.4416628930885373\n",
            "Step 17139, train_loss 0.4539443977267471, test_loss 0.44166252571943765\n",
            "Step 17140, train_loss 0.4539438007263411, test_loss 0.44166215842372797\n",
            "Step 17141, train_loss 0.45394320378348263, test_loss 0.4416617912013932\n",
            "Step 17142, train_loss 0.4539426068981607, test_loss 0.44166142405241837\n",
            "Step 17143, train_loss 0.4539420100703648, test_loss 0.44166105697678865\n",
            "Step 17144, train_loss 0.45394141330008403, test_loss 0.441660689974489\n",
            "Step 17145, train_loss 0.4539408165873078, test_loss 0.4416603230455046\n",
            "Step 17146, train_loss 0.4539402199320255, test_loss 0.44165995618982046\n",
            "Step 17147, train_loss 0.4539396233342262, test_loss 0.4416595894074218\n",
            "Step 17148, train_loss 0.45393902679389947, test_loss 0.4416592226982934\n",
            "Step 17149, train_loss 0.45393843031103437, test_loss 0.44165885606242067\n",
            "Step 17150, train_loss 0.4539378338856203, test_loss 0.44165848949978853\n",
            "Step 17151, train_loss 0.4539372375176466, test_loss 0.44165812301038215\n",
            "Step 17152, train_loss 0.4539366412071026, test_loss 0.4416577565941867\n",
            "Step 17153, train_loss 0.4539360449539776, test_loss 0.441657390251187\n",
            "Step 17154, train_loss 0.45393544875826075, test_loss 0.4416570239813685\n",
            "Step 17155, train_loss 0.45393485261994176, test_loss 0.44165665778471624\n",
            "Step 17156, train_loss 0.4539342565390096, test_loss 0.4416562916612152\n",
            "Step 17157, train_loss 0.4539336605154537, test_loss 0.4416559256108505\n",
            "Step 17158, train_loss 0.4539330645492635, test_loss 0.4416555596336076\n",
            "Step 17159, train_loss 0.45393246864042813, test_loss 0.44165519372947115\n",
            "Step 17160, train_loss 0.4539318727889372, test_loss 0.4416548278984266\n",
            "Step 17161, train_loss 0.45393127699477986, test_loss 0.4416544621404589\n",
            "Step 17162, train_loss 0.4539306812579456, test_loss 0.4416540964555534\n",
            "Step 17163, train_loss 0.45393008557842357, test_loss 0.44165373084369514\n",
            "Step 17164, train_loss 0.45392948995620325, test_loss 0.44165336530486937\n",
            "Step 17165, train_loss 0.45392889439127393, test_loss 0.44165299983906103\n",
            "Step 17166, train_loss 0.45392829888362507, test_loss 0.44165263444625535\n",
            "Step 17167, train_loss 0.45392770343324595, test_loss 0.4416522691264377\n",
            "Step 17168, train_loss 0.453927108040126, test_loss 0.4416519038795929\n",
            "Step 17169, train_loss 0.45392651270425466, test_loss 0.4416515387057064\n",
            "Step 17170, train_loss 0.4539259174256211, test_loss 0.4416511736047633\n",
            "Step 17171, train_loss 0.4539253222042148, test_loss 0.44165080857674877\n",
            "Step 17172, train_loss 0.4539247270400251, test_loss 0.4416504436216479\n",
            "Step 17173, train_loss 0.4539241319330415, test_loss 0.441650078739446\n",
            "Step 17174, train_loss 0.4539235368832532, test_loss 0.4416497139301283\n",
            "Step 17175, train_loss 0.4539229418906497, test_loss 0.4416493491936799\n",
            "Step 17176, train_loss 0.4539223469552204, test_loss 0.4416489845300858\n",
            "Step 17177, train_loss 0.4539217520769547, test_loss 0.44164861993933163\n",
            "Step 17178, train_loss 0.45392115725584187, test_loss 0.44164825542140235\n",
            "Step 17179, train_loss 0.45392056249187157, test_loss 0.44164789097628315\n",
            "Step 17180, train_loss 0.453919967785033, test_loss 0.4416475266039593\n",
            "Step 17181, train_loss 0.45391937313531555, test_loss 0.44164716230441603\n",
            "Step 17182, train_loss 0.4539187785427087, test_loss 0.44164679807763846\n",
            "Step 17183, train_loss 0.45391818400720185, test_loss 0.4416464339236121\n",
            "Step 17184, train_loss 0.4539175895287844, test_loss 0.4416460698423217\n",
            "Step 17185, train_loss 0.4539169951074459, test_loss 0.44164570583375296\n",
            "Step 17186, train_loss 0.4539164007431756, test_loss 0.4416453418978909\n",
            "Step 17187, train_loss 0.4539158064359629, test_loss 0.44164497803472086\n",
            "Step 17188, train_loss 0.4539152121857974, test_loss 0.44164461424422785\n",
            "Step 17189, train_loss 0.45391461799266847, test_loss 0.4416442505263973\n",
            "Step 17190, train_loss 0.45391402385656554, test_loss 0.4416438868812146\n",
            "Step 17191, train_loss 0.45391342977747806, test_loss 0.44164352330866463\n",
            "Step 17192, train_loss 0.45391283575539526, test_loss 0.4416431598087331\n",
            "Step 17193, train_loss 0.453912241790307, test_loss 0.44164279638140486\n",
            "Step 17194, train_loss 0.4539116478822024, test_loss 0.4416424330266654\n",
            "Step 17195, train_loss 0.453911054031071, test_loss 0.44164206974450004\n",
            "Step 17196, train_loss 0.4539104602369023, test_loss 0.4416417065348939\n",
            "Step 17197, train_loss 0.4539098664996857, test_loss 0.44164134339783234\n",
            "Step 17198, train_loss 0.4539092728194107, test_loss 0.4416409803333008\n",
            "Step 17199, train_loss 0.45390867919606664, test_loss 0.4416406173412842\n",
            "Step 17201, train_loss 0.45390749212012976, test_loss 0.4416398915747377\n",
            "Step 17202, train_loss 0.4539068986675157, test_loss 0.4416395288001785\n",
            "Step 17203, train_loss 0.45390630527179066, test_loss 0.4416391660980754\n",
            "Step 17204, train_loss 0.453905711932944, test_loss 0.44163880346841405\n",
            "Step 17205, train_loss 0.4539051186509653, test_loss 0.4416384409111797\n",
            "Step 17206, train_loss 0.45390452542584386, test_loss 0.44163807842635755\n",
            "Step 17207, train_loss 0.4539039322575693, test_loss 0.4416377160139331\n",
            "Step 17208, train_loss 0.4539033391461311, test_loss 0.4416373536738914\n",
            "Step 17209, train_loss 0.45390274609151887, test_loss 0.44163699140621804\n",
            "Step 17210, train_loss 0.45390215309372195, test_loss 0.4416366292108982\n",
            "Step 17211, train_loss 0.4539015601527298, test_loss 0.4416362670879173\n",
            "Step 17212, train_loss 0.4539009672685321, test_loss 0.44163590503726063\n",
            "Step 17213, train_loss 0.45390037444111825, test_loss 0.44163554305891356\n",
            "Step 17214, train_loss 0.4538997816704778, test_loss 0.4416351811528614\n",
            "Step 17215, train_loss 0.45389918895660014, test_loss 0.4416348193190895\n",
            "Step 17216, train_loss 0.45389859629947493, test_loss 0.4416344575575834\n",
            "Step 17217, train_loss 0.4538980036990917, test_loss 0.441634095868328\n",
            "Step 17218, train_loss 0.4538974111554401, test_loss 0.44163373425130925\n",
            "Step 17219, train_loss 0.4538968186685091, test_loss 0.44163337270651204\n",
            "Step 17220, train_loss 0.4538962262382889, test_loss 0.4416330112339219\n",
            "Step 17221, train_loss 0.4538956338647686, test_loss 0.44163264983352435\n",
            "Step 17222, train_loss 0.45389504154793786, test_loss 0.4416322885053046\n",
            "Step 17223, train_loss 0.45389444928778633, test_loss 0.44163192724924805\n",
            "Step 17224, train_loss 0.45389385708430346, test_loss 0.44163156606534\n",
            "Step 17225, train_loss 0.45389326493747867, test_loss 0.44163120495356617\n",
            "Step 17226, train_loss 0.4538926728473017, test_loss 0.44163084391391166\n",
            "Step 17227, train_loss 0.4538920808137621, test_loss 0.4416304829463618\n",
            "Step 17228, train_loss 0.4538914888368493, test_loss 0.44163012205090224\n",
            "Step 17229, train_loss 0.45389089691655304, test_loss 0.44162976122751824\n",
            "Step 17230, train_loss 0.45389030505286254, test_loss 0.44162940047619526\n",
            "Step 17231, train_loss 0.45388971324576766, test_loss 0.44162903979691864\n",
            "Step 17232, train_loss 0.45388912149525795, test_loss 0.44162867918967386\n",
            "Step 17233, train_loss 0.45388852980132294, test_loss 0.44162831865444646\n",
            "Step 17234, train_loss 0.453887938163952, test_loss 0.44162795819122147\n",
            "Step 17235, train_loss 0.453887346583135, test_loss 0.4416275977999849\n",
            "Step 17236, train_loss 0.45388675505886134, test_loss 0.4416272374807216\n",
            "Step 17237, train_loss 0.4538861635911207, test_loss 0.44162687723341737\n",
            "Step 17238, train_loss 0.4538855721799028, test_loss 0.44162651705805744\n",
            "Step 17239, train_loss 0.45388498082519685, test_loss 0.4416261569546274\n",
            "Step 17240, train_loss 0.45388438952699267, test_loss 0.44162579692311277\n",
            "Step 17241, train_loss 0.45388379828527997, test_loss 0.4416254369634987\n",
            "Step 17242, train_loss 0.453883207100048, test_loss 0.44162507707577103\n",
            "Step 17243, train_loss 0.4538826159712867, test_loss 0.4416247172599148\n",
            "Step 17244, train_loss 0.45388202489898555, test_loss 0.4416243575159158\n",
            "Step 17245, train_loss 0.4538814338831342, test_loss 0.44162399784375933\n",
            "Step 17246, train_loss 0.4538808429237221, test_loss 0.4416236382434309\n",
            "Step 17247, train_loss 0.45388025202073906, test_loss 0.44162327871491613\n",
            "Step 17248, train_loss 0.45387966117417455, test_loss 0.44162291925820035\n",
            "Step 17249, train_loss 0.4538790703840184, test_loss 0.44162255987326887\n",
            "Step 17250, train_loss 0.45387847965026007, test_loss 0.44162220056010754\n",
            "Step 17251, train_loss 0.45387788897288905, test_loss 0.4416218413187016\n",
            "Step 17252, train_loss 0.4538772983518951, test_loss 0.4416214821490365\n",
            "Step 17253, train_loss 0.45387670778726796, test_loss 0.44162112305109796\n",
            "Step 17254, train_loss 0.45387611727899724, test_loss 0.4416207640248713\n",
            "Step 17255, train_loss 0.45387552682707255, test_loss 0.4416204050703422\n",
            "Step 17256, train_loss 0.45387493643148336, test_loss 0.441620046187496\n",
            "Step 17257, train_loss 0.45387434609221944, test_loss 0.4416196873763183\n",
            "Step 17258, train_loss 0.4538737558092706, test_loss 0.4416193286367946\n",
            "Step 17259, train_loss 0.45387316558262625, test_loss 0.4416189699689103\n",
            "Step 17260, train_loss 0.45387257541227605, test_loss 0.44161861137265107\n",
            "Step 17261, train_loss 0.4538719852982098, test_loss 0.4416182528480025\n",
            "Step 17262, train_loss 0.4538713952404172, test_loss 0.44161789439495\n",
            "Step 17263, train_loss 0.45387080523888773, test_loss 0.4416175360134789\n",
            "Step 17264, train_loss 0.45387021529361116, test_loss 0.4416171777035751\n",
            "Step 17265, train_loss 0.4538696254045771, test_loss 0.44161681946522413\n",
            "Step 17266, train_loss 0.4538690355717752, test_loss 0.4416164612984113\n",
            "Step 17267, train_loss 0.45386844579519525, test_loss 0.4416161032031223\n",
            "Step 17268, train_loss 0.4538678560748269, test_loss 0.44161574517934254\n",
            "Step 17269, train_loss 0.45386726641065983, test_loss 0.44161538722705784\n",
            "Step 17270, train_loss 0.4538666768026836, test_loss 0.4416150293462535\n",
            "Step 17271, train_loss 0.453866087250888, test_loss 0.4416146715369153\n",
            "Step 17272, train_loss 0.45386549775526275, test_loss 0.44161431379902866\n",
            "Step 17273, train_loss 0.4538649083157975, test_loss 0.44161395613257914\n",
            "Step 17274, train_loss 0.45386431893248197, test_loss 0.4416135985375524\n",
            "Step 17275, train_loss 0.45386372960530574, test_loss 0.441613241013934\n",
            "Step 17276, train_loss 0.45386314033425873, test_loss 0.44161288356170963\n",
            "Step 17277, train_loss 0.45386255111933044, test_loss 0.4416125261808646\n",
            "Step 17278, train_loss 0.4538619619605107, test_loss 0.4416121688713848\n",
            "Step 17279, train_loss 0.4538613728577892, test_loss 0.4416118116332557\n",
            "Step 17280, train_loss 0.4538607838111557, test_loss 0.44161145446646277\n",
            "Step 17281, train_loss 0.4538601948205997, test_loss 0.4416110973709918\n",
            "Step 17282, train_loss 0.4538596058861112, test_loss 0.4416107403468283\n",
            "Step 17283, train_loss 0.4538590170076798, test_loss 0.4416103833939581\n",
            "Step 17284, train_loss 0.4538584281852952, test_loss 0.4416100265123663\n",
            "Step 17285, train_loss 0.4538578394189471, test_loss 0.4416096697020391\n",
            "Step 17286, train_loss 0.4538572507086253, test_loss 0.44160931296296185\n",
            "Step 17287, train_loss 0.4538566620543197, test_loss 0.4416089562951202\n",
            "Step 17288, train_loss 0.45385607345601975, test_loss 0.44160859969849975\n",
            "Step 17289, train_loss 0.45385548491371525, test_loss 0.4416082431730861\n",
            "Step 17290, train_loss 0.4538548964273961, test_loss 0.441607886718865\n",
            "Step 17291, train_loss 0.4538543079970519, test_loss 0.441607530335822\n",
            "Step 17292, train_loss 0.4538537196226725, test_loss 0.4416071740239428\n",
            "Step 17293, train_loss 0.4538531313042476, test_loss 0.44160681778321303\n",
            "Step 17294, train_loss 0.4538525430417669, test_loss 0.44160646161361844\n",
            "Step 17295, train_loss 0.4538519548352203, test_loss 0.4416061055151445\n",
            "Step 17296, train_loss 0.4538513666845974, test_loss 0.4416057494877771\n",
            "Step 17297, train_loss 0.4538507785898881, test_loss 0.4416053935315015\n",
            "Step 17298, train_loss 0.4538501905510821, test_loss 0.4416050376463039\n",
            "Step 17299, train_loss 0.4538496025681693, test_loss 0.4416046818321696\n",
            "Step 17301, train_loss 0.453848426769982, test_loss 0.44160397041703375\n",
            "Step 17302, train_loss 0.45384783895468706, test_loss 0.44160361481600374\n",
            "Step 17303, train_loss 0.4538472511952443, test_loss 0.4416032592859799\n",
            "Step 17304, train_loss 0.45384666349164365, test_loss 0.4416029038269477\n",
            "Step 17305, train_loss 0.45384607584387476, test_loss 0.4416025484388932\n",
            "Step 17306, train_loss 0.45384548825192733, test_loss 0.44160219312180177\n",
            "Step 17307, train_loss 0.4538449007157915, test_loss 0.4416018378756593\n",
            "Step 17308, train_loss 0.4538443132354567, test_loss 0.44160148270045146\n",
            "Step 17309, train_loss 0.4538437258109129, test_loss 0.4416011275961639\n",
            "Step 17310, train_loss 0.45384313844214996, test_loss 0.4416007725627826\n",
            "Step 17311, train_loss 0.45384255112915767, test_loss 0.4416004176002928\n",
            "Step 17312, train_loss 0.45384196387192555, test_loss 0.4416000627086805\n",
            "Step 17313, train_loss 0.45384137667044383, test_loss 0.44159970788793146\n",
            "Step 17314, train_loss 0.45384078952470214, test_loss 0.4415993531380314\n",
            "Step 17315, train_loss 0.4538402024346903, test_loss 0.4415989984589659\n",
            "Step 17316, train_loss 0.4538396154003983, test_loss 0.4415986438507208\n",
            "Step 17317, train_loss 0.45383902842181556, test_loss 0.4415982893132819\n",
            "Step 17318, train_loss 0.45383844149893227, test_loss 0.44159793484663484\n",
            "Step 17319, train_loss 0.45383785463173826, test_loss 0.4415975804507654\n",
            "Step 17320, train_loss 0.45383726782022304, test_loss 0.44159722612565927\n",
            "Step 17321, train_loss 0.4538366810643769, test_loss 0.4415968718713023\n",
            "Step 17322, train_loss 0.45383609436418937, test_loss 0.4415965176876803\n",
            "Step 17323, train_loss 0.45383550771965026, test_loss 0.4415961635747789\n",
            "Step 17324, train_loss 0.45383492113074975, test_loss 0.4415958095325838\n",
            "Step 17325, train_loss 0.4538343345974773, test_loss 0.44159545556108104\n",
            "Step 17326, train_loss 0.4538337481198231, test_loss 0.4415951016602562\n",
            "Step 17327, train_loss 0.4538331616977767, test_loss 0.441594747830095\n",
            "Step 17328, train_loss 0.45383257533132815, test_loss 0.44159439407058326\n",
            "Step 17329, train_loss 0.4538319890204673, test_loss 0.441594040381707\n",
            "Step 17330, train_loss 0.45383140276518386, test_loss 0.4415936867634518\n",
            "Step 17331, train_loss 0.453830816565468, test_loss 0.4415933332158034\n",
            "Step 17332, train_loss 0.4538302304213093, test_loss 0.44159297973874767\n",
            "Step 17333, train_loss 0.45382964433269785, test_loss 0.4415926263322703\n",
            "Step 17334, train_loss 0.4538290582996234, test_loss 0.4415922729963575\n",
            "Step 17335, train_loss 0.4538284723220758, test_loss 0.4415919197309945\n",
            "Step 17336, train_loss 0.45382788640004507, test_loss 0.44159156653616755\n",
            "Step 17337, train_loss 0.4538273005335209, test_loss 0.44159121341186214\n",
            "Step 17338, train_loss 0.4538267147224935, test_loss 0.4415908603580644\n",
            "Step 17339, train_loss 0.4538261289669524, test_loss 0.4415905073747599\n",
            "Step 17340, train_loss 0.4538255432668877, test_loss 0.44159015446193456\n",
            "Step 17341, train_loss 0.45382495762228925, test_loss 0.4415898016195743\n",
            "Step 17342, train_loss 0.4538243720331469, test_loss 0.4415894488476649\n",
            "Step 17343, train_loss 0.45382378649945077, test_loss 0.441589096146192\n",
            "Step 17344, train_loss 0.45382320102119056, test_loss 0.4415887435151416\n",
            "Step 17345, train_loss 0.45382261559835607, test_loss 0.4415883909544997\n",
            "Step 17346, train_loss 0.4538220302309375, test_loss 0.4415880384642518\n",
            "Step 17347, train_loss 0.4538214449189246, test_loss 0.4415876860443842\n",
            "Step 17348, train_loss 0.4538208596623074, test_loss 0.44158733369488234\n",
            "Step 17349, train_loss 0.4538202744610758, test_loss 0.44158698141573216\n",
            "Step 17350, train_loss 0.4538196893152196, test_loss 0.4415866292069197\n",
            "Step 17351, train_loss 0.4538191042247288, test_loss 0.4415862770684307\n",
            "Step 17352, train_loss 0.4538185191895934, test_loss 0.44158592500025107\n",
            "Step 17353, train_loss 0.4538179342098032, test_loss 0.4415855730023667\n",
            "Step 17354, train_loss 0.4538173492853482, test_loss 0.4415852210747633\n",
            "Step 17355, train_loss 0.4538167644162185, test_loss 0.441584869217427\n",
            "Step 17356, train_loss 0.4538161796024036, test_loss 0.44158451743034355\n",
            "Step 17357, train_loss 0.45381559484389405, test_loss 0.4415841657134989\n",
            "Step 17358, train_loss 0.4538150101406794, test_loss 0.4415838140668788\n",
            "Step 17359, train_loss 0.4538144254927496, test_loss 0.4415834624904693\n",
            "Step 17360, train_loss 0.4538138409000947, test_loss 0.4415831109842562\n",
            "Step 17361, train_loss 0.4538132563627048, test_loss 0.44158275954822557\n",
            "Step 17362, train_loss 0.45381267188056956, test_loss 0.44158240818236305\n",
            "Step 17363, train_loss 0.45381208745367924, test_loss 0.4415820568866547\n",
            "Step 17364, train_loss 0.4538115030820235, test_loss 0.44158170566108657\n",
            "Step 17365, train_loss 0.4538109187655927, test_loss 0.44158135450564434\n",
            "Step 17366, train_loss 0.45381033450437636, test_loss 0.441581003420314\n",
            "Step 17367, train_loss 0.45380975029836484, test_loss 0.44158065240508154\n",
            "Step 17368, train_loss 0.45380916614754796, test_loss 0.4415803014599329\n",
            "Step 17369, train_loss 0.4538085820519156, test_loss 0.4415799505848539\n",
            "Step 17370, train_loss 0.453807998011458, test_loss 0.44157959977983047\n",
            "Step 17371, train_loss 0.4538074140261649, test_loss 0.4415792490448488\n",
            "Step 17372, train_loss 0.4538068300960264, test_loss 0.4415788983798945\n",
            "Step 17373, train_loss 0.4538062462210325, test_loss 0.44157854778495365\n",
            "Step 17374, train_loss 0.4538056624011732, test_loss 0.4415781972600123\n",
            "Step 17375, train_loss 0.4538050786364385, test_loss 0.44157784680505635\n",
            "Step 17376, train_loss 0.4538044949268184, test_loss 0.4415774964200717\n",
            "Step 17377, train_loss 0.45380391127230285, test_loss 0.4415771461050444\n",
            "Step 17378, train_loss 0.45380332767288184, test_loss 0.44157679585996024\n",
            "Step 17379, train_loss 0.45380274412854554, test_loss 0.44157644568480536\n",
            "Step 17380, train_loss 0.4538021606392838, test_loss 0.44157609557956573\n",
            "Step 17381, train_loss 0.4538015772050868, test_loss 0.4415757455442272\n",
            "Step 17382, train_loss 0.4538009938259443, test_loss 0.4415753955787759\n",
            "Step 17383, train_loss 0.45380041050184655, test_loss 0.4415750456831977\n",
            "Step 17384, train_loss 0.4537998272327835, test_loss 0.44157469585747866\n",
            "Step 17385, train_loss 0.4537992440187453, test_loss 0.4415743461016047\n",
            "Step 17386, train_loss 0.45379866085972176, test_loss 0.44157399641556194\n",
            "Step 17387, train_loss 0.453798077755703, test_loss 0.4415736467993362\n",
            "Step 17388, train_loss 0.45379749470667924, test_loss 0.4415732972529137\n",
            "Step 17389, train_loss 0.4537969117126402, test_loss 0.4415729477762803\n",
            "Step 17390, train_loss 0.4537963287735762, test_loss 0.4415725983694219\n",
            "Step 17391, train_loss 0.45379574588947713, test_loss 0.4415722490323248\n",
            "Step 17392, train_loss 0.4537951630603331, test_loss 0.4415718997649748\n",
            "Step 17393, train_loss 0.4537945802861342, test_loss 0.44157155056735803\n",
            "Step 17394, train_loss 0.4537939975668703, test_loss 0.44157120143946044\n",
            "Step 17395, train_loss 0.4537934149025317, test_loss 0.441570852381268\n",
            "Step 17396, train_loss 0.45379283229310835, test_loss 0.441570503392767\n",
            "Step 17397, train_loss 0.4537922497385903, test_loss 0.44157015447394315\n",
            "Step 17398, train_loss 0.4537916672389676, test_loss 0.4415698056247827\n",
            "Step 17399, train_loss 0.4537910847942304, test_loss 0.44156945684527166\n",
            "Step 17401, train_loss 0.4537899200693726, test_loss 0.44156875949514185\n",
            "Step 17402, train_loss 0.4537893377892322, test_loss 0.44156841092449534\n",
            "Step 17403, train_loss 0.4537887555639376, test_loss 0.44156806242344226\n",
            "Step 17404, train_loss 0.4537881733934787, test_loss 0.441567713991969\n",
            "Step 17405, train_loss 0.4537875912778459, test_loss 0.44156736563006144\n",
            "Step 17406, train_loss 0.453787009217029, test_loss 0.4415670173377055\n",
            "Step 17407, train_loss 0.45378642721101836, test_loss 0.44156666911488757\n",
            "Step 17408, train_loss 0.4537858452598039, test_loss 0.44156632096159343\n",
            "Step 17409, train_loss 0.4537852633633757, test_loss 0.44156597287780947\n",
            "Step 17410, train_loss 0.4537846815217241, test_loss 0.4415656248635215\n",
            "Step 17411, train_loss 0.4537840997348388, test_loss 0.4415652769187157\n",
            "Step 17412, train_loss 0.4537835180027102, test_loss 0.4415649290433783\n",
            "Step 17413, train_loss 0.4537829363253284, test_loss 0.4415645812374952\n",
            "Step 17414, train_loss 0.4537823547026834, test_loss 0.4415642335010525\n",
            "Step 17415, train_loss 0.45378177313476536, test_loss 0.44156388583403644\n",
            "Step 17416, train_loss 0.45378119162156444, test_loss 0.441563538236433\n",
            "Step 17417, train_loss 0.45378061016307064, test_loss 0.4415631907082284\n",
            "Step 17418, train_loss 0.4537800287592744, test_loss 0.44156284324940864\n",
            "Step 17419, train_loss 0.45377944741016546, test_loss 0.44156249585995994\n",
            "Step 17420, train_loss 0.45377886611573415, test_loss 0.4415621485398683\n",
            "Step 17421, train_loss 0.45377828487597066, test_loss 0.4415618012891199\n",
            "Step 17422, train_loss 0.453777703690865, test_loss 0.4415614541077009\n",
            "Step 17423, train_loss 0.4537771225604073, test_loss 0.4415611069955975\n",
            "Step 17424, train_loss 0.4537765414845878, test_loss 0.44156075995279565\n",
            "Step 17425, train_loss 0.4537759604633966, test_loss 0.4415604129792815\n",
            "Step 17426, train_loss 0.45377537949682384, test_loss 0.44156006607504134\n",
            "Step 17427, train_loss 0.4537747985848597, test_loss 0.44155971924006115\n",
            "Step 17428, train_loss 0.4537742177274943, test_loss 0.4415593724743273\n",
            "Step 17429, train_loss 0.45377363692471784, test_loss 0.44155902577782574\n",
            "Step 17430, train_loss 0.4537730561765204, test_loss 0.44155867915054264\n",
            "Step 17431, train_loss 0.45377247548289223, test_loss 0.4415583325924644\n",
            "Step 17432, train_loss 0.45377189484382346, test_loss 0.4415579861035768\n",
            "Step 17433, train_loss 0.4537713142593043, test_loss 0.44155763968386635\n",
            "Step 17434, train_loss 0.4537707337293248, test_loss 0.44155729333331883\n",
            "Step 17435, train_loss 0.45377015325387526, test_loss 0.441556947051921\n",
            "Step 17436, train_loss 0.45376957283294583, test_loss 0.44155660083965853\n",
            "Step 17437, train_loss 0.45376899246652674, test_loss 0.44155625469651755\n",
            "Step 17438, train_loss 0.45376841215460806, test_loss 0.4415559086224847\n",
            "Step 17439, train_loss 0.45376783189718006, test_loss 0.44155556261754586\n",
            "Step 17440, train_loss 0.4537672516942329, test_loss 0.44155521668168735\n",
            "Step 17441, train_loss 0.4537666715457567, test_loss 0.4415548708148953\n",
            "Step 17442, train_loss 0.45376609145174174, test_loss 0.44155452501715586\n",
            "Step 17443, train_loss 0.45376551141217825, test_loss 0.44155417928845536\n",
            "Step 17444, train_loss 0.4537649314270563, test_loss 0.44155383362877976\n",
            "Step 17445, train_loss 0.4537643514963662, test_loss 0.44155348803811556\n",
            "Step 17446, train_loss 0.4537637716200982, test_loss 0.4415531425164488\n",
            "Step 17447, train_loss 0.4537631917982423, test_loss 0.44155279706376577\n",
            "Step 17448, train_loss 0.453762612030789, test_loss 0.44155245168005275\n",
            "Step 17449, train_loss 0.4537620323177283, test_loss 0.44155210636529574\n",
            "Step 17450, train_loss 0.45376145265905055, test_loss 0.4415517611194812\n",
            "Step 17451, train_loss 0.45376087305474594, test_loss 0.44155141594259517\n",
            "Step 17452, train_loss 0.45376029350480446, test_loss 0.4415510708346241\n",
            "Step 17453, train_loss 0.4537597140092167, test_loss 0.44155072579555404\n",
            "Step 17454, train_loss 0.45375913456797273, test_loss 0.44155038082537124\n",
            "Step 17455, train_loss 0.4537585551810628, test_loss 0.4415500359240621\n",
            "Step 17456, train_loss 0.453757975848477, test_loss 0.44154969109161285\n",
            "Step 17457, train_loss 0.45375739657020575, test_loss 0.4415493463280096\n",
            "Step 17458, train_loss 0.4537568173462393, test_loss 0.4415490016332386\n",
            "Step 17459, train_loss 0.4537562381765678, test_loss 0.44154865700728635\n",
            "Step 17460, train_loss 0.4537556590611814, test_loss 0.44154831245013887\n",
            "Step 17461, train_loss 0.4537550800000707, test_loss 0.44154796796178236\n",
            "Step 17462, train_loss 0.4537545009932255, test_loss 0.44154762354220345\n",
            "Step 17463, train_loss 0.45375392204063636, test_loss 0.44154727919138803\n",
            "Step 17464, train_loss 0.45375334314229354, test_loss 0.44154693490932273\n",
            "Step 17465, train_loss 0.4537527642981872, test_loss 0.4415465906959935\n",
            "Step 17466, train_loss 0.4537521855083076, test_loss 0.44154624655138675\n",
            "Step 17467, train_loss 0.453751606772645, test_loss 0.44154590247548886\n",
            "Step 17468, train_loss 0.4537510280911898, test_loss 0.4415455584682862\n",
            "Step 17469, train_loss 0.453750449463932, test_loss 0.4415452145297648\n",
            "Step 17470, train_loss 0.4537498708908621, test_loss 0.441544870659911\n",
            "Step 17471, train_loss 0.4537492923719704, test_loss 0.4415445268587113\n",
            "Step 17472, train_loss 0.45374871390724714, test_loss 0.44154418312615185\n",
            "Step 17473, train_loss 0.4537481354966825, test_loss 0.4415438394622191\n",
            "Step 17474, train_loss 0.4537475571402668, test_loss 0.44154349586689906\n",
            "Step 17475, train_loss 0.4537469788379905, test_loss 0.44154315234017827\n",
            "Step 17476, train_loss 0.4537464005898436, test_loss 0.44154280888204306\n",
            "Step 17477, train_loss 0.4537458223958167, test_loss 0.44154246549247983\n",
            "Step 17478, train_loss 0.4537452442558998, test_loss 0.44154212217147465\n",
            "Step 17479, train_loss 0.45374466617008347, test_loss 0.44154177891901414\n",
            "Step 17480, train_loss 0.453744088138358, test_loss 0.4415414357350844\n",
            "Step 17481, train_loss 0.4537435101607134, test_loss 0.4415410926196719\n",
            "Step 17482, train_loss 0.45374293223714035, test_loss 0.441540749572763\n",
            "Step 17483, train_loss 0.45374235436762883, test_loss 0.4415404065943439\n",
            "Step 17484, train_loss 0.4537417765521693, test_loss 0.44154006368440096\n",
            "Step 17485, train_loss 0.4537411987907522, test_loss 0.44153972084292087\n",
            "Step 17486, train_loss 0.45374062108336766, test_loss 0.4415393780698895\n",
            "Step 17487, train_loss 0.4537400434300061, test_loss 0.4415390353652936\n",
            "Step 17488, train_loss 0.45373946583065783, test_loss 0.44153869272911944\n",
            "Step 17489, train_loss 0.4537388882853132, test_loss 0.4415383501613532\n",
            "Step 17490, train_loss 0.4537383107939625, test_loss 0.44153800766198126\n",
            "Step 17491, train_loss 0.4537377333565961, test_loss 0.44153766523099014\n",
            "Step 17492, train_loss 0.45373715597320435, test_loss 0.44153732286836633\n",
            "Step 17493, train_loss 0.45373657864377753, test_loss 0.441536980574096\n",
            "Step 17494, train_loss 0.453736001368306, test_loss 0.44153663834816553\n",
            "Step 17495, train_loss 0.45373542414678014, test_loss 0.4415362961905614\n",
            "Step 17496, train_loss 0.45373484697919036, test_loss 0.44153595410127\n",
            "Step 17497, train_loss 0.4537342698655268, test_loss 0.4415356120802777\n",
            "Step 17498, train_loss 0.45373369280578, test_loss 0.4415352701275709\n",
            "Step 17499, train_loss 0.45373311579994036, test_loss 0.44153492824313595\n",
            "Step 17501, train_loss 0.4537319619499436, test_loss 0.44153424467902747\n",
            "Step 17502, train_loss 0.45373138510576716, test_loss 0.44153390299932666\n",
            "Step 17503, train_loss 0.4537308083154594, test_loss 0.4415335613878434\n",
            "Step 17504, train_loss 0.45373023157901055, test_loss 0.4415332198445642\n",
            "Step 17505, train_loss 0.453729654896411, test_loss 0.4415328783694754\n",
            "Step 17506, train_loss 0.45372907826765096, test_loss 0.4415325369625634\n",
            "Step 17507, train_loss 0.4537285016927209, test_loss 0.44153219562381457\n",
            "Step 17508, train_loss 0.45372792517161137, test_loss 0.4415318543532154\n",
            "Step 17509, train_loss 0.45372734870431264, test_loss 0.44153151315075234\n",
            "Step 17510, train_loss 0.4537267722908151, test_loss 0.44153117201641195\n",
            "Step 17511, train_loss 0.453726195931109, test_loss 0.4415308309501805\n",
            "Step 17512, train_loss 0.45372561962518504, test_loss 0.44153048995204464\n",
            "Step 17513, train_loss 0.4537250433730335, test_loss 0.4415301490219905\n",
            "Step 17514, train_loss 0.4537244671746445, test_loss 0.44152980816000476\n",
            "Step 17515, train_loss 0.4537238910300088, test_loss 0.44152946736607385\n",
            "Step 17516, train_loss 0.4537233149391168, test_loss 0.44152912664018423\n",
            "Step 17517, train_loss 0.4537227389019586, test_loss 0.4415287859823223\n",
            "Step 17518, train_loss 0.4537221629185249, test_loss 0.44152844539247466\n",
            "Step 17519, train_loss 0.4537215869888059, test_loss 0.44152810487062766\n",
            "Step 17520, train_loss 0.45372101111279217, test_loss 0.4415277644167679\n",
            "Step 17521, train_loss 0.4537204352904742, test_loss 0.4415274240308817\n",
            "Step 17522, train_loss 0.45371985952184213, test_loss 0.4415270837129556\n",
            "Step 17523, train_loss 0.4537192838068866, test_loss 0.44152674346297616\n",
            "Step 17524, train_loss 0.45371870814559806, test_loss 0.4415264032809298\n",
            "Step 17525, train_loss 0.45371813253796683, test_loss 0.4415260631668031\n",
            "Step 17526, train_loss 0.4537175569839834, test_loss 0.44152572312058236\n",
            "Step 17527, train_loss 0.4537169814836382, test_loss 0.44152538314225437\n",
            "Step 17528, train_loss 0.4537164060369217, test_loss 0.44152504323180536\n",
            "Step 17529, train_loss 0.4537158306438242, test_loss 0.441524703389222\n",
            "Step 17530, train_loss 0.45371525530433626, test_loss 0.4415243636144908\n",
            "Step 17531, train_loss 0.45371468001844834, test_loss 0.4415240239075982\n",
            "Step 17532, train_loss 0.4537141047861508, test_loss 0.4415236842685308\n",
            "Step 17533, train_loss 0.45371352960743433, test_loss 0.441523344697275\n",
            "Step 17534, train_loss 0.45371295448228915, test_loss 0.44152300519381743\n",
            "Step 17535, train_loss 0.4537123794107057, test_loss 0.44152266575814464\n",
            "Step 17536, train_loss 0.4537118043926746, test_loss 0.441522326390243\n",
            "Step 17537, train_loss 0.45371122942818615, test_loss 0.44152198709009927\n",
            "Step 17538, train_loss 0.4537106545172309, test_loss 0.4415216478576998\n",
            "Step 17539, train_loss 0.45371007965979937, test_loss 0.4415213086930312\n",
            "Step 17540, train_loss 0.4537095048558821, test_loss 0.44152096959607995\n",
            "Step 17541, train_loss 0.4537089301054693, test_loss 0.4415206305668327\n",
            "Step 17542, train_loss 0.45370835540855164, test_loss 0.4415202916052761\n",
            "Step 17543, train_loss 0.4537077807651196, test_loss 0.44151995271139655\n",
            "Step 17544, train_loss 0.4537072061751636, test_loss 0.4415196138851806\n",
            "Step 17545, train_loss 0.45370663163867414, test_loss 0.44151927512661493\n",
            "Step 17546, train_loss 0.4537060571556417, test_loss 0.44151893643568596\n",
            "Step 17547, train_loss 0.4537054827260568, test_loss 0.44151859781238034\n",
            "Step 17548, train_loss 0.45370490834990995, test_loss 0.4415182592566846\n",
            "Step 17549, train_loss 0.4537043340271916, test_loss 0.4415179207685855\n",
            "Step 17550, train_loss 0.4537037597578923, test_loss 0.4415175823480693\n",
            "Step 17551, train_loss 0.4537031855420025, test_loss 0.44151724399512293\n",
            "Step 17552, train_loss 0.45370261137951273, test_loss 0.4415169057097327\n",
            "Step 17553, train_loss 0.45370203727041364, test_loss 0.44151656749188534\n",
            "Step 17554, train_loss 0.4537014632146954, test_loss 0.44151622934156737\n",
            "Step 17555, train_loss 0.4537008892123488, test_loss 0.44151589125876556\n",
            "Step 17556, train_loss 0.45370031526336435, test_loss 0.44151555324346636\n",
            "Step 17557, train_loss 0.4536997413677324, test_loss 0.4415152152956563\n",
            "Step 17558, train_loss 0.45369916752544365, test_loss 0.44151487741532214\n",
            "Step 17559, train_loss 0.4536985937364885, test_loss 0.44151453960245046\n",
            "Step 17560, train_loss 0.4536980200008576, test_loss 0.4415142018570279\n",
            "Step 17561, train_loss 0.45369744631854125, test_loss 0.44151386417904104\n",
            "Step 17562, train_loss 0.45369687268953035, test_loss 0.4415135265684764\n",
            "Step 17563, train_loss 0.45369629911381515, test_loss 0.4415131890253208\n",
            "Step 17564, train_loss 0.45369572559138627, test_loss 0.44151285154956077\n",
            "Step 17565, train_loss 0.45369515212223427, test_loss 0.4415125141411829\n",
            "Step 17566, train_loss 0.45369457870634955, test_loss 0.4415121768001739\n",
            "Step 17567, train_loss 0.45369400534372284, test_loss 0.4415118395265204\n",
            "Step 17568, train_loss 0.4536934320343447, test_loss 0.441511502320209\n",
            "Step 17569, train_loss 0.4536928587782056, test_loss 0.44151116518122646\n",
            "Step 17570, train_loss 0.4536922855752961, test_loss 0.4415108281095593\n",
            "Step 17571, train_loss 0.45369171242560674, test_loss 0.44151049110519425\n",
            "Step 17572, train_loss 0.45369113932912813, test_loss 0.44151015416811795\n",
            "Step 17573, train_loss 0.4536905662858508, test_loss 0.44150981729831706\n",
            "Step 17574, train_loss 0.45368999329576537, test_loss 0.44150948049577826\n",
            "Step 17575, train_loss 0.45368942035886234, test_loss 0.44150914376048805\n",
            "Step 17576, train_loss 0.4536888474751323, test_loss 0.44150880709243345\n",
            "Step 17577, train_loss 0.45368827464456585, test_loss 0.4415084704916008\n",
            "Step 17578, train_loss 0.45368770186715357, test_loss 0.44150813395797683\n",
            "Step 17579, train_loss 0.45368712914288595, test_loss 0.44150779749154856\n",
            "Step 17580, train_loss 0.45368655647175377, test_loss 0.44150746109230227\n",
            "Step 17581, train_loss 0.4536859838537474, test_loss 0.4415071247602247\n",
            "Step 17582, train_loss 0.4536854112888576, test_loss 0.4415067884953027\n",
            "Step 17583, train_loss 0.4536848387770747, test_loss 0.44150645229752306\n",
            "Step 17584, train_loss 0.45368426631838976, test_loss 0.4415061161668721\n",
            "Step 17585, train_loss 0.45368369391279284, test_loss 0.44150578010333685\n",
            "Step 17586, train_loss 0.45368312156027496, test_loss 0.4415054441069039\n",
            "Step 17587, train_loss 0.4536825492608265, test_loss 0.44150510817756006\n",
            "Step 17588, train_loss 0.4536819770144381, test_loss 0.44150477231529184\n",
            "Step 17589, train_loss 0.45368140482110036, test_loss 0.441504436520086\n",
            "Step 17590, train_loss 0.45368083268080395, test_loss 0.44150410079192953\n",
            "Step 17591, train_loss 0.4536802605935395, test_loss 0.4415037651308087\n",
            "Step 17592, train_loss 0.4536796885592975, test_loss 0.44150342953671073\n",
            "Step 17593, train_loss 0.45367911657806864, test_loss 0.44150309400962207\n",
            "Step 17594, train_loss 0.4536785446498437, test_loss 0.4415027585495294\n",
            "Step 17595, train_loss 0.45367797277461297, test_loss 0.4415024231564196\n",
            "Step 17596, train_loss 0.45367740095236736, test_loss 0.4415020878302793\n",
            "Step 17597, train_loss 0.4536768291830973, test_loss 0.44150175257109536\n",
            "Step 17598, train_loss 0.4536762574667936, test_loss 0.44150141737885434\n",
            "Step 17599, train_loss 0.45367568580344686, test_loss 0.44150108225354323\n",
            "Step 17601, train_loss 0.4536745426355865, test_loss 0.4415004122036573\n",
            "Step 17602, train_loss 0.45367397113105423, test_loss 0.4415000772790562\n",
            "Step 17603, train_loss 0.45367339967944154, test_loss 0.44149974242133183\n",
            "Step 17604, train_loss 0.4536728282807389, test_loss 0.4414994076304711\n",
            "Step 17605, train_loss 0.4536722569349372, test_loss 0.4414990729064607\n",
            "Step 17606, train_loss 0.45367168564202676, test_loss 0.44149873824928737\n",
            "Step 17607, train_loss 0.45367111440199837, test_loss 0.44149840365893805\n",
            "Step 17608, train_loss 0.45367054321484285, test_loss 0.4414980691353995\n",
            "Step 17609, train_loss 0.45366997208055077, test_loss 0.4414977346786584\n",
            "Step 17610, train_loss 0.45366940099911274, test_loss 0.4414974002887016\n",
            "Step 17611, train_loss 0.4536688299705194, test_loss 0.44149706596551574\n",
            "Step 17612, train_loss 0.4536682589947615, test_loss 0.4414967317090878\n",
            "Step 17613, train_loss 0.4536676880718297, test_loss 0.44149639751940456\n",
            "Step 17614, train_loss 0.4536671172017146, test_loss 0.4414960633964528\n",
            "Step 17615, train_loss 0.453666546384407, test_loss 0.44149572934021936\n",
            "Step 17616, train_loss 0.4536659756198975, test_loss 0.44149539535069093\n",
            "Step 17617, train_loss 0.4536654049081769, test_loss 0.4414950614278543\n",
            "Step 17618, train_loss 0.4536648342492358, test_loss 0.4414947275716966\n",
            "Step 17619, train_loss 0.45366426364306484, test_loss 0.44149439378220423\n",
            "Step 17620, train_loss 0.45366369308965465, test_loss 0.4414940600593643\n",
            "Step 17621, train_loss 0.45366312258899616, test_loss 0.4414937264031634\n",
            "Step 17622, train_loss 0.4536625521410799, test_loss 0.44149339281358874\n",
            "Step 17623, train_loss 0.45366198174589667, test_loss 0.44149305929062677\n",
            "Step 17624, train_loss 0.45366141140343696, test_loss 0.44149272583426447\n",
            "Step 17625, train_loss 0.4536608411136918, test_loss 0.4414923924444887\n",
            "Step 17626, train_loss 0.4536602708766517, test_loss 0.44149205912128636\n",
            "Step 17627, train_loss 0.4536597006923073, test_loss 0.4414917258646441\n",
            "Step 17628, train_loss 0.4536591305606495, test_loss 0.4414913926745489\n",
            "Step 17629, train_loss 0.4536585604816688, test_loss 0.44149105955098766\n",
            "Step 17630, train_loss 0.45365799045535626, test_loss 0.4414907264939472\n",
            "Step 17631, train_loss 0.45365742048170216, test_loss 0.4414903935034142\n",
            "Step 17632, train_loss 0.45365685056069754, test_loss 0.4414900605793758\n",
            "Step 17633, train_loss 0.4536562806923331, test_loss 0.4414897277218187\n",
            "Step 17634, train_loss 0.45365571087659945, test_loss 0.44148939493072986\n",
            "Step 17635, train_loss 0.45365514111348737, test_loss 0.4414890622060961\n",
            "Step 17636, train_loss 0.45365457140298765, test_loss 0.4414887295479042\n",
            "Step 17637, train_loss 0.45365400174509096, test_loss 0.4414883969561412\n",
            "Step 17638, train_loss 0.45365343213978804, test_loss 0.44148806443079414\n",
            "Step 17639, train_loss 0.45365286258706977, test_loss 0.4414877319718494\n",
            "Step 17640, train_loss 0.4536522930869267, test_loss 0.4414873995792943\n",
            "Step 17641, train_loss 0.4536517236393496, test_loss 0.4414870672531156\n",
            "Step 17642, train_loss 0.4536511542443294, test_loss 0.4414867349933001\n",
            "Step 17643, train_loss 0.45365058490185667, test_loss 0.44148640279983487\n",
            "Step 17644, train_loss 0.4536500156119223, test_loss 0.44148607067270673\n",
            "Step 17645, train_loss 0.4536494463745169, test_loss 0.4414857386119026\n",
            "Step 17646, train_loss 0.45364887718963137, test_loss 0.4414854066174093\n",
            "Step 17647, train_loss 0.45364830805725637, test_loss 0.44148507468921394\n",
            "Step 17648, train_loss 0.4536477389773828, test_loss 0.4414847428273033\n",
            "Step 17649, train_loss 0.4536471699500013, test_loss 0.4414844110316644\n",
            "Step 17650, train_loss 0.45364660097510273, test_loss 0.44148407930228395\n",
            "Step 17651, train_loss 0.4536460320526778, test_loss 0.44148374763914905\n",
            "Step 17652, train_loss 0.4536454631827173, test_loss 0.4414834160422466\n",
            "Step 17653, train_loss 0.453644894365212, test_loss 0.44148308451156365\n",
            "Step 17654, train_loss 0.45364432560015266, test_loss 0.44148275304708695\n",
            "Step 17655, train_loss 0.4536437568875302, test_loss 0.4414824216488035\n",
            "Step 17656, train_loss 0.45364318822733546, test_loss 0.4414820903167001\n",
            "Step 17657, train_loss 0.4536426196195589, test_loss 0.4414817590507642\n",
            "Step 17658, train_loss 0.4536420510641916, test_loss 0.4414814278509821\n",
            "Step 17659, train_loss 0.4536414825612242, test_loss 0.44148109671734115\n",
            "Step 17660, train_loss 0.4536409141106476, test_loss 0.44148076564982824\n",
            "Step 17661, train_loss 0.45364034571245265, test_loss 0.4414804346484304\n",
            "Step 17662, train_loss 0.4536397773666299, test_loss 0.4414801037131343\n",
            "Step 17663, train_loss 0.45363920907317046, test_loss 0.44147977284392714\n",
            "Step 17664, train_loss 0.453638640832065, test_loss 0.4414794420407959\n",
            "Step 17665, train_loss 0.45363807264330425, test_loss 0.4414791113037276\n",
            "Step 17666, train_loss 0.45363750450687923, test_loss 0.4414787806327091\n",
            "Step 17667, train_loss 0.4536369364227806, test_loss 0.4414784500277273\n",
            "Step 17668, train_loss 0.4536363683909992, test_loss 0.4414781194887693\n",
            "Step 17669, train_loss 0.45363580041152596, test_loss 0.4414777890158222\n",
            "Step 17670, train_loss 0.4536352324843516, test_loss 0.4414774586088727\n",
            "Step 17671, train_loss 0.45363466460946694, test_loss 0.44147712826790814\n",
            "Step 17672, train_loss 0.4536340967868629, test_loss 0.4414767979929152\n",
            "Step 17673, train_loss 0.45363352901653026, test_loss 0.4414764677838812\n",
            "Step 17674, train_loss 0.4536329612984598, test_loss 0.441476137640793\n",
            "Step 17675, train_loss 0.45363239363264235, test_loss 0.44147580756363736\n",
            "Step 17676, train_loss 0.453631826019069, test_loss 0.4414754775524017\n",
            "Step 17677, train_loss 0.45363125845773034, test_loss 0.4414751476070726\n",
            "Step 17678, train_loss 0.45363069094861724, test_loss 0.44147481772763747\n",
            "Step 17679, train_loss 0.4536301234917206, test_loss 0.44147448791408334\n",
            "Step 17680, train_loss 0.4536295560870313, test_loss 0.44147415816639696\n",
            "Step 17681, train_loss 0.4536289887345402, test_loss 0.4414738284845654\n",
            "Step 17682, train_loss 0.4536284214342381, test_loss 0.44147349886857584\n",
            "Step 17683, train_loss 0.45362785418611595, test_loss 0.4414731693184152\n",
            "Step 17684, train_loss 0.4536272869901645, test_loss 0.4414728398340706\n",
            "Step 17685, train_loss 0.45362671984637465, test_loss 0.441472510415529\n",
            "Step 17686, train_loss 0.4536261527547374, test_loss 0.44147218106277747\n",
            "Step 17687, train_loss 0.45362558571524336, test_loss 0.4414718517758033\n",
            "Step 17688, train_loss 0.45362501872788363, test_loss 0.441471522554593\n",
            "Step 17689, train_loss 0.45362445179264893, test_loss 0.44147119339913404\n",
            "Step 17690, train_loss 0.45362388490953026, test_loss 0.4414708643094134\n",
            "Step 17691, train_loss 0.45362331807851847, test_loss 0.44147053528541813\n",
            "Step 17692, train_loss 0.45362275129960444, test_loss 0.4414702063271353\n",
            "Step 17693, train_loss 0.4536221845727791, test_loss 0.4414698774345519\n",
            "Step 17694, train_loss 0.4536216178980333, test_loss 0.4414695486076551\n",
            "Step 17695, train_loss 0.4536210512753578, test_loss 0.441469219846432\n",
            "Step 17696, train_loss 0.45362048470474364, test_loss 0.44146889115086957\n",
            "Step 17697, train_loss 0.4536199181861817, test_loss 0.4414685625209549\n",
            "Step 17698, train_loss 0.4536193517196629, test_loss 0.4414682339566751\n",
            "Step 17699, train_loss 0.45361878530517813, test_loss 0.44146790545801734\n",
            "Step 17701, train_loss 0.45361765263227416, test_loss 0.44146724865751596\n",
            "Step 17702, train_loss 0.4536170863738369, test_loss 0.4414669203556466\n",
            "Step 17703, train_loss 0.4536165201673973, test_loss 0.4414665921193476\n",
            "Step 17704, train_loss 0.45361595401294624, test_loss 0.44146626394860616\n",
            "Step 17705, train_loss 0.4536153879104747, test_loss 0.44146593584340915\n",
            "Step 17706, train_loss 0.45361482185997354, test_loss 0.4414656078037439\n",
            "Step 17707, train_loss 0.4536142558614337, test_loss 0.44146527982959743\n",
            "Step 17708, train_loss 0.45361368991484613, test_loss 0.4414649519209568\n",
            "Step 17709, train_loss 0.45361312402020176, test_loss 0.44146462407780923\n",
            "Step 17710, train_loss 0.4536125581774915, test_loss 0.44146429630014183\n",
            "Step 17711, train_loss 0.4536119923867063, test_loss 0.44146396858794174\n",
            "Step 17712, train_loss 0.45361142664783705, test_loss 0.44146364094119606\n",
            "Step 17713, train_loss 0.4536108609608747, test_loss 0.4414633133598918\n",
            "Step 17714, train_loss 0.4536102953258104, test_loss 0.44146298584401633\n",
            "Step 17715, train_loss 0.4536097297426348, test_loss 0.44146265839355664\n",
            "Step 17716, train_loss 0.453609164211339, test_loss 0.4414623310085\n",
            "Step 17717, train_loss 0.4536085987319138, test_loss 0.4414620036888334\n",
            "Step 17718, train_loss 0.45360803330435046, test_loss 0.44146167643454415\n",
            "Step 17719, train_loss 0.45360746792863965, test_loss 0.4414613492456192\n",
            "Step 17720, train_loss 0.4536069026047723, test_loss 0.44146102212204585\n",
            "Step 17721, train_loss 0.4536063373327397, test_loss 0.4414606950638113\n",
            "Step 17722, train_loss 0.4536057721125326, test_loss 0.44146036807090255\n",
            "Step 17723, train_loss 0.45360520694414186, test_loss 0.44146004114330695\n",
            "Step 17724, train_loss 0.45360464182755866, test_loss 0.44145971428101166\n",
            "Step 17725, train_loss 0.45360407676277376, test_loss 0.4414593874840037\n",
            "Step 17726, train_loss 0.4536035117497783, test_loss 0.4414590607522704\n",
            "Step 17727, train_loss 0.45360294678856317, test_loss 0.44145873408579867\n",
            "Step 17728, train_loss 0.45360238187911944, test_loss 0.44145840748457604\n",
            "Step 17729, train_loss 0.453601817021438, test_loss 0.4414580809485896\n",
            "Step 17730, train_loss 0.45360125221550984, test_loss 0.4414577544778264\n",
            "Step 17731, train_loss 0.4536006874613259, test_loss 0.4414574280722738\n",
            "Step 17732, train_loss 0.45360012275887734, test_loss 0.4414571017319189\n",
            "Step 17733, train_loss 0.4535995581081551, test_loss 0.44145677545674894\n",
            "Step 17734, train_loss 0.45359899350915006, test_loss 0.44145644924675104\n",
            "Step 17735, train_loss 0.4535984289618533, test_loss 0.4414561231019124\n",
            "Step 17736, train_loss 0.45359786446625583, test_loss 0.4414557970222205\n",
            "Step 17737, train_loss 0.4535973000223486, test_loss 0.44145547100766236\n",
            "Step 17738, train_loss 0.4535967356301226, test_loss 0.44145514505822503\n",
            "Step 17739, train_loss 0.4535961712895689, test_loss 0.44145481917389595\n",
            "Step 17740, train_loss 0.4535956070006785, test_loss 0.4414544933546623\n",
            "Step 17741, train_loss 0.4535950427634423, test_loss 0.44145416760051126\n",
            "Step 17742, train_loss 0.45359447857785157, test_loss 0.4414538419114302\n",
            "Step 17743, train_loss 0.45359391444389713, test_loss 0.44145351628740603\n",
            "Step 17744, train_loss 0.45359335036157, test_loss 0.4414531907284263\n",
            "Step 17745, train_loss 0.45359278633086125, test_loss 0.4414528652344782\n",
            "Step 17746, train_loss 0.453592222351762, test_loss 0.4414525398055489\n",
            "Step 17747, train_loss 0.4535916584242631, test_loss 0.44145221444162563\n",
            "Step 17748, train_loss 0.45359109454835567, test_loss 0.4414518891426958\n",
            "Step 17749, train_loss 0.45359053072403066, test_loss 0.44145156390874646\n",
            "Step 17750, train_loss 0.4535899669512793, test_loss 0.441451238739765\n",
            "Step 17751, train_loss 0.4535894032300924, test_loss 0.4414509136357384\n",
            "Step 17752, train_loss 0.4535888395604612, test_loss 0.44145058859665437\n",
            "Step 17753, train_loss 0.4535882759423766, test_loss 0.4414502636224999\n",
            "Step 17754, train_loss 0.4535877123758296, test_loss 0.4414499387132624\n",
            "Step 17755, train_loss 0.45358714886081153, test_loss 0.44144961386892884\n",
            "Step 17756, train_loss 0.4535865853973132, test_loss 0.4414492890894869\n",
            "Step 17757, train_loss 0.4535860219853257, test_loss 0.44144896437492354\n",
            "Step 17758, train_loss 0.45358545862484, test_loss 0.44144863972522624\n",
            "Step 17759, train_loss 0.4535848953158474, test_loss 0.4414483151403823\n",
            "Step 17760, train_loss 0.453584332058339, test_loss 0.44144799062037876\n",
            "Step 17761, train_loss 0.45358376885230545, test_loss 0.44144766616520315\n",
            "Step 17762, train_loss 0.453583205697738, test_loss 0.4414473417748427\n",
            "Step 17763, train_loss 0.453582642594628, test_loss 0.4414470174492846\n",
            "Step 17764, train_loss 0.4535820795429661, test_loss 0.44144669318851637\n",
            "Step 17765, train_loss 0.4535815165427437, test_loss 0.44144636899252526\n",
            "Step 17766, train_loss 0.45358095359395173, test_loss 0.44144604486129835\n",
            "Step 17767, train_loss 0.4535803906965813, test_loss 0.4414457207948232\n",
            "Step 17768, train_loss 0.45357982785062345, test_loss 0.441445396793087\n",
            "Step 17769, train_loss 0.4535792650560693, test_loss 0.44144507285607704\n",
            "Step 17770, train_loss 0.45357870231291, test_loss 0.4414447489837808\n",
            "Step 17771, train_loss 0.45357813962113647, test_loss 0.44144442517618554\n",
            "Step 17772, train_loss 0.45357757698074, test_loss 0.4414441014332785\n",
            "Step 17773, train_loss 0.4535770143917116, test_loss 0.44144377775504706\n",
            "Step 17774, train_loss 0.45357645185404233, test_loss 0.44144345414147856\n",
            "Step 17775, train_loss 0.45357588936772325, test_loss 0.4414431305925604\n",
            "Step 17776, train_loss 0.45357532693274566, test_loss 0.4414428071082798\n",
            "Step 17777, train_loss 0.4535747645491005, test_loss 0.44144248368862427\n",
            "Step 17778, train_loss 0.4535742022167789, test_loss 0.44144216033358097\n",
            "Step 17779, train_loss 0.45357363993577204, test_loss 0.4414418370431372\n",
            "Step 17780, train_loss 0.45357307770607086, test_loss 0.44144151381728064\n",
            "Step 17781, train_loss 0.4535725155276667, test_loss 0.4414411906559984\n",
            "Step 17782, train_loss 0.4535719534005505, test_loss 0.44144086755927786\n",
            "Step 17783, train_loss 0.4535713913247136, test_loss 0.44144054452710646\n",
            "Step 17784, train_loss 0.4535708293001468, test_loss 0.4414402215594715\n",
            "Step 17785, train_loss 0.45357026732684147, test_loss 0.4414398986563604\n",
            "Step 17786, train_loss 0.4535697054047887, test_loss 0.44143957581776044\n",
            "Step 17787, train_loss 0.4535691435339796, test_loss 0.441439253043659\n",
            "Step 17788, train_loss 0.4535685817144053, test_loss 0.4414389303340437\n",
            "Step 17789, train_loss 0.45356801994605683, test_loss 0.4414386076889017\n",
            "Step 17790, train_loss 0.4535674582289255, test_loss 0.4414382851082202\n",
            "Step 17791, train_loss 0.4535668965630025, test_loss 0.4414379625919871\n",
            "Step 17792, train_loss 0.45356633494827875, test_loss 0.44143764014018927\n",
            "Step 17793, train_loss 0.4535657733847454, test_loss 0.4414373177528145\n",
            "Step 17794, train_loss 0.45356521187239396, test_loss 0.4414369954298501\n",
            "Step 17795, train_loss 0.453564650411215, test_loss 0.44143667317128327\n",
            "Step 17796, train_loss 0.4535640890012002, test_loss 0.4414363509771016\n",
            "Step 17797, train_loss 0.4535635276423405, test_loss 0.44143602884729244\n",
            "Step 17798, train_loss 0.453562966334627, test_loss 0.4414357067818432\n",
            "Step 17799, train_loss 0.453562405078051, test_loss 0.44143538478074124\n",
            "Step 17801, train_loss 0.45356128271827584, test_loss 0.4414347409715291\n",
            "Step 17802, train_loss 0.4535607216150591, test_loss 0.4414344191633938\n",
            "Step 17803, train_loss 0.45356016056294457, test_loss 0.4414340974195554\n",
            "Step 17804, train_loss 0.45355959956192315, test_loss 0.44143377574000153\n",
            "Step 17805, train_loss 0.45355903861198626, test_loss 0.4414334541247196\n",
            "Step 17806, train_loss 0.453558477713125, test_loss 0.44143313257369704\n",
            "Step 17807, train_loss 0.45355791686533053, test_loss 0.4414328110869212\n",
            "Step 17808, train_loss 0.45355735606859404, test_loss 0.4414324896643796\n",
            "Step 17809, train_loss 0.4535567953229068, test_loss 0.4414321683060596\n",
            "Step 17810, train_loss 0.45355623462825995, test_loss 0.4414318470119487\n",
            "Step 17811, train_loss 0.4535556739846445, test_loss 0.44143152578203443\n",
            "Step 17812, train_loss 0.453555113392052, test_loss 0.4414312046163042\n",
            "Step 17813, train_loss 0.45355455285047347, test_loss 0.44143088351474546\n",
            "Step 17814, train_loss 0.4535539923599, test_loss 0.44143056247734563\n",
            "Step 17815, train_loss 0.4535534319203229, test_loss 0.44143024150409216\n",
            "Step 17816, train_loss 0.45355287153173335, test_loss 0.4414299205949726\n",
            "Step 17817, train_loss 0.45355231119412254, test_loss 0.44142959974997453\n",
            "Step 17818, train_loss 0.45355175090748184, test_loss 0.44142927896908507\n",
            "Step 17819, train_loss 0.4535511906718023, test_loss 0.44142895825229217\n",
            "Step 17820, train_loss 0.4535506304870751, test_loss 0.4414286375995829\n",
            "Step 17821, train_loss 0.45355007035329153, test_loss 0.44142831701094487\n",
            "Step 17822, train_loss 0.45354951027044277, test_loss 0.4414279964863657\n",
            "Step 17823, train_loss 0.45354895023852015, test_loss 0.4414276760258328\n",
            "Step 17824, train_loss 0.45354839025751476, test_loss 0.4414273556293336\n",
            "Step 17825, train_loss 0.4535478303274179, test_loss 0.4414270352968557\n",
            "Step 17826, train_loss 0.45354727044822085, test_loss 0.4414267150283865\n",
            "Step 17827, train_loss 0.45354671061991475, test_loss 0.44142639482391366\n",
            "Step 17828, train_loss 0.4535461508424909, test_loss 0.44142607468342443\n",
            "Step 17829, train_loss 0.45354559111594045, test_loss 0.44142575460690664\n",
            "Step 17830, train_loss 0.45354503144025465, test_loss 0.44142543459434747\n",
            "Step 17831, train_loss 0.45354447181542484, test_loss 0.44142511464573475\n",
            "Step 17832, train_loss 0.4535439122414422, test_loss 0.4414247947610558\n",
            "Step 17833, train_loss 0.45354335271829793, test_loss 0.44142447494029813\n",
            "Step 17834, train_loss 0.45354279324598346, test_loss 0.4414241551834493\n",
            "Step 17835, train_loss 0.4535422338244898, test_loss 0.441423835490497\n",
            "Step 17836, train_loss 0.4535416744538084, test_loss 0.44142351586142864\n",
            "Step 17837, train_loss 0.4535411151339304, test_loss 0.4414231962962317\n",
            "Step 17838, train_loss 0.45354055586484704, test_loss 0.44142287679489384\n",
            "Step 17839, train_loss 0.4535399966465498, test_loss 0.4414225573574024\n",
            "Step 17840, train_loss 0.45353943747902964, test_loss 0.44142223798374514\n",
            "Step 17841, train_loss 0.45353887836227796, test_loss 0.44142191867390973\n",
            "Step 17842, train_loss 0.4535383192962862, test_loss 0.4414215994278832\n",
            "Step 17843, train_loss 0.4535377602810453, test_loss 0.4414212802456536\n",
            "Step 17844, train_loss 0.45353720131654685, test_loss 0.44142096112720836\n",
            "Step 17845, train_loss 0.4535366424027819, test_loss 0.44142064207253484\n",
            "Step 17846, train_loss 0.4535360835397419, test_loss 0.4414203230816208\n",
            "Step 17847, train_loss 0.453535524727418, test_loss 0.44142000415445387\n",
            "Step 17848, train_loss 0.4535349659658015, test_loss 0.44141968529102144\n",
            "Step 17849, train_loss 0.45353440725488375, test_loss 0.4414193664913113\n",
            "Step 17850, train_loss 0.4535338485946561, test_loss 0.4414190477553108\n",
            "Step 17851, train_loss 0.45353328998510967, test_loss 0.44141872908300755\n",
            "Step 17852, train_loss 0.45353273142623574, test_loss 0.4414184104743893\n",
            "Step 17853, train_loss 0.4535321729180259, test_loss 0.4414180919294434\n",
            "Step 17854, train_loss 0.45353161446047113, test_loss 0.44141777344815786\n",
            "Step 17855, train_loss 0.4535310560535628, test_loss 0.4414174550305197\n",
            "Step 17856, train_loss 0.4535304976972925, test_loss 0.4414171366765169\n",
            "Step 17857, train_loss 0.4535299393916511, test_loss 0.441416818386137\n",
            "Step 17858, train_loss 0.4535293811366302, test_loss 0.4414165001593675\n",
            "Step 17859, train_loss 0.4535288229322211, test_loss 0.4414161819961961\n",
            "Step 17860, train_loss 0.4535282647784149, test_loss 0.4414158638966104\n",
            "Step 17861, train_loss 0.4535277066752032, test_loss 0.44141554586059806\n",
            "Step 17862, train_loss 0.4535271486225771, test_loss 0.44141522788814663\n",
            "Step 17863, train_loss 0.4535265906205281, test_loss 0.4414149099792436\n",
            "Step 17864, train_loss 0.4535260326690473, test_loss 0.44141459213387685\n",
            "Step 17865, train_loss 0.4535254747681262, test_loss 0.4414142743520339\n",
            "Step 17866, train_loss 0.4535249169177562, test_loss 0.44141395663370214\n",
            "Step 17867, train_loss 0.4535243591179283, test_loss 0.4414136389788696\n",
            "Step 17868, train_loss 0.4535238013686343, test_loss 0.4414133213875237\n",
            "Step 17869, train_loss 0.45352324366986513, test_loss 0.441413003859652\n",
            "Step 17870, train_loss 0.45352268602161233, test_loss 0.44141268639524234\n",
            "Step 17871, train_loss 0.45352212842386724, test_loss 0.44141236899428227\n",
            "Step 17872, train_loss 0.45352157087662115, test_loss 0.4414120516567594\n",
            "Step 17873, train_loss 0.45352101337986533, test_loss 0.4414117343826615\n",
            "Step 17874, train_loss 0.4535204559335913, test_loss 0.4414114171719761\n",
            "Step 17875, train_loss 0.4535198985377904, test_loss 0.44141110002469086\n",
            "Step 17876, train_loss 0.4535193411924538, test_loss 0.44141078294079356\n",
            "Step 17877, train_loss 0.4535187838975731, test_loss 0.4414104659202717\n",
            "Step 17878, train_loss 0.4535182266531394, test_loss 0.44141014896311315\n",
            "Step 17879, train_loss 0.45351766945914435, test_loss 0.4414098320693053\n",
            "Step 17880, train_loss 0.4535171123155791, test_loss 0.4414095152388361\n",
            "Step 17881, train_loss 0.45351655522243506, test_loss 0.4414091984716929\n",
            "Step 17882, train_loss 0.4535159981797036, test_loss 0.4414088817678638\n",
            "Step 17883, train_loss 0.45351544118737613, test_loss 0.44140856512733606\n",
            "Step 17884, train_loss 0.453514884245444, test_loss 0.4414082485500978\n",
            "Step 17885, train_loss 0.45351432735389857, test_loss 0.44140793203613626\n",
            "Step 17886, train_loss 0.4535137705127313, test_loss 0.44140761558543945\n",
            "Step 17887, train_loss 0.45351321372193354, test_loss 0.44140729919799493\n",
            "Step 17888, train_loss 0.45351265698149656, test_loss 0.44140698287379054\n",
            "Step 17889, train_loss 0.45351210029141187, test_loss 0.4414066666128137\n",
            "Step 17890, train_loss 0.4535115436516708, test_loss 0.44140635041505233\n",
            "Step 17891, train_loss 0.4535109870622647, test_loss 0.4414060342804942\n",
            "Step 17892, train_loss 0.4535104305231852, test_loss 0.44140571820912683\n",
            "Step 17893, train_loss 0.4535098740344234, test_loss 0.44140540220093794\n",
            "Step 17894, train_loss 0.4535093175959708, test_loss 0.4414050862559154\n",
            "Step 17895, train_loss 0.4535087612078189, test_loss 0.44140477037404674\n",
            "Step 17896, train_loss 0.453508204869959, test_loss 0.4414044545553198\n",
            "Step 17897, train_loss 0.45350764858238257, test_loss 0.44140413879972246\n",
            "Step 17898, train_loss 0.4535070923450809, test_loss 0.44140382310724213\n",
            "Step 17899, train_loss 0.45350653615804554, test_loss 0.44140350747786666\n",
            "Step 17901, train_loss 0.45350542393473914, test_loss 0.4414028764083815\n",
            "Step 17902, train_loss 0.45350486789845107, test_loss 0.4414025609682472\n",
            "Step 17903, train_loss 0.4535043119123949, test_loss 0.44140224559116864\n",
            "Step 17904, train_loss 0.45350375597656195, test_loss 0.4414019302771337\n",
            "Step 17905, train_loss 0.45350320009094397, test_loss 0.4414016150261302\n",
            "Step 17906, train_loss 0.453502644255532, test_loss 0.4414012998381458\n",
            "Step 17907, train_loss 0.4535020884703178, test_loss 0.4414009847131682\n",
            "Step 17908, train_loss 0.45350153273529253, test_loss 0.44140066965118524\n",
            "Step 17909, train_loss 0.45350097705044784, test_loss 0.44140035465218463\n",
            "Step 17910, train_loss 0.4535004214157751, test_loss 0.44140003971615416\n",
            "Step 17911, train_loss 0.45349986583126567, test_loss 0.4413997248430816\n",
            "Step 17912, train_loss 0.45349931029691115, test_loss 0.4413994100329547\n",
            "Step 17913, train_loss 0.45349875481270285, test_loss 0.4413990952857612\n",
            "Step 17914, train_loss 0.4534981993786322, test_loss 0.4413987806014891\n",
            "Step 17915, train_loss 0.45349764399469084, test_loss 0.4413984659801259\n",
            "Step 17916, train_loss 0.45349708866086996, test_loss 0.4413981514216595\n",
            "Step 17917, train_loss 0.4534965333771612, test_loss 0.44139783692607776\n",
            "Step 17918, train_loss 0.45349597814355613, test_loss 0.4413975224933684\n",
            "Step 17919, train_loss 0.4534954229600458, test_loss 0.44139720812351907\n",
            "Step 17920, train_loss 0.4534948678266221, test_loss 0.44139689381651787\n",
            "Step 17921, train_loss 0.4534943127432762, test_loss 0.4413965795723524\n",
            "Step 17922, train_loss 0.4534937577099997, test_loss 0.44139626539101046\n",
            "Step 17923, train_loss 0.45349320272678406, test_loss 0.4413959512724799\n",
            "Step 17924, train_loss 0.45349264779362075, test_loss 0.4413956372167485\n",
            "Step 17925, train_loss 0.45349209291050124, test_loss 0.4413953232238042\n",
            "Step 17926, train_loss 0.453491538077417, test_loss 0.4413950092936346\n",
            "Step 17927, train_loss 0.45349098329435955, test_loss 0.44139469542622767\n",
            "Step 17928, train_loss 0.4534904285613204, test_loss 0.44139438162157124\n",
            "Step 17929, train_loss 0.45348987387829076, test_loss 0.4413940678796531\n",
            "Step 17930, train_loss 0.4534893192452625, test_loss 0.441393754200461\n",
            "Step 17931, train_loss 0.4534887646622268, test_loss 0.4413934405839828\n",
            "Step 17932, train_loss 0.45348821012917545, test_loss 0.4413931270302065\n",
            "Step 17933, train_loss 0.4534876556460997, test_loss 0.4413928135391196\n",
            "Step 17934, train_loss 0.4534871012129912, test_loss 0.4413925001107103\n",
            "Step 17935, train_loss 0.4534865468298413, test_loss 0.4413921867449662\n",
            "Step 17936, train_loss 0.45348599249664157, test_loss 0.44139187344187536\n",
            "Step 17937, train_loss 0.4534854382133836, test_loss 0.4413915602014254\n",
            "Step 17938, train_loss 0.45348488398005876, test_loss 0.4413912470236044\n",
            "Step 17939, train_loss 0.45348432979665865, test_loss 0.44139093390840006\n",
            "Step 17940, train_loss 0.4534837756631746, test_loss 0.4413906208558002\n",
            "Step 17941, train_loss 0.45348322157959847, test_loss 0.44139030786579275\n",
            "Step 17942, train_loss 0.4534826675459215, test_loss 0.4413899949383656\n",
            "Step 17943, train_loss 0.45348211356213525, test_loss 0.44138968207350654\n",
            "Step 17944, train_loss 0.45348155962823133, test_loss 0.4413893692712037\n",
            "Step 17945, train_loss 0.4534810057442012, test_loss 0.4413890565314445\n",
            "Step 17946, train_loss 0.45348045191003644, test_loss 0.44138874385421717\n",
            "Step 17947, train_loss 0.4534798981257284, test_loss 0.44138843123950955\n",
            "Step 17948, train_loss 0.45347934439126886, test_loss 0.4413881186873093\n",
            "Step 17949, train_loss 0.45347879070664926, test_loss 0.4413878061976046\n",
            "Step 17950, train_loss 0.453478237071861, test_loss 0.4413874937703832\n",
            "Step 17951, train_loss 0.45347768348689577, test_loss 0.4413871814056329\n",
            "Step 17952, train_loss 0.45347712995174505, test_loss 0.44138686910334185\n",
            "Step 17953, train_loss 0.4534765764664004, test_loss 0.4413865568634976\n",
            "Step 17954, train_loss 0.4534760230308534, test_loss 0.4413862446860884\n",
            "Step 17955, train_loss 0.4534754696450956, test_loss 0.44138593257110187\n",
            "Step 17956, train_loss 0.4534749163091184, test_loss 0.44138562051852614\n",
            "Step 17957, train_loss 0.45347436302291355, test_loss 0.44138530852834906\n",
            "Step 17958, train_loss 0.4534738097864725, test_loss 0.4413849966005584\n",
            "Step 17959, train_loss 0.4534732565997869, test_loss 0.44138468473514225\n",
            "Step 17960, train_loss 0.4534727034628481, test_loss 0.4413843729320884\n",
            "Step 17961, train_loss 0.45347215037564786, test_loss 0.4413840611913849\n",
            "Step 17962, train_loss 0.4534715973381777, test_loss 0.44138374951301956\n",
            "Step 17963, train_loss 0.4534710443504291, test_loss 0.44138343789698037\n",
            "Step 17964, train_loss 0.45347049141239376, test_loss 0.44138312634325544\n",
            "Step 17965, train_loss 0.45346993852406314, test_loss 0.44138281485183223\n",
            "Step 17966, train_loss 0.4534693856854289, test_loss 0.4413825034226991\n",
            "Step 17967, train_loss 0.45346883289648254, test_loss 0.4413821920558439\n",
            "Step 17968, train_loss 0.4534682801572156, test_loss 0.44138188075125456\n",
            "Step 17969, train_loss 0.45346772746761993, test_loss 0.4413815695089189\n",
            "Step 17970, train_loss 0.4534671748276867, test_loss 0.4413812583288249\n",
            "Step 17971, train_loss 0.4534666222374078, test_loss 0.4413809472109607\n",
            "Step 17972, train_loss 0.45346606969677483, test_loss 0.44138063615531414\n",
            "Step 17973, train_loss 0.45346551720577916, test_loss 0.44138032516187325\n",
            "Step 17974, train_loss 0.4534649647644126, test_loss 0.4413800142306258\n",
            "Step 17975, train_loss 0.45346441237266655, test_loss 0.4413797033615599\n",
            "Step 17976, train_loss 0.45346386003053274, test_loss 0.4413793925546634\n",
            "Step 17977, train_loss 0.4534633077380027, test_loss 0.4413790818099245\n",
            "Step 17978, train_loss 0.45346275549506815, test_loss 0.4413787711273311\n",
            "Step 17979, train_loss 0.4534622033017206, test_loss 0.4413784605068711\n",
            "Step 17980, train_loss 0.4534616511579516, test_loss 0.4413781499485324\n",
            "Step 17981, train_loss 0.453461099063753, test_loss 0.44137783945230313\n",
            "Step 17982, train_loss 0.4534605470191159, test_loss 0.44137752901817123\n",
            "Step 17983, train_loss 0.45345999502403256, test_loss 0.4413772186461248\n",
            "Step 17984, train_loss 0.45345944307849423, test_loss 0.4413769083361516\n",
            "Step 17985, train_loss 0.45345889118249244, test_loss 0.44137659808823987\n",
            "Step 17986, train_loss 0.45345833933601915, test_loss 0.4413762879023773\n",
            "Step 17987, train_loss 0.4534577875390658, test_loss 0.4413759777785522\n",
            "Step 17988, train_loss 0.4534572357916239, test_loss 0.4413756677167524\n",
            "Step 17989, train_loss 0.45345668409368517, test_loss 0.44137535771696607\n",
            "Step 17990, train_loss 0.45345613244524136, test_loss 0.4413750477791811\n",
            "Step 17991, train_loss 0.45345558084628396, test_loss 0.44137473790338544\n",
            "Step 17992, train_loss 0.4534550292968046, test_loss 0.4413744280895672\n",
            "Step 17993, train_loss 0.4534544777967951, test_loss 0.44137411833771445\n",
            "Step 17994, train_loss 0.4534539263462468, test_loss 0.4413738086478151\n",
            "Step 17995, train_loss 0.45345337494515164, test_loss 0.44137349901985723\n",
            "Step 17996, train_loss 0.45345282359350114, test_loss 0.4413731894538289\n",
            "Step 17997, train_loss 0.4534522722912868, test_loss 0.44137287994971813\n",
            "Step 17998, train_loss 0.4534517210385005, test_loss 0.4413725705075129\n",
            "Step 17999, train_loss 0.45345116983513384, test_loss 0.4413722611272013\n",
            "Step 18001, train_loss 0.453450067576626, test_loss 0.44137164255221106\n",
            "Step 18002, train_loss 0.453449516521468, test_loss 0.44137133335750856\n",
            "Step 18003, train_loss 0.45344896551569625, test_loss 0.44137102422465185\n",
            "Step 18004, train_loss 0.45344841455930246, test_loss 0.44137071515362897\n",
            "Step 18005, train_loss 0.4534478636522783, test_loss 0.4413704061444281\n",
            "Step 18006, train_loss 0.4534473127946152, test_loss 0.44137009719703707\n",
            "Step 18007, train_loss 0.4534467619863052, test_loss 0.441369788311444\n",
            "Step 18008, train_loss 0.4534462112273396, test_loss 0.4413694794876372\n",
            "Step 18009, train_loss 0.4534456605177104, test_loss 0.4413691707256045\n",
            "Step 18010, train_loss 0.45344510985740905, test_loss 0.4413688620253341\n",
            "Step 18011, train_loss 0.4534445592464273, test_loss 0.44136855338681397\n",
            "Step 18012, train_loss 0.4534440086847569, test_loss 0.4413682448100322\n",
            "Step 18013, train_loss 0.4534434581723895, test_loss 0.441367936294977\n",
            "Step 18014, train_loss 0.45344290770931683, test_loss 0.44136762784163613\n",
            "Step 18015, train_loss 0.45344235729553034, test_loss 0.44136731944999796\n",
            "Step 18016, train_loss 0.453441806931022, test_loss 0.44136701112005067\n",
            "Step 18017, train_loss 0.4534412566157835, test_loss 0.44136670285178214\n",
            "Step 18018, train_loss 0.4534407063498063, test_loss 0.4413663946451805\n",
            "Step 18019, train_loss 0.4534401561330824, test_loss 0.44136608650023385\n",
            "Step 18020, train_loss 0.4534396059656033, test_loss 0.4413657784169305\n",
            "Step 18021, train_loss 0.45343905584736055, test_loss 0.4413654703952581\n",
            "Step 18022, train_loss 0.4534385057783462, test_loss 0.4413651624352051\n",
            "Step 18023, train_loss 0.4534379557585519, test_loss 0.44136485453675955\n",
            "Step 18024, train_loss 0.4534374057879692, test_loss 0.4413645466999097\n",
            "Step 18025, train_loss 0.4534368558665898, test_loss 0.4413642389246433\n",
            "Step 18026, train_loss 0.45343630599440565, test_loss 0.44136393121094886\n",
            "Step 18027, train_loss 0.4534357561714083, test_loss 0.44136362355881426\n",
            "Step 18028, train_loss 0.4534352063975894, test_loss 0.4413633159682277\n",
            "Step 18029, train_loss 0.4534346566729409, test_loss 0.4413630084391773\n",
            "Step 18030, train_loss 0.4534341069974543, test_loss 0.4413627009716512\n",
            "Step 18031, train_loss 0.4534335573711214, test_loss 0.4413623935656376\n",
            "Step 18032, train_loss 0.453433007793934, test_loss 0.44136208622112444\n",
            "Step 18033, train_loss 0.45343245826588374, test_loss 0.44136177893810014\n",
            "Step 18034, train_loss 0.4534319087869625, test_loss 0.4413614717165526\n",
            "Step 18035, train_loss 0.45343135935716183, test_loss 0.4413611645564702\n",
            "Step 18036, train_loss 0.4534308099764736, test_loss 0.4413608574578407\n",
            "Step 18037, train_loss 0.45343026064488945, test_loss 0.4413605504206527\n",
            "Step 18038, train_loss 0.4534297113624012, test_loss 0.4413602434448941\n",
            "Step 18039, train_loss 0.45342916212900064, test_loss 0.441359936530553\n",
            "Step 18040, train_loss 0.45342861294467934, test_loss 0.4413596296776178\n",
            "Step 18041, train_loss 0.45342806380942924, test_loss 0.4413593228860766\n",
            "Step 18042, train_loss 0.453427514723242, test_loss 0.4413590161559174\n",
            "Step 18043, train_loss 0.4534269656861094, test_loss 0.4413587094871286\n",
            "Step 18044, train_loss 0.45342641669802314, test_loss 0.44135840287969813\n",
            "Step 18045, train_loss 0.45342586775897514, test_loss 0.4413580963336144\n",
            "Step 18046, train_loss 0.453425318868957, test_loss 0.4413577898488654\n",
            "Step 18047, train_loss 0.4534247700279605, test_loss 0.4413574834254394\n",
            "Step 18048, train_loss 0.4534242212359775, test_loss 0.44135717706332456\n",
            "Step 18049, train_loss 0.45342367249299964, test_loss 0.4413568707625092\n",
            "Step 18050, train_loss 0.4534231237990188, test_loss 0.4413565645229813\n",
            "Step 18051, train_loss 0.45342257515402673, test_loss 0.4413562583447292\n",
            "Step 18052, train_loss 0.4534220265580153, test_loss 0.441355952227741\n",
            "Step 18053, train_loss 0.45342147801097604, test_loss 0.4413556461720049\n",
            "Step 18054, train_loss 0.45342092951290097, test_loss 0.4413553401775092\n",
            "Step 18055, train_loss 0.4534203810637817, test_loss 0.4413550342442421\n",
            "Step 18056, train_loss 0.4534198326636102, test_loss 0.44135472837219186\n",
            "Step 18057, train_loss 0.45341928431237805, test_loss 0.4413544225613464\n",
            "Step 18058, train_loss 0.45341873601007726, test_loss 0.4413541168116943\n",
            "Step 18059, train_loss 0.4534181877566994, test_loss 0.4413538111232235\n",
            "Step 18060, train_loss 0.45341763955223646, test_loss 0.44135350549592234\n",
            "Step 18061, train_loss 0.45341709139668007, test_loss 0.44135319992977906\n",
            "Step 18062, train_loss 0.4534165432900222, test_loss 0.4413528944247819\n",
            "Step 18063, train_loss 0.4534159952322545, test_loss 0.4413525889809191\n",
            "Step 18064, train_loss 0.4534154472233689, test_loss 0.44135228359817863\n",
            "Step 18065, train_loss 0.45341489926335715, test_loss 0.44135197827654915\n",
            "Step 18066, train_loss 0.4534143513522109, test_loss 0.4413516730160187\n",
            "Step 18067, train_loss 0.45341380348992233, test_loss 0.44135136781657536\n",
            "Step 18068, train_loss 0.453413255676483, test_loss 0.4413510626782077\n",
            "Step 18069, train_loss 0.4534127079118847, test_loss 0.4413507576009036\n",
            "Step 18070, train_loss 0.4534121601961192, test_loss 0.4413504525846516\n",
            "Step 18071, train_loss 0.45341161252917866, test_loss 0.44135014762943997\n",
            "Step 18072, train_loss 0.45341106491105454, test_loss 0.44134984273525674\n",
            "Step 18073, train_loss 0.45341051734173893, test_loss 0.4413495379020903\n",
            "Step 18074, train_loss 0.4534099698212234, test_loss 0.4413492331299289\n",
            "Step 18075, train_loss 0.45340942234949994, test_loss 0.44134892841876083\n",
            "Step 18076, train_loss 0.4534088749265603, test_loss 0.4413486237685743\n",
            "Step 18077, train_loss 0.4534083275523964, test_loss 0.4413483191793577\n",
            "Step 18078, train_loss 0.45340778022700023, test_loss 0.4413480146510991\n",
            "Step 18079, train_loss 0.4534072329503632, test_loss 0.4413477101837869\n",
            "Step 18080, train_loss 0.45340668572247755, test_loss 0.4413474057774095\n",
            "Step 18081, train_loss 0.4534061385433349, test_loss 0.44134710143195494\n",
            "Step 18082, train_loss 0.45340559141292713, test_loss 0.4413467971474117\n",
            "Step 18083, train_loss 0.4534050443312461, test_loss 0.44134649292376793\n",
            "Step 18084, train_loss 0.4534044972982838, test_loss 0.441346188761012\n",
            "Step 18085, train_loss 0.45340395031403197, test_loss 0.44134588465913227\n",
            "Step 18086, train_loss 0.4534034033784824, test_loss 0.44134558061811685\n",
            "Step 18087, train_loss 0.45340285649162704, test_loss 0.4413452766379542\n",
            "Step 18088, train_loss 0.4534023096534576, test_loss 0.4413449727186326\n",
            "Step 18089, train_loss 0.4534017628639662, test_loss 0.44134466886014023\n",
            "Step 18090, train_loss 0.45340121612314455, test_loss 0.4413443650624655\n",
            "Step 18091, train_loss 0.4534006694309845, test_loss 0.4413440613255968\n",
            "Step 18092, train_loss 0.453400122787478, test_loss 0.4413437576495224\n",
            "Step 18093, train_loss 0.4533995761926168, test_loss 0.44134345403423053\n",
            "Step 18094, train_loss 0.45339902964639284, test_loss 0.44134315047970957\n",
            "Step 18095, train_loss 0.453398483148798, test_loss 0.4413428469859478\n",
            "Step 18096, train_loss 0.4533979366998242, test_loss 0.4413425435529336\n",
            "Step 18097, train_loss 0.45339739029946324, test_loss 0.4413422401806553\n",
            "Step 18098, train_loss 0.4533968439477071, test_loss 0.4413419368691013\n",
            "Step 18099, train_loss 0.45339629764454753, test_loss 0.44134163361825973\n",
            "Step 18101, train_loss 0.45339520518398607, test_loss 0.44134102729866775\n",
            "Step 18102, train_loss 0.4533946590265677, test_loss 0.441340724229894\n",
            "Step 18103, train_loss 0.45339411291771375, test_loss 0.4413404212217861\n",
            "Step 18104, train_loss 0.4533935668574158, test_loss 0.44134011827433256\n",
            "Step 18105, train_loss 0.4533930208456659, test_loss 0.44133981538752154\n",
            "Step 18106, train_loss 0.4533924748824559, test_loss 0.44133951256134146\n",
            "Step 18107, train_loss 0.4533919289677777, test_loss 0.4413392097957809\n",
            "Step 18108, train_loss 0.4533913831016233, test_loss 0.4413389070908279\n",
            "Step 18109, train_loss 0.4533908372839843, test_loss 0.44133860444647116\n",
            "Step 18110, train_loss 0.45339029151485294, test_loss 0.4413383018626987\n",
            "Step 18111, train_loss 0.4533897457942212, test_loss 0.441337999339499\n",
            "Step 18112, train_loss 0.4533892001220806, test_loss 0.44133769687686075\n",
            "Step 18113, train_loss 0.45338865449842336, test_loss 0.441337394474772\n",
            "Step 18114, train_loss 0.4533881089232413, test_loss 0.4413370921332211\n",
            "Step 18115, train_loss 0.4533875633965263, test_loss 0.4413367898521966\n",
            "Step 18116, train_loss 0.4533870179182705, test_loss 0.4413364876316866\n",
            "Step 18117, train_loss 0.4533864724884655, test_loss 0.4413361854716799\n",
            "Step 18118, train_loss 0.4533859271071035, test_loss 0.4413358833721648\n",
            "Step 18119, train_loss 0.4533853817741763, test_loss 0.44133558133312933\n",
            "Step 18120, train_loss 0.45338483648967604, test_loss 0.44133527935456235\n",
            "Step 18121, train_loss 0.4533842912535942, test_loss 0.441334977436452\n",
            "Step 18122, train_loss 0.45338374606592313, test_loss 0.4413346755787867\n",
            "Step 18123, train_loss 0.45338320092665446, test_loss 0.4413343737815549\n",
            "Step 18124, train_loss 0.4533826558357805, test_loss 0.44133407204474495\n",
            "Step 18125, train_loss 0.45338211079329305, test_loss 0.44133377036834537\n",
            "Step 18126, train_loss 0.4533815657991838, test_loss 0.4413334687523446\n",
            "Step 18127, train_loss 0.453381020853445, test_loss 0.44133316719673094\n",
            "Step 18128, train_loss 0.4533804759560686, test_loss 0.4413328657014926\n",
            "Step 18129, train_loss 0.4533799311070464, test_loss 0.4413325642666185\n",
            "Step 18130, train_loss 0.45337938630637026, test_loss 0.44133226289209687\n",
            "Step 18131, train_loss 0.45337884155403246, test_loss 0.44133196157791593\n",
            "Step 18132, train_loss 0.45337829685002484, test_loss 0.4413316603240644\n",
            "Step 18133, train_loss 0.4533777521943391, test_loss 0.44133135913053045\n",
            "Step 18134, train_loss 0.4533772075869676, test_loss 0.4413310579973027\n",
            "Step 18135, train_loss 0.453376663027902, test_loss 0.44133075692436957\n",
            "Step 18136, train_loss 0.4533761185171345, test_loss 0.4413304559117195\n",
            "Step 18137, train_loss 0.45337557405465695, test_loss 0.4413301549593409\n",
            "Step 18138, train_loss 0.4533750296404613, test_loss 0.4413298540672222\n",
            "Step 18139, train_loss 0.45337448527453955, test_loss 0.44132955323535183\n",
            "Step 18140, train_loss 0.45337394095688377, test_loss 0.4413292524637183\n",
            "Step 18141, train_loss 0.45337339668748583, test_loss 0.44132895175231024\n",
            "Step 18142, train_loss 0.4533728524663378, test_loss 0.4413286511011158\n",
            "Step 18143, train_loss 0.4533723082934315, test_loss 0.4413283505101237\n",
            "Step 18144, train_loss 0.453371764168759, test_loss 0.4413280499793222\n",
            "Step 18145, train_loss 0.45337122009231245, test_loss 0.44132774950869985\n",
            "Step 18146, train_loss 0.45337067606408366, test_loss 0.4413274490982452\n",
            "Step 18147, train_loss 0.4533701320840647, test_loss 0.4413271487479466\n",
            "Step 18148, train_loss 0.45336958815224754, test_loss 0.4413268484577926\n",
            "Step 18149, train_loss 0.45336904426862407, test_loss 0.4413265482277716\n",
            "Step 18150, train_loss 0.45336850043318655, test_loss 0.4413262480578722\n",
            "Step 18151, train_loss 0.4533679566459268, test_loss 0.4413259479480829\n",
            "Step 18152, train_loss 0.4533674129068368, test_loss 0.441325647898392\n",
            "Step 18153, train_loss 0.4533668692159087, test_loss 0.4413253479087882\n",
            "Step 18154, train_loss 0.4533663255731343, test_loss 0.4413250479792599\n",
            "Step 18155, train_loss 0.4533657819785058, test_loss 0.4413247481097957\n",
            "Step 18156, train_loss 0.45336523843201526, test_loss 0.44132444830038403\n",
            "Step 18157, train_loss 0.45336469493365444, test_loss 0.4413241485510133\n",
            "Step 18158, train_loss 0.4533641514834156, test_loss 0.44132384886167203\n",
            "Step 18159, train_loss 0.4533636080812905, test_loss 0.44132354923234896\n",
            "Step 18160, train_loss 0.45336306472727145, test_loss 0.4413232496630323\n",
            "Step 18161, train_loss 0.45336252142135036, test_loss 0.4413229501537108\n",
            "Step 18162, train_loss 0.4533619781635193, test_loss 0.4413226507043729\n",
            "Step 18163, train_loss 0.4533614349537702, test_loss 0.4413223513150069\n",
            "Step 18164, train_loss 0.4533608917920951, test_loss 0.44132205198560165\n",
            "Step 18165, train_loss 0.4533603486784862, test_loss 0.4413217527161457\n",
            "Step 18166, train_loss 0.4533598056129353, test_loss 0.44132145350662727\n",
            "Step 18167, train_loss 0.45335926259543463, test_loss 0.4413211543570352\n",
            "Step 18168, train_loss 0.4533587196259761, test_loss 0.4413208552673578\n",
            "Step 18169, train_loss 0.4533581767045518, test_loss 0.4413205562375837\n",
            "Step 18170, train_loss 0.4533576338311539, test_loss 0.4413202572677016\n",
            "Step 18171, train_loss 0.45335709100577426, test_loss 0.44131995835769955\n",
            "Step 18172, train_loss 0.4533565482284049, test_loss 0.4413196595075665\n",
            "Step 18173, train_loss 0.45335600549903815, test_loss 0.4413193607172912\n",
            "Step 18174, train_loss 0.45335546281766576, test_loss 0.44131906198686177\n",
            "Step 18175, train_loss 0.4533549201842799, test_loss 0.44131876331626685\n",
            "Step 18176, train_loss 0.4533543775988728, test_loss 0.4413184647054953\n",
            "Step 18177, train_loss 0.45335383506143623, test_loss 0.44131816615453534\n",
            "Step 18178, train_loss 0.45335329257196244, test_loss 0.44131786766337566\n",
            "Step 18179, train_loss 0.45335275013044357, test_loss 0.44131756923200466\n",
            "Step 18180, train_loss 0.45335220773687135, test_loss 0.4413172708604113\n",
            "Step 18181, train_loss 0.4533516653912381, test_loss 0.44131697254858376\n",
            "Step 18182, train_loss 0.453351123093536, test_loss 0.441316674296511\n",
            "Step 18183, train_loss 0.45335058084375685, test_loss 0.4413163761041811\n",
            "Step 18184, train_loss 0.4533500386418928, test_loss 0.4413160779715831\n",
            "Step 18185, train_loss 0.4533494964879361, test_loss 0.4413157798987055\n",
            "Step 18186, train_loss 0.45334895438187867, test_loss 0.44131548188553665\n",
            "Step 18187, train_loss 0.4533484123237127, test_loss 0.4413151839320652\n",
            "Step 18188, train_loss 0.45334787031343016, test_loss 0.44131488603827995\n",
            "Step 18189, train_loss 0.4533473283510232, test_loss 0.44131458820416947\n",
            "Step 18190, train_loss 0.4533467864364839, test_loss 0.4413142904297221\n",
            "Step 18191, train_loss 0.4533462445698043, test_loss 0.4413139927149266\n",
            "Step 18192, train_loss 0.4533457027509767, test_loss 0.4413136950597716\n",
            "Step 18193, train_loss 0.4533451609799929, test_loss 0.44131339746424575\n",
            "Step 18194, train_loss 0.45334461925684527, test_loss 0.4413130999283375\n",
            "Step 18195, train_loss 0.4533440775815256, test_loss 0.4413128024520356\n",
            "Step 18196, train_loss 0.4533435359540264, test_loss 0.4413125050353286\n",
            "Step 18197, train_loss 0.45334299437433934, test_loss 0.4413122076782052\n",
            "Step 18198, train_loss 0.4533424528424569, test_loss 0.44131191038065404\n",
            "Step 18199, train_loss 0.453341911358371, test_loss 0.44131161314266354\n",
            "Step 18201, train_loss 0.45334082853355734, test_loss 0.44131101884531937\n",
            "Step 18202, train_loss 0.45334028719281383, test_loss 0.44131072178594316\n",
            "Step 18203, train_loss 0.4533397458998354, test_loss 0.4413104247860821\n",
            "Step 18204, train_loss 0.4533392046546141, test_loss 0.4413101278457251\n",
            "Step 18205, train_loss 0.4533386634571419, test_loss 0.4413098309648606\n",
            "Step 18206, train_loss 0.4533381223074112, test_loss 0.4413095341434774\n",
            "Step 18207, train_loss 0.45333758120541423, test_loss 0.441309237381564\n",
            "Step 18208, train_loss 0.45333704015114273, test_loss 0.4413089406791092\n",
            "Step 18209, train_loss 0.45333649914458907, test_loss 0.4413086440361016\n",
            "Step 18210, train_loss 0.45333595818574535, test_loss 0.4413083474525298\n",
            "Step 18211, train_loss 0.45333541727460364, test_loss 0.44130805092838254\n",
            "Step 18212, train_loss 0.4533348764111562, test_loss 0.44130775446364834\n",
            "Step 18213, train_loss 0.4533343355953951, test_loss 0.4413074580583161\n",
            "Step 18214, train_loss 0.4533337948273124, test_loss 0.44130716171237444\n",
            "Step 18215, train_loss 0.4533332541069004, test_loss 0.44130686542581177\n",
            "Step 18216, train_loss 0.45333271343415116, test_loss 0.441306569198617\n",
            "Step 18217, train_loss 0.45333217280905685, test_loss 0.4413062730307787\n",
            "Step 18218, train_loss 0.45333163223160966, test_loss 0.4413059769222855\n",
            "Step 18219, train_loss 0.45333109170180164, test_loss 0.4413056808731264\n",
            "Step 18220, train_loss 0.45333055121962507, test_loss 0.44130538488328974\n",
            "Step 18221, train_loss 0.453330010785072, test_loss 0.44130508895276427\n",
            "Step 18222, train_loss 0.45332947039813465, test_loss 0.4413047930815387\n",
            "Step 18223, train_loss 0.45332893005880526, test_loss 0.4413044972696018\n",
            "Step 18224, train_loss 0.45332838976707573, test_loss 0.44130420151694233\n",
            "Step 18225, train_loss 0.4533278495229386, test_loss 0.4413039058235488\n",
            "Step 18226, train_loss 0.4533273093263857, test_loss 0.4413036101894099\n",
            "Step 18227, train_loss 0.45332676917740933, test_loss 0.44130331461451455\n",
            "Step 18228, train_loss 0.4533262290760017, test_loss 0.44130301909885133\n",
            "Step 18229, train_loss 0.45332568902215503, test_loss 0.44130272364240886\n",
            "Step 18230, train_loss 0.4533251490158614, test_loss 0.441302428245176\n",
            "Step 18231, train_loss 0.45332460905711297, test_loss 0.4413021329071413\n",
            "Step 18232, train_loss 0.4533240691459021, test_loss 0.44130183762829367\n",
            "Step 18233, train_loss 0.4533235292822208, test_loss 0.4413015424086218\n",
            "Step 18234, train_loss 0.45332298946606125, test_loss 0.44130124724811415\n",
            "Step 18235, train_loss 0.45332244969741575, test_loss 0.44130095214675996\n",
            "Step 18236, train_loss 0.4533219099762764, test_loss 0.4413006571045474\n",
            "Step 18237, train_loss 0.4533213703026355, test_loss 0.44130036212146545\n",
            "Step 18238, train_loss 0.4533208306764851, test_loss 0.4413000671975029\n",
            "Step 18239, train_loss 0.4533202910978175, test_loss 0.4412997723326485\n",
            "Step 18240, train_loss 0.45331975156662485, test_loss 0.4412994775268908\n",
            "Step 18241, train_loss 0.4533192120828994, test_loss 0.44129918278021874\n",
            "Step 18242, train_loss 0.45331867264663334, test_loss 0.441298888092621\n",
            "Step 18243, train_loss 0.45331813325781883, test_loss 0.4412985934640863\n",
            "Step 18244, train_loss 0.45331759391644816, test_loss 0.44129829889460337\n",
            "Step 18245, train_loss 0.4533170546225134, test_loss 0.44129800438416106\n",
            "Step 18246, train_loss 0.4533165153760069, test_loss 0.441297709932748\n",
            "Step 18247, train_loss 0.4533159761769209, test_loss 0.4412974155403532\n",
            "Step 18248, train_loss 0.45331543702524746, test_loss 0.441297121206965\n",
            "Step 18249, train_loss 0.4533148979209789, test_loss 0.44129682693257255\n",
            "Step 18250, train_loss 0.45331435886410754, test_loss 0.4412965327171644\n",
            "Step 18251, train_loss 0.4533138198546255, test_loss 0.4412962385607295\n",
            "Step 18252, train_loss 0.4533132808925249, test_loss 0.4412959444632565\n",
            "Step 18253, train_loss 0.45331274197779803, test_loss 0.4412956504247341\n",
            "Step 18254, train_loss 0.4533122031104372, test_loss 0.4412953564451512\n",
            "Step 18255, train_loss 0.45331166429043473, test_loss 0.44129506252449663\n",
            "Step 18256, train_loss 0.45331112551778263, test_loss 0.441294768662759\n",
            "Step 18257, train_loss 0.45331058679247327, test_loss 0.44129447485992734\n",
            "Step 18258, train_loss 0.4533100481144988, test_loss 0.4412941811159902\n",
            "Step 18259, train_loss 0.45330950948385146, test_loss 0.4412938874309365\n",
            "Step 18260, train_loss 0.4533089709005237, test_loss 0.4412935938047549\n",
            "Step 18261, train_loss 0.4533084323645075, test_loss 0.44129330023743457\n",
            "Step 18262, train_loss 0.45330789387579523, test_loss 0.44129300672896393\n",
            "Step 18263, train_loss 0.45330735543437917, test_loss 0.4412927132793317\n",
            "Step 18264, train_loss 0.45330681704025144, test_loss 0.4412924198885272\n",
            "Step 18265, train_loss 0.45330627869340456, test_loss 0.44129212655653877\n",
            "Step 18266, train_loss 0.4533057403938306, test_loss 0.4412918332833555\n",
            "Step 18267, train_loss 0.45330520214152165, test_loss 0.441291540068966\n",
            "Step 18268, train_loss 0.45330466393647023, test_loss 0.4412912469133591\n",
            "Step 18269, train_loss 0.4533041257786686, test_loss 0.4412909538165239\n",
            "Step 18270, train_loss 0.45330358766810885, test_loss 0.44129066077844886\n",
            "Step 18271, train_loss 0.4533030496047834, test_loss 0.441290367799123\n",
            "Step 18272, train_loss 0.45330251158868445, test_loss 0.44129007487853517\n",
            "Step 18273, train_loss 0.4533019736198043, test_loss 0.4412897820166741\n",
            "Step 18274, train_loss 0.4533014356981352, test_loss 0.4412894892135287\n",
            "Step 18275, train_loss 0.45330089782366945, test_loss 0.44128919646908776\n",
            "Step 18276, train_loss 0.4533003599963993, test_loss 0.4412889037833402\n",
            "Step 18277, train_loss 0.45329982221631704, test_loss 0.44128861115627477\n",
            "Step 18278, train_loss 0.453299284483415, test_loss 0.4412883185878803\n",
            "Step 18279, train_loss 0.4532987467976854, test_loss 0.44128802607814577\n",
            "Step 18280, train_loss 0.45329820915912056, test_loss 0.44128773362705986\n",
            "Step 18281, train_loss 0.4532976715677128, test_loss 0.44128744123461167\n",
            "Step 18282, train_loss 0.4532971340234543, test_loss 0.4412871489007898\n",
            "Step 18283, train_loss 0.4532965965263373, test_loss 0.44128685662558326\n",
            "Step 18284, train_loss 0.45329605907635434, test_loss 0.4412865644089808\n",
            "Step 18285, train_loss 0.4532955216734976, test_loss 0.44128627225097145\n",
            "Step 18286, train_loss 0.45329498431775944, test_loss 0.44128598015154386\n",
            "Step 18287, train_loss 0.453294447009132, test_loss 0.4412856881106871\n",
            "Step 18288, train_loss 0.45329390974760764, test_loss 0.4412853961283899\n",
            "Step 18289, train_loss 0.4532933725331788, test_loss 0.44128510420464123\n",
            "Step 18290, train_loss 0.4532928353658376, test_loss 0.44128481233943007\n",
            "Step 18291, train_loss 0.45329229824557654, test_loss 0.4412845205327451\n",
            "Step 18292, train_loss 0.4532917611723878, test_loss 0.4412842287845751\n",
            "Step 18293, train_loss 0.45329122414626366, test_loss 0.44128393709490926\n",
            "Step 18294, train_loss 0.45329068716719656, test_loss 0.44128364546373644\n",
            "Step 18295, train_loss 0.4532901502351788, test_loss 0.4412833538910454\n",
            "Step 18296, train_loss 0.4532896133502026, test_loss 0.4412830623768249\n",
            "Step 18297, train_loss 0.4532890765122603, test_loss 0.44128277092106427\n",
            "Step 18298, train_loss 0.45328853972134436, test_loss 0.441282479523752\n",
            "Step 18299, train_loss 0.45328800297744704, test_loss 0.4412821881848772\n",
            "Step 18301, train_loss 0.4532869296306774, test_loss 0.44128160568239544\n",
            "Step 18302, train_loss 0.45328639302778984, test_loss 0.4412813145187664\n",
            "Step 18303, train_loss 0.45328585647189024, test_loss 0.44128102341353026\n",
            "Step 18304, train_loss 0.4532853199629708, test_loss 0.44128073236667614\n",
            "Step 18305, train_loss 0.453284783501024, test_loss 0.441280441378193\n",
            "Step 18306, train_loss 0.4532842470860421, test_loss 0.4412801504480697\n",
            "Step 18307, train_loss 0.45328371071801765, test_loss 0.44127985957629506\n",
            "Step 18308, train_loss 0.45328317439694277, test_loss 0.441279568762858\n",
            "Step 18309, train_loss 0.45328263812280983, test_loss 0.4412792780077478\n",
            "Step 18310, train_loss 0.4532821018956112, test_loss 0.441278987310953\n",
            "Step 18311, train_loss 0.45328156571533934, test_loss 0.4412786966724627\n",
            "Step 18312, train_loss 0.45328102958198646, test_loss 0.44127840609226576\n",
            "Step 18313, train_loss 0.453280493495545, test_loss 0.4412781155703513\n",
            "Step 18314, train_loss 0.45327995745600735, test_loss 0.44127782510670804\n",
            "Step 18315, train_loss 0.45327942146336575, test_loss 0.4412775347013252\n",
            "Step 18316, train_loss 0.45327888551761275, test_loss 0.4412772443541914\n",
            "Step 18317, train_loss 0.45327834961874053, test_loss 0.44127695406529577\n",
            "Step 18318, train_loss 0.4532778137667415, test_loss 0.4412766638346273\n",
            "Step 18319, train_loss 0.4532772779616082, test_loss 0.441276373662175\n",
            "Step 18320, train_loss 0.45327674220333264, test_loss 0.4412760835479275\n",
            "Step 18321, train_loss 0.4532762064919076, test_loss 0.4412757934918741\n",
            "Step 18322, train_loss 0.4532756708273251, test_loss 0.4412755034940038\n",
            "Step 18323, train_loss 0.45327513520957785, test_loss 0.4412752135543053\n",
            "Step 18324, train_loss 0.453274599638658, test_loss 0.4412749236727678\n",
            "Step 18325, train_loss 0.45327406411455806, test_loss 0.4412746338493802\n",
            "Step 18326, train_loss 0.4532735286372703, test_loss 0.4412743440841313\n",
            "Step 18327, train_loss 0.45327299320678716, test_loss 0.4412740543770104\n",
            "Step 18328, train_loss 0.45327245782310105, test_loss 0.4412737647280063\n",
            "Step 18329, train_loss 0.4532719224862043, test_loss 0.4412734751371079\n",
            "Step 18330, train_loss 0.4532713871960894, test_loss 0.44127318560430445\n",
            "Step 18331, train_loss 0.45327085195274863, test_loss 0.44127289612958465\n",
            "Step 18332, train_loss 0.4532703167561745, test_loss 0.44127260671293783\n",
            "Step 18333, train_loss 0.4532697816063594, test_loss 0.4412723173543527\n",
            "Step 18334, train_loss 0.45326924650329564, test_loss 0.4412720280538183\n",
            "Step 18335, train_loss 0.4532687114469756, test_loss 0.4412717388113237\n",
            "Step 18336, train_loss 0.4532681764373919, test_loss 0.4412714496268579\n",
            "Step 18337, train_loss 0.4532676414745367, test_loss 0.44127116050040993\n",
            "Step 18338, train_loss 0.45326710655840263, test_loss 0.4412708714319688\n",
            "Step 18339, train_loss 0.4532665716889819, test_loss 0.4412705824215235\n",
            "Step 18340, train_loss 0.4532660368662669, test_loss 0.441270293469063\n",
            "Step 18341, train_loss 0.45326550209025035, test_loss 0.4412700045745763\n",
            "Step 18342, train_loss 0.4532649673609244, test_loss 0.4412697157380527\n",
            "Step 18343, train_loss 0.45326443267828154, test_loss 0.4412694269594808\n",
            "Step 18344, train_loss 0.45326389804231415, test_loss 0.4412691382388499\n",
            "Step 18345, train_loss 0.45326336345301477, test_loss 0.4412688495761489\n",
            "Step 18346, train_loss 0.4532628289103757, test_loss 0.441268560971367\n",
            "Step 18347, train_loss 0.4532622944143896, test_loss 0.441268272424493\n",
            "Step 18348, train_loss 0.45326175996504864, test_loss 0.4412679839355162\n",
            "Step 18349, train_loss 0.4532612255623453, test_loss 0.4412676955044254\n",
            "Step 18350, train_loss 0.4532606912062721, test_loss 0.4412674071312098\n",
            "Step 18351, train_loss 0.4532601568968214, test_loss 0.4412671188158585\n",
            "Step 18352, train_loss 0.45325962263398567, test_loss 0.44126683055836036\n",
            "Step 18353, train_loss 0.4532590884177574, test_loss 0.4412665423587045\n",
            "Step 18354, train_loss 0.45325855424812894, test_loss 0.44126625421688\n",
            "Step 18355, train_loss 0.4532580201250928, test_loss 0.4412659661328759\n",
            "Step 18356, train_loss 0.4532574860486415, test_loss 0.4412656781066814\n",
            "Step 18357, train_loss 0.4532569520187674, test_loss 0.44126539013828525\n",
            "Step 18358, train_loss 0.45325641803546285, test_loss 0.4412651022276768\n",
            "Step 18359, train_loss 0.4532558840987204, test_loss 0.44126481437484494\n",
            "Step 18360, train_loss 0.4532553502085326, test_loss 0.4412645265797789\n",
            "Step 18361, train_loss 0.45325481636489173, test_loss 0.4412642388424676\n",
            "Step 18362, train_loss 0.45325428256779043, test_loss 0.4412639511629001\n",
            "Step 18363, train_loss 0.4532537488172211, test_loss 0.4412636635410657\n",
            "Step 18364, train_loss 0.45325321511317607, test_loss 0.4412633759769534\n",
            "Step 18365, train_loss 0.45325268145564807, test_loss 0.44126308847055223\n",
            "Step 18366, train_loss 0.4532521478446293, test_loss 0.4412628010218511\n",
            "Step 18367, train_loss 0.45325161428011235, test_loss 0.44126251363083935\n",
            "Step 18368, train_loss 0.45325108076208975, test_loss 0.4412622262975061\n",
            "Step 18369, train_loss 0.4532505472905538, test_loss 0.4412619390218402\n",
            "Step 18370, train_loss 0.4532500138654972, test_loss 0.44126165180383103\n",
            "Step 18371, train_loss 0.45324948048691227, test_loss 0.4412613646434673\n",
            "Step 18372, train_loss 0.4532489471547916, test_loss 0.4412610775407386\n",
            "Step 18373, train_loss 0.45324841386912756, test_loss 0.4412607904956337\n",
            "Step 18374, train_loss 0.45324788062991267, test_loss 0.44126050350814183\n",
            "Step 18375, train_loss 0.45324734743713946, test_loss 0.4412602165782521\n",
            "Step 18376, train_loss 0.4532468142908004, test_loss 0.44125992970595357\n",
            "Step 18377, train_loss 0.45324628119088795, test_loss 0.44125964289123537\n",
            "Step 18378, train_loss 0.4532457481373947, test_loss 0.44125935613408673\n",
            "Step 18379, train_loss 0.453245215130313, test_loss 0.44125906943449644\n",
            "Step 18380, train_loss 0.4532446821696355, test_loss 0.441258782792454\n",
            "Step 18381, train_loss 0.45324414925535456, test_loss 0.44125849620794844\n",
            "Step 18382, train_loss 0.45324361638746286, test_loss 0.4412582096809688\n",
            "Step 18383, train_loss 0.4532430835659526, test_loss 0.44125792321150425\n",
            "Step 18384, train_loss 0.4532425507908166, test_loss 0.441257636799544\n",
            "Step 18385, train_loss 0.45324201806204717, test_loss 0.441257350445077\n",
            "Step 18386, train_loss 0.453241485379637, test_loss 0.44125706414809257\n",
            "Step 18387, train_loss 0.4532409527435784, test_loss 0.44125677790857987\n",
            "Step 18388, train_loss 0.45324042015386407, test_loss 0.4412564917265279\n",
            "Step 18389, train_loss 0.4532398876104864, test_loss 0.44125620560192597\n",
            "Step 18390, train_loss 0.45323935511343794, test_loss 0.44125591953476306\n",
            "Step 18391, train_loss 0.45323882266271126, test_loss 0.44125563352502833\n",
            "Step 18392, train_loss 0.45323829025829876, test_loss 0.441255347572711\n",
            "Step 18393, train_loss 0.45323775790019316, test_loss 0.4412550616778003\n",
            "Step 18394, train_loss 0.45323722558838675, test_loss 0.44125477584028544\n",
            "Step 18395, train_loss 0.4532366933228722, test_loss 0.44125449006015527\n",
            "Step 18396, train_loss 0.453236161103642, test_loss 0.4412542043373993\n",
            "Step 18397, train_loss 0.4532356289306887, test_loss 0.44125391867200653\n",
            "Step 18398, train_loss 0.453235096804005, test_loss 0.44125363306396626\n",
            "Step 18399, train_loss 0.45323456472358314, test_loss 0.4412533475132674\n",
            "Step 18401, train_loss 0.4532335007014956, test_loss 0.4412527765838513\n",
            "Step 18402, train_loss 0.4532329687598149, test_loss 0.44125249120511234\n",
            "Step 18403, train_loss 0.4532324368643663, test_loss 0.44125220588367164\n",
            "Step 18404, train_loss 0.45323190501514254, test_loss 0.4412519206195185\n",
            "Step 18405, train_loss 0.45323137321213586, test_loss 0.44125163541264206\n",
            "Step 18406, train_loss 0.45323084145533904, test_loss 0.4412513502630315\n",
            "Step 18407, train_loss 0.4532303097447446, test_loss 0.4412510651706759\n",
            "Step 18408, train_loss 0.4532297780803451, test_loss 0.4412507801355648\n",
            "Step 18409, train_loss 0.4532292464621329, test_loss 0.44125049515768694\n",
            "Step 18410, train_loss 0.4532287148901009, test_loss 0.44125021023703187\n",
            "Step 18411, train_loss 0.4532281833642414, test_loss 0.4412499253735886\n",
            "Step 18412, train_loss 0.4532276518845471, test_loss 0.44124964056734667\n",
            "Step 18413, train_loss 0.45322712045101043, test_loss 0.4412493558182948\n",
            "Step 18414, train_loss 0.45322658906362406, test_loss 0.44124907112642253\n",
            "Step 18415, train_loss 0.4532260577223805, test_loss 0.4412487864917191\n",
            "Step 18416, train_loss 0.4532255264272725, test_loss 0.4412485019141735\n",
            "Step 18417, train_loss 0.4532249951782924, test_loss 0.4412482173937751\n",
            "Step 18418, train_loss 0.4532244639754328, test_loss 0.4412479329305132\n",
            "Step 18419, train_loss 0.45322393281868645, test_loss 0.4412476485243769\n",
            "Step 18420, train_loss 0.45322340170804587, test_loss 0.4412473641753555\n",
            "Step 18421, train_loss 0.45322287064350353, test_loss 0.4412470798834381\n",
            "Step 18422, train_loss 0.45322233962505204, test_loss 0.44124679564861424\n",
            "Step 18423, train_loss 0.4532218086526841, test_loss 0.4412465114708727\n",
            "Step 18424, train_loss 0.45322127772639215, test_loss 0.4412462273502032\n",
            "Step 18425, train_loss 0.453220746846169, test_loss 0.4412459432865948\n",
            "Step 18426, train_loss 0.453220216012007, test_loss 0.44124565928003656\n",
            "Step 18427, train_loss 0.4532196852238989, test_loss 0.4412453753305179\n",
            "Step 18428, train_loss 0.45321915448183725, test_loss 0.4412450914380281\n",
            "Step 18429, train_loss 0.4532186237858145, test_loss 0.44124480760255635\n",
            "Step 18430, train_loss 0.4532180931358235, test_loss 0.4412445238240919\n",
            "Step 18431, train_loss 0.45321756253185674, test_loss 0.4412442401026242\n",
            "Step 18432, train_loss 0.45321703197390684, test_loss 0.4412439564381422\n",
            "Step 18433, train_loss 0.45321650146196646, test_loss 0.44124367283063526\n",
            "Step 18434, train_loss 0.4532159709960281, test_loss 0.44124338928009293\n",
            "Step 18435, train_loss 0.4532154405760844, test_loss 0.44124310578650405\n",
            "Step 18436, train_loss 0.45321491020212795, test_loss 0.4412428223498581\n",
            "Step 18437, train_loss 0.4532143798741515, test_loss 0.44124253897014454\n",
            "Step 18438, train_loss 0.45321384959214744, test_loss 0.44124225564735237\n",
            "Step 18439, train_loss 0.45321331935610865, test_loss 0.44124197238147106\n",
            "Step 18440, train_loss 0.4532127891660276, test_loss 0.4412416891724896\n",
            "Step 18441, train_loss 0.4532122590218969, test_loss 0.44124140602039763\n",
            "Step 18442, train_loss 0.4532117289237093, test_loss 0.44124112292518414\n",
            "Step 18443, train_loss 0.45321119887145717, test_loss 0.44124083988683876\n",
            "Step 18444, train_loss 0.4532106688651334, test_loss 0.44124055690535047\n",
            "Step 18445, train_loss 0.45321013890473055, test_loss 0.4412402739807088\n",
            "Step 18446, train_loss 0.4532096089902412, test_loss 0.4412399911129028\n",
            "Step 18447, train_loss 0.4532090791216581, test_loss 0.4412397083019219\n",
            "Step 18448, train_loss 0.4532085492989737, test_loss 0.4412394255477555\n",
            "Step 18449, train_loss 0.45320801952218076, test_loss 0.44123914285039284\n",
            "Step 18450, train_loss 0.4532074897912719, test_loss 0.441238860209823\n",
            "Step 18451, train_loss 0.45320696010623973, test_loss 0.44123857762603574\n",
            "Step 18452, train_loss 0.45320643046707704, test_loss 0.4412382950990201\n",
            "Step 18453, train_loss 0.4532059008737763, test_loss 0.4412380126287655\n",
            "Step 18454, train_loss 0.45320537132633026, test_loss 0.4412377302152612\n",
            "Step 18455, train_loss 0.4532048418247316, test_loss 0.44123744785849633\n",
            "Step 18456, train_loss 0.4532043123689728, test_loss 0.4412371655584605\n",
            "Step 18457, train_loss 0.45320378295904673, test_loss 0.44123688331514305\n",
            "Step 18458, train_loss 0.4532032535949459, test_loss 0.44123660112853313\n",
            "Step 18459, train_loss 0.453202724276663, test_loss 0.4412363189986202\n",
            "Step 18460, train_loss 0.45320219500419073, test_loss 0.4412360369253935\n",
            "Step 18461, train_loss 0.4532016657775217, test_loss 0.44123575490884254\n",
            "Step 18462, train_loss 0.4532011365966486, test_loss 0.44123547294895643\n",
            "Step 18463, train_loss 0.45320060746156415, test_loss 0.4412351910457246\n",
            "Step 18464, train_loss 0.45320007837226106, test_loss 0.4412349091991364\n",
            "Step 18465, train_loss 0.4531995493287318, test_loss 0.44123462740918135\n",
            "Step 18466, train_loss 0.45319902033096926, test_loss 0.4412343456758485\n",
            "Step 18467, train_loss 0.45319849137896595, test_loss 0.44123406399912757\n",
            "Step 18468, train_loss 0.4531979624727146, test_loss 0.44123378237900746\n",
            "Step 18469, train_loss 0.45319743361220793, test_loss 0.4412335008154779\n",
            "Step 18470, train_loss 0.45319690479743857, test_loss 0.4412332193085282\n",
            "Step 18471, train_loss 0.45319637602839924, test_loss 0.44123293785814754\n",
            "Step 18472, train_loss 0.4531958473050826, test_loss 0.4412326564643254\n",
            "Step 18473, train_loss 0.4531953186274814, test_loss 0.44123237512705127\n",
            "Step 18474, train_loss 0.4531947899955883, test_loss 0.4412320938463143\n",
            "Step 18475, train_loss 0.45319426140939595, test_loss 0.44123181262210404\n",
            "Step 18476, train_loss 0.453193732868897, test_loss 0.4412315314544098\n",
            "Step 18477, train_loss 0.4531932043740842, test_loss 0.44123125034322097\n",
            "Step 18478, train_loss 0.4531926759249504, test_loss 0.4412309692885268\n",
            "Step 18479, train_loss 0.4531921475214882, test_loss 0.4412306882903168\n",
            "Step 18480, train_loss 0.4531916191636901, test_loss 0.44123040734858066\n",
            "Step 18481, train_loss 0.453191090851549, test_loss 0.4412301264633072\n",
            "Step 18482, train_loss 0.4531905625850575, test_loss 0.4412298456344861\n",
            "Step 18483, train_loss 0.4531900343642086, test_loss 0.4412295648621068\n",
            "Step 18484, train_loss 0.45318950618899473, test_loss 0.4412292841461587\n",
            "Step 18485, train_loss 0.45318897805940855, test_loss 0.4412290034866311\n",
            "Step 18486, train_loss 0.45318844997544294, test_loss 0.44122872288351334\n",
            "Step 18487, train_loss 0.4531879219370906, test_loss 0.4412284423367951\n",
            "Step 18488, train_loss 0.45318739394434426, test_loss 0.4412281618464655\n",
            "Step 18489, train_loss 0.45318686599719643, test_loss 0.4412278814125142\n",
            "Step 18490, train_loss 0.4531863380956402, test_loss 0.4412276010349303\n",
            "Step 18491, train_loss 0.4531858102396679, test_loss 0.44122732071370363\n",
            "Step 18492, train_loss 0.45318528242927253, test_loss 0.44122704044882327\n",
            "Step 18493, train_loss 0.45318475466444674, test_loss 0.44122676024027874\n",
            "Step 18494, train_loss 0.45318422694518323, test_loss 0.44122648008805965\n",
            "Step 18495, train_loss 0.4531836992714747, test_loss 0.441226199992155\n",
            "Step 18496, train_loss 0.453183171643314, test_loss 0.4412259199525547\n",
            "Step 18497, train_loss 0.4531826440606938, test_loss 0.44122563996924796\n",
            "Step 18498, train_loss 0.4531821165236068, test_loss 0.4412253600422241\n",
            "Step 18499, train_loss 0.45318158903204586, test_loss 0.4412250801714727\n",
            "Step 18501, train_loss 0.4531805341854728, test_loss 0.4412245205987451\n",
            "Step 18502, train_loss 0.45318000683044624, test_loss 0.4412242408967478\n",
            "Step 18503, train_loss 0.45317947952091653, test_loss 0.4412239612509806\n",
            "Step 18504, train_loss 0.4531789522568767, test_loss 0.4412236816614331\n",
            "Step 18505, train_loss 0.45317842503831923, test_loss 0.44122340212809474\n",
            "Step 18506, train_loss 0.45317789786523704, test_loss 0.441223122650955\n",
            "Step 18507, train_loss 0.4531773707376227, test_loss 0.44122284323000327\n",
            "Step 18508, train_loss 0.4531768436554692, test_loss 0.44122256386522896\n",
            "Step 18509, train_loss 0.4531763166187691, test_loss 0.44122228455662166\n",
            "Step 18510, train_loss 0.4531757896275153, test_loss 0.44122200530417083\n",
            "Step 18511, train_loss 0.45317526268170055, test_loss 0.441221726107866\n",
            "Step 18512, train_loss 0.45317473578131745, test_loss 0.4412214469676964\n",
            "Step 18513, train_loss 0.45317420892635907, test_loss 0.4412211678836516\n",
            "Step 18514, train_loss 0.45317368211681786, test_loss 0.44122088885572136\n",
            "Step 18515, train_loss 0.4531731553526868, test_loss 0.44122060988389467\n",
            "Step 18516, train_loss 0.45317262863395863, test_loss 0.4412203309681614\n",
            "Step 18517, train_loss 0.4531721019606261, test_loss 0.44122005210851084\n",
            "Step 18518, train_loss 0.45317157533268193, test_loss 0.44121977330493256\n",
            "Step 18519, train_loss 0.45317104875011893, test_loss 0.44121949455741605\n",
            "Step 18520, train_loss 0.4531705222129301, test_loss 0.4412192158659507\n",
            "Step 18521, train_loss 0.45316999572110783, test_loss 0.4412189372305262\n",
            "Step 18522, train_loss 0.45316946927464513, test_loss 0.44121865865113186\n",
            "Step 18523, train_loss 0.4531689428735348, test_loss 0.4412183801277572\n",
            "Step 18524, train_loss 0.45316841651776957, test_loss 0.4412181016603918\n",
            "Step 18525, train_loss 0.4531678902073424, test_loss 0.44121782324902525\n",
            "Step 18526, train_loss 0.4531673639422457, test_loss 0.44121754489364695\n",
            "Step 18527, train_loss 0.4531668377224726, test_loss 0.4412172665942461\n",
            "Step 18528, train_loss 0.45316631154801584, test_loss 0.4412169883508129\n",
            "Step 18529, train_loss 0.4531657854188682, test_loss 0.4412167101633363\n",
            "Step 18530, train_loss 0.4531652593350224, test_loss 0.4412164320318061\n",
            "Step 18531, train_loss 0.45316473329647133, test_loss 0.44121615395621155\n",
            "Step 18532, train_loss 0.4531642073032078, test_loss 0.4412158759365425\n",
            "Step 18533, train_loss 0.45316368135522456, test_loss 0.44121559797278825\n",
            "Step 18534, train_loss 0.45316315545251434, test_loss 0.4412153200649385\n",
            "Step 18535, train_loss 0.45316262959507014, test_loss 0.4412150422129826\n",
            "Step 18536, train_loss 0.4531621037828848, test_loss 0.4412147644169102\n",
            "Step 18537, train_loss 0.45316157801595086, test_loss 0.4412144866767108\n",
            "Step 18538, train_loss 0.4531610522942614, test_loss 0.4412142089923739\n",
            "Step 18539, train_loss 0.4531605266178091, test_loss 0.44121393136388914\n",
            "Step 18540, train_loss 0.45316000098658676, test_loss 0.4412136537912459\n",
            "Step 18541, train_loss 0.45315947540058743, test_loss 0.44121337627443397\n",
            "Step 18542, train_loss 0.4531589498598037, test_loss 0.4412130988134426\n",
            "Step 18543, train_loss 0.4531584243642283, test_loss 0.4412128214082617\n",
            "Step 18544, train_loss 0.45315789891385455, test_loss 0.44121254405888044\n",
            "Step 18545, train_loss 0.4531573735086747, test_loss 0.4412122667652887\n",
            "Step 18546, train_loss 0.453156848148682, test_loss 0.4412119895274758\n",
            "Step 18547, train_loss 0.45315632283386903, test_loss 0.44121171234543155\n",
            "Step 18548, train_loss 0.4531557975642287, test_loss 0.4412114352191452\n",
            "Step 18549, train_loss 0.45315527233975383, test_loss 0.4412111581486065\n",
            "Step 18550, train_loss 0.4531547471604374, test_loss 0.441210881133805\n",
            "Step 18551, train_loss 0.4531542220262722, test_loss 0.44121060417473046\n",
            "Step 18552, train_loss 0.4531536969372509, test_loss 0.4412103272713721\n",
            "Step 18553, train_loss 0.4531531718933666, test_loss 0.44121005042371964\n",
            "Step 18554, train_loss 0.4531526468946118, test_loss 0.4412097736317628\n",
            "Step 18555, train_loss 0.4531521219409798, test_loss 0.4412094968954909\n",
            "Step 18556, train_loss 0.4531515970324631, test_loss 0.4412092202148938\n",
            "Step 18557, train_loss 0.45315107216905476, test_loss 0.4412089435899608\n",
            "Step 18558, train_loss 0.4531505473507475, test_loss 0.4412086670206819\n",
            "Step 18559, train_loss 0.4531500225775343, test_loss 0.4412083905070463\n",
            "Step 18560, train_loss 0.4531494978494079, test_loss 0.44120811404904375\n",
            "Step 18561, train_loss 0.45314897316636127, test_loss 0.4412078376466637\n",
            "Step 18562, train_loss 0.4531484485283871, test_loss 0.44120756129989613\n",
            "Step 18563, train_loss 0.4531479239354785, test_loss 0.4412072850087302\n",
            "Step 18564, train_loss 0.45314739938762816, test_loss 0.4412070087731558\n",
            "Step 18565, train_loss 0.453146874884829, test_loss 0.44120673259316234\n",
            "Step 18566, train_loss 0.4531463504270739, test_loss 0.4412064564687396\n",
            "Step 18567, train_loss 0.4531458260143558, test_loss 0.4412061803998771\n",
            "Step 18568, train_loss 0.45314530164666744, test_loss 0.44120590438656454\n",
            "Step 18569, train_loss 0.4531447773240017, test_loss 0.44120562842879146\n",
            "Step 18570, train_loss 0.45314425304635164, test_loss 0.44120535252654747\n",
            "Step 18571, train_loss 0.45314372881370996, test_loss 0.44120507667982223\n",
            "Step 18572, train_loss 0.45314320462606966, test_loss 0.44120480088860536\n",
            "Step 18573, train_loss 0.4531426804834235, test_loss 0.44120452515288655\n",
            "Step 18574, train_loss 0.4531421563857645, test_loss 0.44120424947265535\n",
            "Step 18575, train_loss 0.4531416323330854, test_loss 0.4412039738479012\n",
            "Step 18576, train_loss 0.4531411083253794, test_loss 0.44120369827861416\n",
            "Step 18577, train_loss 0.453140584362639, test_loss 0.4412034227647836\n",
            "Step 18578, train_loss 0.4531400604448573, test_loss 0.4412031473063992\n",
            "Step 18579, train_loss 0.4531395365720272, test_loss 0.4412028719034507\n",
            "Step 18580, train_loss 0.4531390127441415, test_loss 0.4412025965559275\n",
            "Step 18581, train_loss 0.4531384889611932, test_loss 0.4412023212638195\n",
            "Step 18582, train_loss 0.4531379652231752, test_loss 0.4412020460271164\n",
            "Step 18583, train_loss 0.4531374415300804, test_loss 0.4412017708458075\n",
            "Step 18584, train_loss 0.45313691788190164, test_loss 0.4412014957198827\n",
            "Step 18585, train_loss 0.4531363942786318, test_loss 0.44120122064933165\n",
            "Step 18586, train_loss 0.45313587072026396, test_loss 0.441200945634144\n",
            "Step 18587, train_loss 0.45313534720679083, test_loss 0.44120067067430935\n",
            "Step 18588, train_loss 0.4531348237382055, test_loss 0.4412003957698175\n",
            "Step 18589, train_loss 0.45313430031450075, test_loss 0.4412001209206579\n",
            "Step 18590, train_loss 0.4531337769356697, test_loss 0.44119984612682045\n",
            "Step 18591, train_loss 0.453133253601705, test_loss 0.44119957138829463\n",
            "Step 18592, train_loss 0.4531327303125999, test_loss 0.4411992967050703\n",
            "Step 18593, train_loss 0.4531322070683469, test_loss 0.441199022077137\n",
            "Step 18594, train_loss 0.45313168386893926, test_loss 0.44119874750448457\n",
            "Step 18595, train_loss 0.4531311607143699, test_loss 0.4411984729871025\n",
            "Step 18596, train_loss 0.4531306376046315, test_loss 0.44119819852498054\n",
            "Step 18597, train_loss 0.4531301145397172, test_loss 0.44119792411810843\n",
            "Step 18598, train_loss 0.45312959151961985, test_loss 0.44119764976647574\n",
            "Step 18599, train_loss 0.4531290685443325, test_loss 0.4411973754700723\n",
            "Step 18601, train_loss 0.45312802272815916, test_loss 0.4411968270429118\n",
            "Step 18602, train_loss 0.4531274998872592, test_loss 0.4411965529121342\n",
            "Step 18603, train_loss 0.45312697709114086, test_loss 0.44119627883654455\n",
            "Step 18604, train_loss 0.4531264543397972, test_loss 0.4411960048161325\n",
            "Step 18605, train_loss 0.453125931633221, test_loss 0.44119573085088803\n",
            "Step 18606, train_loss 0.4531254089714054, test_loss 0.4411954569408006\n",
            "Step 18607, train_loss 0.4531248863543433, test_loss 0.4411951830858601\n",
            "Step 18608, train_loss 0.4531243637820276, test_loss 0.4411949092860561\n",
            "Step 18609, train_loss 0.4531238412544512, test_loss 0.44119463554137833\n",
            "Step 18610, train_loss 0.4531233187716072, test_loss 0.44119436185181654\n",
            "Step 18611, train_loss 0.4531227963334885, test_loss 0.44119408821736045\n",
            "Step 18612, train_loss 0.45312227394008814, test_loss 0.4411938146379998\n",
            "Step 18613, train_loss 0.4531217515913988, test_loss 0.44119354111372433\n",
            "Step 18614, train_loss 0.45312122928741383, test_loss 0.4411932676445238\n",
            "Step 18615, train_loss 0.453120707028126, test_loss 0.4411929942303879\n",
            "Step 18616, train_loss 0.4531201848135282, test_loss 0.4411927208713064\n",
            "Step 18617, train_loss 0.45311966264361353, test_loss 0.44119244756726894\n",
            "Step 18618, train_loss 0.45311914051837493, test_loss 0.4411921743182653\n",
            "Step 18619, train_loss 0.4531186184378054, test_loss 0.4411919011242852\n",
            "Step 18620, train_loss 0.4531180964018977, test_loss 0.4411916279853186\n",
            "Step 18621, train_loss 0.4531175744106452, test_loss 0.44119135490135497\n",
            "Step 18622, train_loss 0.4531170524640406, test_loss 0.44119108187238426\n",
            "Step 18623, train_loss 0.4531165305620769, test_loss 0.44119080889839596\n",
            "Step 18624, train_loss 0.45311600870474716, test_loss 0.4411905359793801\n",
            "Step 18625, train_loss 0.4531154868920444, test_loss 0.4411902631153265\n",
            "Step 18626, train_loss 0.4531149651239615, test_loss 0.44118999030622463\n",
            "Step 18627, train_loss 0.4531144434004916, test_loss 0.4411897175520644\n",
            "Step 18628, train_loss 0.45311392172162746, test_loss 0.4411894448528356\n",
            "Step 18629, train_loss 0.4531134000873623, test_loss 0.44118917220852777\n",
            "Step 18630, train_loss 0.4531128784976889, test_loss 0.44118889961913105\n",
            "Step 18631, train_loss 0.45311235695260044, test_loss 0.441188627084635\n",
            "Step 18632, train_loss 0.4531118354520899, test_loss 0.44118835460502936\n",
            "Step 18633, train_loss 0.45311131399615023, test_loss 0.44118808218030403\n",
            "Step 18634, train_loss 0.4531107925847744, test_loss 0.4411878098104488\n",
            "Step 18635, train_loss 0.45311027121795544, test_loss 0.44118753749545325\n",
            "Step 18636, train_loss 0.45310974989568636, test_loss 0.4411872652353075\n",
            "Step 18637, train_loss 0.4531092286179603, test_loss 0.44118699303000103\n",
            "Step 18638, train_loss 0.45310870738477, test_loss 0.44118672087952365\n",
            "Step 18639, train_loss 0.45310818619610865, test_loss 0.4411864487838654\n",
            "Step 18640, train_loss 0.45310766505196926, test_loss 0.44118617674301586\n",
            "Step 18641, train_loss 0.4531071439523448, test_loss 0.44118590475696495\n",
            "Step 18642, train_loss 0.45310662289722836, test_loss 0.44118563282570233\n",
            "Step 18643, train_loss 0.45310610188661293, test_loss 0.4411853609492179\n",
            "Step 18644, train_loss 0.4531055809204914, test_loss 0.44118508912750143\n",
            "Step 18645, train_loss 0.453105059998857, test_loss 0.4411848173605428\n",
            "Step 18646, train_loss 0.4531045391217027, test_loss 0.4411845456483318\n",
            "Step 18647, train_loss 0.45310401828902136, test_loss 0.44118427399085813\n",
            "Step 18648, train_loss 0.45310349750080625, test_loss 0.44118400238811173\n",
            "Step 18649, train_loss 0.45310297675705014, test_loss 0.4411837308400823\n",
            "Step 18650, train_loss 0.45310245605774635, test_loss 0.4411834593467599\n",
            "Step 18651, train_loss 0.45310193540288773, test_loss 0.44118318790813404\n",
            "Step 18652, train_loss 0.45310141479246735, test_loss 0.4411829165241947\n",
            "Step 18653, train_loss 0.45310089422647826, test_loss 0.44118264519493183\n",
            "Step 18654, train_loss 0.45310037370491363, test_loss 0.441182373920335\n",
            "Step 18655, train_loss 0.4530998532277662, test_loss 0.44118210270039415\n",
            "Step 18656, train_loss 0.45309933279502923, test_loss 0.4411818315350993\n",
            "Step 18657, train_loss 0.45309881240669586, test_loss 0.44118156042444007\n",
            "Step 18658, train_loss 0.45309829206275887, test_loss 0.44118128936840617\n",
            "Step 18659, train_loss 0.45309777176321153, test_loss 0.4411810183669878\n",
            "Step 18660, train_loss 0.45309725150804675, test_loss 0.4411807474201747\n",
            "Step 18661, train_loss 0.45309673129725764, test_loss 0.44118047652795644\n",
            "Step 18662, train_loss 0.4530962111308372, test_loss 0.4411802056903233\n",
            "Step 18663, train_loss 0.45309569100877867, test_loss 0.44117993490726487\n",
            "Step 18664, train_loss 0.45309517093107493, test_loss 0.44117966417877086\n",
            "Step 18665, train_loss 0.4530946508977191, test_loss 0.4411793935048315\n",
            "Step 18666, train_loss 0.45309413090870426, test_loss 0.4411791228854364\n",
            "Step 18667, train_loss 0.45309361096402345, test_loss 0.4411788523205756\n",
            "Step 18668, train_loss 0.45309309106366974, test_loss 0.4411785818102387\n",
            "Step 18669, train_loss 0.45309257120763613, test_loss 0.44117831135441576\n",
            "Step 18670, train_loss 0.45309205139591585, test_loss 0.44117804095309665\n",
            "Step 18671, train_loss 0.4530915316285018, test_loss 0.44117777060627117\n",
            "Step 18672, train_loss 0.4530910119053872, test_loss 0.44117750031392927\n",
            "Step 18673, train_loss 0.45309049222656506, test_loss 0.4411772300760607\n",
            "Step 18674, train_loss 0.4530899725920285, test_loss 0.44117695989265543\n",
            "Step 18675, train_loss 0.45308945300177056, test_loss 0.4411766897637034\n",
            "Step 18676, train_loss 0.4530889334557842, test_loss 0.4411764196891944\n",
            "Step 18677, train_loss 0.45308841395406285, test_loss 0.4411761496691184\n",
            "Step 18678, train_loss 0.4530878944965992, test_loss 0.4411758797034651\n",
            "Step 18679, train_loss 0.4530873750833866, test_loss 0.44117560979222453\n",
            "Step 18680, train_loss 0.4530868557144181, test_loss 0.44117533993538677\n",
            "Step 18681, train_loss 0.45308633638968665, test_loss 0.4411750701329415\n",
            "Step 18682, train_loss 0.45308581710918544, test_loss 0.4411748003848785\n",
            "Step 18683, train_loss 0.4530852978729076, test_loss 0.44117453069118795\n",
            "Step 18684, train_loss 0.4530847786808462, test_loss 0.44117426105185953\n",
            "Step 18685, train_loss 0.45308425953299425, test_loss 0.4411739914668832\n",
            "Step 18686, train_loss 0.45308374042934496, test_loss 0.441173721936249\n",
            "Step 18687, train_loss 0.4530832213698916, test_loss 0.44117345245994677\n",
            "Step 18688, train_loss 0.4530827023546268, test_loss 0.4411731830379662\n",
            "Step 18689, train_loss 0.4530821833835441, test_loss 0.44117291367029765\n",
            "Step 18690, train_loss 0.4530816644566364, test_loss 0.44117264435693077\n",
            "Step 18691, train_loss 0.45308114557389695, test_loss 0.44117237509785534\n",
            "Step 18692, train_loss 0.45308062673531874, test_loss 0.4411721058930616\n",
            "Step 18693, train_loss 0.4530801079408948, test_loss 0.44117183674253935\n",
            "Step 18694, train_loss 0.4530795891906185, test_loss 0.44117156764627846\n",
            "Step 18695, train_loss 0.45307907048448287, test_loss 0.44117129860426885\n",
            "Step 18696, train_loss 0.45307855182248086, test_loss 0.44117102961650057\n",
            "Step 18697, train_loss 0.4530780332046057, test_loss 0.4411707606829634\n",
            "Step 18698, train_loss 0.4530775146308506, test_loss 0.44117049180364737\n",
            "Step 18699, train_loss 0.45307699610120855, test_loss 0.4411702229785424\n",
            "Step 18701, train_loss 0.4530759591742365, test_loss 0.4411696854909256\n",
            "Step 18702, train_loss 0.4530754407768926, test_loss 0.44116941682839356\n",
            "Step 18703, train_loss 0.4530749224236344, test_loss 0.4411691482200325\n",
            "Step 18704, train_loss 0.4530744041144548, test_loss 0.44116887966583196\n",
            "Step 18705, train_loss 0.45307388584934727, test_loss 0.4411686111657825\n",
            "Step 18706, train_loss 0.4530733676283047, test_loss 0.4411683427198736\n",
            "Step 18707, train_loss 0.4530728494513204, test_loss 0.4411680743280954\n",
            "Step 18708, train_loss 0.45307233131838737, test_loss 0.44116780599043787\n",
            "Step 18709, train_loss 0.45307181322949874, test_loss 0.44116753770689093\n",
            "Step 18710, train_loss 0.4530712951846478, test_loss 0.44116726947744467\n",
            "Step 18711, train_loss 0.4530707771838277, test_loss 0.4411670013020889\n",
            "Step 18712, train_loss 0.4530702592270314, test_loss 0.4411667331808136\n",
            "Step 18713, train_loss 0.45306974131425226, test_loss 0.44116646511360896\n",
            "Step 18714, train_loss 0.4530692234454833, test_loss 0.44116619710046456\n",
            "Step 18715, train_loss 0.4530687056207175, test_loss 0.4411659291413707\n",
            "Step 18716, train_loss 0.4530681878399485, test_loss 0.4411656612363173\n",
            "Step 18717, train_loss 0.453067670103169, test_loss 0.4411653933852944\n",
            "Step 18718, train_loss 0.45306715241037243, test_loss 0.44116512558829185\n",
            "Step 18719, train_loss 0.45306663476155185, test_loss 0.44116485784529974\n",
            "Step 18720, train_loss 0.45306611715670053, test_loss 0.441164590156308\n",
            "Step 18721, train_loss 0.45306559959581144, test_loss 0.44116432252130666\n",
            "Step 18722, train_loss 0.45306508207887786, test_loss 0.44116405494028554\n",
            "Step 18723, train_loss 0.453064564605893, test_loss 0.44116378741323503\n",
            "Step 18724, train_loss 0.45306404717685006, test_loss 0.44116351994014463\n",
            "Step 18725, train_loss 0.4530635297917419, test_loss 0.4411632525210049\n",
            "Step 18726, train_loss 0.4530630124505622, test_loss 0.44116298515580543\n",
            "Step 18727, train_loss 0.4530624951533037, test_loss 0.4411627178445365\n",
            "Step 18728, train_loss 0.45306197789995983, test_loss 0.4411624505871878\n",
            "Step 18729, train_loss 0.45306146069052367, test_loss 0.4411621833837495\n",
            "Step 18730, train_loss 0.45306094352498844, test_loss 0.44116191623421164\n",
            "Step 18731, train_loss 0.4530604264033473, test_loss 0.44116164913856437\n",
            "Step 18732, train_loss 0.4530599093255934, test_loss 0.44116138209679745\n",
            "Step 18733, train_loss 0.45305939229172004, test_loss 0.44116111510890105\n",
            "Step 18734, train_loss 0.45305887530172034, test_loss 0.4411608481748652\n",
            "Step 18735, train_loss 0.4530583583555875, test_loss 0.4411605812946799\n",
            "Step 18736, train_loss 0.4530578414533146, test_loss 0.4411603144683353\n",
            "Step 18737, train_loss 0.45305732459489506, test_loss 0.4411600476958211\n",
            "Step 18738, train_loss 0.45305680778032204, test_loss 0.4411597809771278\n",
            "Step 18739, train_loss 0.45305629100958844, test_loss 0.441159514312245\n",
            "Step 18740, train_loss 0.45305577428268784, test_loss 0.441159247701163\n",
            "Step 18741, train_loss 0.45305525759961324, test_loss 0.44115898114387164\n",
            "Step 18742, train_loss 0.45305474096035786, test_loss 0.4411587146403613\n",
            "Step 18743, train_loss 0.4530542243649149, test_loss 0.44115844819062183\n",
            "Step 18744, train_loss 0.45305370781327775, test_loss 0.44115818179464306\n",
            "Step 18745, train_loss 0.45305319130543936, test_loss 0.4411579154524154\n",
            "Step 18746, train_loss 0.4530526748413931, test_loss 0.44115764916392886\n",
            "Step 18747, train_loss 0.45305215842113206, test_loss 0.4411573829291733\n",
            "Step 18748, train_loss 0.45305164204464965, test_loss 0.44115711674813873\n",
            "Step 18749, train_loss 0.45305112571193895, test_loss 0.44115685062081555\n",
            "Step 18750, train_loss 0.45305060942299313, test_loss 0.4411565845471936\n",
            "Step 18751, train_loss 0.45305009317780554, test_loss 0.44115631852726284\n",
            "Step 18752, train_loss 0.45304957697636933, test_loss 0.44115605256101365\n",
            "Step 18753, train_loss 0.45304906081867774, test_loss 0.4411557866484358\n",
            "Step 18754, train_loss 0.453048544704724, test_loss 0.44115552078951953\n",
            "Step 18755, train_loss 0.45304802863450133, test_loss 0.44115525498425484\n",
            "Step 18756, train_loss 0.4530475126080031, test_loss 0.4411549892326318\n",
            "Step 18757, train_loss 0.45304699662522224, test_loss 0.4411547235346407\n",
            "Step 18758, train_loss 0.4530464806861521, test_loss 0.4411544578902714\n",
            "Step 18759, train_loss 0.45304596479078607, test_loss 0.441154192299514\n",
            "Step 18760, train_loss 0.4530454489391173, test_loss 0.4411539267623588\n",
            "Step 18761, train_loss 0.45304493313113897, test_loss 0.4411536612787955\n",
            "Step 18762, train_loss 0.4530444173668445, test_loss 0.4411533958488144\n",
            "Step 18763, train_loss 0.45304390164622677, test_loss 0.4411531304724058\n",
            "Step 18764, train_loss 0.45304338596927934, test_loss 0.44115286514955937\n",
            "Step 18765, train_loss 0.4530428703359955, test_loss 0.4411525998802658\n",
            "Step 18766, train_loss 0.45304235474636817, test_loss 0.44115233466451453\n",
            "Step 18767, train_loss 0.453041839200391, test_loss 0.4411520695022961\n",
            "Step 18768, train_loss 0.4530413236980569, test_loss 0.44115180439360047\n",
            "Step 18769, train_loss 0.45304080823935927, test_loss 0.44115153933841783\n",
            "Step 18770, train_loss 0.4530402928242914, test_loss 0.4411512743367382\n",
            "Step 18771, train_loss 0.45303977745284657, test_loss 0.44115100938855156\n",
            "Step 18772, train_loss 0.4530392621250179, test_loss 0.4411507444938484\n",
            "Step 18773, train_loss 0.45303874684079876, test_loss 0.44115047965261855\n",
            "Step 18774, train_loss 0.45303823160018236, test_loss 0.4411502148648522\n",
            "Step 18775, train_loss 0.4530377164031619, test_loss 0.4411499501305394\n",
            "Step 18776, train_loss 0.4530372012497309, test_loss 0.4411496854496705\n",
            "Step 18777, train_loss 0.45303668613988235, test_loss 0.4411494208222355\n",
            "Step 18778, train_loss 0.4530361710736097, test_loss 0.44114915624822454\n",
            "Step 18779, train_loss 0.4530356560509062, test_loss 0.44114889172762767\n",
            "Step 18780, train_loss 0.45303514107176496, test_loss 0.4411486272604351\n",
            "Step 18781, train_loss 0.4530346261361794, test_loss 0.44114836284663694\n",
            "Step 18782, train_loss 0.45303411124414283, test_loss 0.4411480984862234\n",
            "Step 18783, train_loss 0.4530335963956484, test_loss 0.4411478341791845\n",
            "Step 18784, train_loss 0.4530330815906895, test_loss 0.4411475699255105\n",
            "Step 18785, train_loss 0.4530325668292593, test_loss 0.4411473057251915\n",
            "Step 18786, train_loss 0.4530320521113514, test_loss 0.4411470415782177\n",
            "Step 18787, train_loss 0.4530315374369585, test_loss 0.4411467774845792\n",
            "Step 18788, train_loss 0.45303102280607455, test_loss 0.4411465134442661\n",
            "Step 18789, train_loss 0.45303050821869234, test_loss 0.44114624945726866\n",
            "Step 18790, train_loss 0.45302999367480545, test_loss 0.441145985523577\n",
            "Step 18791, train_loss 0.45302947917440695, test_loss 0.44114572164318133\n",
            "Step 18792, train_loss 0.4530289647174903, test_loss 0.4411454578160716\n",
            "Step 18793, train_loss 0.4530284503040488, test_loss 0.4411451940422383\n",
            "Step 18794, train_loss 0.45302793593407564, test_loss 0.44114493032167124\n",
            "Step 18795, train_loss 0.45302742160756426, test_loss 0.4411446666543611\n",
            "Step 18796, train_loss 0.45302690732450784, test_loss 0.4411444030402975\n",
            "Step 18797, train_loss 0.4530263930848998, test_loss 0.4411441394794709\n",
            "Step 18798, train_loss 0.45302587888873325, test_loss 0.44114387597187155\n",
            "Step 18799, train_loss 0.4530253647360017, test_loss 0.44114361251748946\n",
            "Step 18801, train_loss 0.45302433656081653, test_loss 0.4411430857683379\n",
            "Step 18802, train_loss 0.4530238225383497, test_loss 0.44114282247354886\n",
            "Step 18803, train_loss 0.453023308559291, test_loss 0.44114255923193785\n",
            "Step 18804, train_loss 0.4530227946236337, test_loss 0.4411422960434951\n",
            "Step 18805, train_loss 0.45302228073137135, test_loss 0.44114203290821086\n",
            "Step 18806, train_loss 0.45302176688249696, test_loss 0.4411417698260753\n",
            "Step 18807, train_loss 0.4530212530770042, test_loss 0.4411415067970785\n",
            "Step 18808, train_loss 0.4530207393148862, test_loss 0.44114124382121067\n",
            "Step 18809, train_loss 0.4530202255961362, test_loss 0.44114098089846226\n",
            "Step 18810, train_loss 0.4530197119207477, test_loss 0.44114071802882326\n",
            "Step 18811, train_loss 0.4530191982887139, test_loss 0.44114045521228396\n",
            "Step 18812, train_loss 0.4530186847000282, test_loss 0.44114019244883446\n",
            "Step 18813, train_loss 0.4530181711546839, test_loss 0.44113992973846516\n",
            "Step 18814, train_loss 0.4530176576526746, test_loss 0.44113966708116614\n",
            "Step 18815, train_loss 0.45301714419399325, test_loss 0.4411394044769276\n",
            "Step 18816, train_loss 0.4530166307786333, test_loss 0.4411391419257399\n",
            "Step 18817, train_loss 0.4530161174065881, test_loss 0.44113887942759294\n",
            "Step 18818, train_loss 0.45301560407785113, test_loss 0.4411386169824774\n",
            "Step 18819, train_loss 0.45301509079241553, test_loss 0.4411383545903833\n",
            "Step 18820, train_loss 0.45301457755027486, test_loss 0.44113809225130074\n",
            "Step 18821, train_loss 0.4530140643514222, test_loss 0.4411378299652203\n",
            "Step 18822, train_loss 0.4530135511958512, test_loss 0.4411375677321318\n",
            "Step 18823, train_loss 0.4530130380835551, test_loss 0.44113730555202557\n",
            "Step 18824, train_loss 0.453012525014527, test_loss 0.4411370434248921\n",
            "Step 18825, train_loss 0.45301201198876057, test_loss 0.44113678135072143\n",
            "Step 18826, train_loss 0.4530114990062491, test_loss 0.4411365193295038\n",
            "Step 18827, train_loss 0.45301098606698587, test_loss 0.44113625736122963\n",
            "Step 18828, train_loss 0.45301047317096427, test_loss 0.4411359954458889\n",
            "Step 18829, train_loss 0.4530099603181777, test_loss 0.441135733583472\n",
            "Step 18830, train_loss 0.4530094475086195, test_loss 0.4411354717739694\n",
            "Step 18831, train_loss 0.4530089347422831, test_loss 0.44113521001737094\n",
            "Step 18832, train_loss 0.4530084220191617, test_loss 0.44113494831366706\n",
            "Step 18833, train_loss 0.4530079093392489, test_loss 0.4411346866628482\n",
            "Step 18834, train_loss 0.4530073967025378, test_loss 0.44113442506490436\n",
            "Step 18835, train_loss 0.45300688410902196, test_loss 0.44113416351982604\n",
            "Step 18836, train_loss 0.4530063715586947, test_loss 0.44113390202760316\n",
            "Step 18837, train_loss 0.45300585905154944, test_loss 0.4411336405882264\n",
            "Step 18838, train_loss 0.4530053465875795, test_loss 0.4411333792016857\n",
            "Step 18839, train_loss 0.4530048341667783, test_loss 0.4411331178679716\n",
            "Step 18840, train_loss 0.45300432178913913, test_loss 0.4411328565870742\n",
            "Step 18841, train_loss 0.45300380945465557, test_loss 0.4411325953589839\n",
            "Step 18842, train_loss 0.45300329716332083, test_loss 0.4411323341836908\n",
            "Step 18843, train_loss 0.4530027849151283, test_loss 0.4411320730611854\n",
            "Step 18844, train_loss 0.45300227271007154, test_loss 0.44113181199145785\n",
            "Step 18845, train_loss 0.4530017605481437, test_loss 0.4411315509744984\n",
            "Step 18846, train_loss 0.45300124842933837, test_loss 0.4411312900102975\n",
            "Step 18847, train_loss 0.45300073635364874, test_loss 0.4411310290988453\n",
            "Step 18848, train_loss 0.45300022432106835, test_loss 0.44113076824013214\n",
            "Step 18849, train_loss 0.4529997123315907, test_loss 0.44113050743414833\n",
            "Step 18850, train_loss 0.452999200385209, test_loss 0.4411302466808843\n",
            "Step 18851, train_loss 0.4529986884819167, test_loss 0.44112998598033004\n",
            "Step 18852, train_loss 0.4529981766217072, test_loss 0.44112972533247613\n",
            "Step 18853, train_loss 0.45299766480457404, test_loss 0.44112946473731285\n",
            "Step 18854, train_loss 0.45299715303051036, test_loss 0.44112920419483037\n",
            "Step 18855, train_loss 0.45299664129950984, test_loss 0.4411289437050191\n",
            "Step 18856, train_loss 0.4529961296115657, test_loss 0.44112868326786936\n",
            "Step 18857, train_loss 0.4529956179666714, test_loss 0.44112842288337145\n",
            "Step 18858, train_loss 0.4529951063648205, test_loss 0.4411281625515156\n",
            "Step 18859, train_loss 0.4529945948060062, test_loss 0.4411279022722922\n",
            "Step 18860, train_loss 0.45299408329022195, test_loss 0.44112764204569166\n",
            "Step 18861, train_loss 0.45299357181746136, test_loss 0.4411273818717042\n",
            "Step 18862, train_loss 0.4529930603877176, test_loss 0.44112712175032015\n",
            "Step 18863, train_loss 0.4529925490009842, test_loss 0.4411268616815299\n",
            "Step 18864, train_loss 0.45299203765725465, test_loss 0.44112660166532386\n",
            "Step 18865, train_loss 0.4529915263565223, test_loss 0.4411263417016922\n",
            "Step 18866, train_loss 0.4529910150987805, test_loss 0.44112608179062507\n",
            "Step 18867, train_loss 0.45299050388402295, test_loss 0.44112582193211336\n",
            "Step 18868, train_loss 0.45298999271224283, test_loss 0.44112556212614695\n",
            "Step 18869, train_loss 0.45298948158343355, test_loss 0.4411253023727164\n",
            "Step 18870, train_loss 0.45298897049758885, test_loss 0.4411250426718119\n",
            "Step 18871, train_loss 0.45298845945470173, test_loss 0.44112478302342406\n",
            "Step 18872, train_loss 0.4529879484547659, test_loss 0.4411245234275429\n",
            "Step 18873, train_loss 0.4529874374977749, test_loss 0.441124263884159\n",
            "Step 18874, train_loss 0.4529869265837218, test_loss 0.4411240043932627\n",
            "Step 18875, train_loss 0.4529864157126004, test_loss 0.44112374495484424\n",
            "Step 18876, train_loss 0.4529859048844041, test_loss 0.4411234855688942\n",
            "Step 18877, train_loss 0.45298539409912614, test_loss 0.4411232262354026\n",
            "Step 18878, train_loss 0.4529848833567601, test_loss 0.4411229669543601\n",
            "Step 18879, train_loss 0.4529843726572994, test_loss 0.44112270772575696\n",
            "Step 18880, train_loss 0.4529838620007376, test_loss 0.44112244854958366\n",
            "Step 18881, train_loss 0.45298335138706797, test_loss 0.44112218942583037\n",
            "Step 18882, train_loss 0.45298284081628415, test_loss 0.4411219303544875\n",
            "Step 18883, train_loss 0.45298233028837936, test_loss 0.44112167133554553\n",
            "Step 18884, train_loss 0.4529818198033473, test_loss 0.44112141236899494\n",
            "Step 18885, train_loss 0.45298130936118136, test_loss 0.44112115345482583\n",
            "Step 18886, train_loss 0.452980798961875, test_loss 0.44112089459302894\n",
            "Step 18887, train_loss 0.4529802886054216, test_loss 0.4411206357835942\n",
            "Step 18888, train_loss 0.4529797782918147, test_loss 0.4411203770265123\n",
            "Step 18889, train_loss 0.45297926802104777, test_loss 0.4411201183217736\n",
            "Step 18890, train_loss 0.4529787577931143, test_loss 0.44111985966936856\n",
            "Step 18891, train_loss 0.4529782476080076, test_loss 0.4411196010692873\n",
            "Step 18892, train_loss 0.4529777374657214, test_loss 0.4411193425215205\n",
            "Step 18893, train_loss 0.452977227366249, test_loss 0.4411190840260585\n",
            "Step 18894, train_loss 0.45297671730958394, test_loss 0.44111882558289156\n",
            "Step 18895, train_loss 0.45297620729571975, test_loss 0.4411185671920103\n",
            "Step 18896, train_loss 0.45297569732464965, test_loss 0.4411183088534049\n",
            "Step 18897, train_loss 0.4529751873963675, test_loss 0.441118050567066\n",
            "Step 18898, train_loss 0.45297467751086645, test_loss 0.4411177923329838\n",
            "Step 18899, train_loss 0.4529741676681402, test_loss 0.44111753415114885\n",
            "Step 18901, train_loss 0.4529731481109858, test_loss 0.4411170179441823\n",
            "Step 18902, train_loss 0.45297263839654467, test_loss 0.4411167599190314\n",
            "Step 18903, train_loss 0.4529721287248522, test_loss 0.4411165019460894\n",
            "Step 18904, train_loss 0.45297161909590195, test_loss 0.4411162440253468\n",
            "Step 18905, train_loss 0.4529711095096874, test_loss 0.4411159861567939\n",
            "Step 18906, train_loss 0.4529705999662019, test_loss 0.4411157283404212\n",
            "Step 18907, train_loss 0.4529700904654392, test_loss 0.441115470576219\n",
            "Step 18908, train_loss 0.45296958100739254, test_loss 0.4411152128641778\n",
            "Step 18909, train_loss 0.4529690715920557, test_loss 0.4411149552042882\n",
            "Step 18910, train_loss 0.452968562219422, test_loss 0.44111469759654043\n",
            "Step 18911, train_loss 0.45296805288948494, test_loss 0.4411144400409251\n",
            "Step 18912, train_loss 0.45296754360223807, test_loss 0.44111418253743245\n",
            "Step 18913, train_loss 0.452967034357675, test_loss 0.4411139250860532\n",
            "Step 18914, train_loss 0.45296652515578906, test_loss 0.44111366768677746\n",
            "Step 18915, train_loss 0.4529660159965738, test_loss 0.4411134103395959\n",
            "Step 18916, train_loss 0.45296550688002285, test_loss 0.44111315304449894\n",
            "Step 18917, train_loss 0.4529649978061297, test_loss 0.441112895801477\n",
            "Step 18918, train_loss 0.4529644887748878, test_loss 0.44111263861052064\n",
            "Step 18919, train_loss 0.45296397978629066, test_loss 0.44111238147162013\n",
            "Step 18920, train_loss 0.45296347084033184, test_loss 0.4411121243847661\n",
            "Step 18921, train_loss 0.45296296193700486, test_loss 0.44111186734994906\n",
            "Step 18922, train_loss 0.45296245307630323, test_loss 0.4411116103671592\n",
            "Step 18923, train_loss 0.45296194425822056, test_loss 0.4411113534363872\n",
            "Step 18924, train_loss 0.4529614354827503, test_loss 0.4411110965576235\n",
            "Step 18925, train_loss 0.4529609267498859, test_loss 0.4411108397308585\n",
            "Step 18926, train_loss 0.4529604180596211, test_loss 0.441110582956083\n",
            "Step 18927, train_loss 0.4529599094119493, test_loss 0.44111032623328694\n",
            "Step 18928, train_loss 0.4529594008068641, test_loss 0.4411100695624611\n",
            "Step 18929, train_loss 0.4529588922443588, test_loss 0.4411098129435962\n",
            "Step 18930, train_loss 0.45295838372442726, test_loss 0.44110955637668214\n",
            "Step 18931, train_loss 0.4529578752470628, test_loss 0.44110929986170977\n",
            "Step 18932, train_loss 0.4529573668122592, test_loss 0.4411090433986697\n",
            "Step 18933, train_loss 0.4529568584200097, test_loss 0.4411087869875521\n",
            "Step 18934, train_loss 0.45295635007030804, test_loss 0.4411085306283477\n",
            "Step 18935, train_loss 0.45295584176314785, test_loss 0.44110827432104704\n",
            "Step 18936, train_loss 0.4529553334985224, test_loss 0.4411080180656404\n",
            "Step 18937, train_loss 0.4529548252764255, test_loss 0.44110776186211836\n",
            "Step 18938, train_loss 0.4529543170968505, test_loss 0.44110750571047136\n",
            "Step 18939, train_loss 0.45295380895979115, test_loss 0.44110724961069014\n",
            "Step 18940, train_loss 0.4529533008652408, test_loss 0.44110699356276484\n",
            "Step 18941, train_loss 0.45295279281319323, test_loss 0.44110673756668634\n",
            "Step 18942, train_loss 0.4529522848036418, test_loss 0.4411064816224449\n",
            "Step 18943, train_loss 0.45295177683658033, test_loss 0.4411062257300313\n",
            "Step 18944, train_loss 0.452951268912002, test_loss 0.44110596988943573\n",
            "Step 18945, train_loss 0.4529507610299006, test_loss 0.4411057141006489\n",
            "Step 18946, train_loss 0.4529502531902698, test_loss 0.4411054583636613\n",
            "Step 18947, train_loss 0.45294974539310295, test_loss 0.44110520267846337\n",
            "Step 18948, train_loss 0.4529492376383939, test_loss 0.4411049470450457\n",
            "Step 18949, train_loss 0.45294872992613583, test_loss 0.4411046914633989\n",
            "Step 18950, train_loss 0.4529482222563226, test_loss 0.4411044359335135\n",
            "Step 18951, train_loss 0.4529477146289477, test_loss 0.4411041804553799\n",
            "Step 18952, train_loss 0.4529472070440047, test_loss 0.4411039250289886\n",
            "Step 18953, train_loss 0.45294669950148725, test_loss 0.4411036696543303\n",
            "Step 18954, train_loss 0.4529461920013888, test_loss 0.4411034143313956\n",
            "Step 18955, train_loss 0.4529456845437031, test_loss 0.4411031590601748\n",
            "Step 18956, train_loss 0.4529451771284235, test_loss 0.44110290384065853\n",
            "Step 18957, train_loss 0.45294466975554376, test_loss 0.4411026486728374\n",
            "Step 18958, train_loss 0.45294416242505753, test_loss 0.44110239355670183\n",
            "Step 18959, train_loss 0.45294365513695817, test_loss 0.4411021384922425\n",
            "Step 18960, train_loss 0.4529431478912395, test_loss 0.44110188347944984\n",
            "Step 18961, train_loss 0.45294264068789497, test_loss 0.4411016285183146\n",
            "Step 18962, train_loss 0.4529421335269182, test_loss 0.44110137360882723\n",
            "Step 18963, train_loss 0.4529416264083028, test_loss 0.4411011187509782\n",
            "Step 18964, train_loss 0.45294111933204234, test_loss 0.44110086394475817\n",
            "Step 18965, train_loss 0.4529406122981306, test_loss 0.44110060919015776\n",
            "Step 18966, train_loss 0.45294010530656087, test_loss 0.4411003544871673\n",
            "Step 18967, train_loss 0.452939598357327, test_loss 0.44110009983577764\n",
            "Step 18968, train_loss 0.4529390914504224, test_loss 0.4410998452359792\n",
            "Step 18969, train_loss 0.4529385845858408, test_loss 0.4410995906877627\n",
            "Step 18970, train_loss 0.45293807776357575, test_loss 0.4410993361911183\n",
            "Step 18971, train_loss 0.452937570983621, test_loss 0.4410990817460371\n",
            "Step 18972, train_loss 0.4529370642459701, test_loss 0.4410988273525094\n",
            "Step 18973, train_loss 0.4529365575506164, test_loss 0.4410985730105258\n",
            "Step 18974, train_loss 0.45293605089755373, test_loss 0.44109831872007693\n",
            "Step 18975, train_loss 0.4529355442867759, test_loss 0.44109806448115346\n",
            "Step 18976, train_loss 0.4529350377182762, test_loss 0.44109781029374573\n",
            "Step 18977, train_loss 0.45293453119204835, test_loss 0.4410975561578446\n",
            "Step 18978, train_loss 0.4529340247080861, test_loss 0.44109730207344044\n",
            "Step 18979, train_loss 0.45293351826638295, test_loss 0.441097048040524\n",
            "Step 18980, train_loss 0.45293301186693247, test_loss 0.44109679405908586\n",
            "Step 18981, train_loss 0.45293250550972847, test_loss 0.44109654012911653\n",
            "Step 18982, train_loss 0.45293199919476457, test_loss 0.4410962862506067\n",
            "Step 18983, train_loss 0.45293149292203405, test_loss 0.44109603242354695\n",
            "Step 18984, train_loss 0.45293098669153087, test_loss 0.44109577864792776\n",
            "Step 18985, train_loss 0.4529304805032487, test_loss 0.44109552492374\n",
            "Step 18986, train_loss 0.45292997435718085, test_loss 0.4410952712509741\n",
            "Step 18987, train_loss 0.45292946825332137, test_loss 0.4410950176296207\n",
            "Step 18988, train_loss 0.45292896219166356, test_loss 0.4410947640596703\n",
            "Step 18989, train_loss 0.4529284561722013, test_loss 0.4410945105411138\n",
            "Step 18990, train_loss 0.452927950194928, test_loss 0.4410942570739417\n",
            "Step 18991, train_loss 0.4529274442598375, test_loss 0.44109400365814444\n",
            "Step 18992, train_loss 0.45292693836692327, test_loss 0.44109375029371295\n",
            "Step 18993, train_loss 0.4529264325161792, test_loss 0.4410934969806375\n",
            "Step 18994, train_loss 0.4529259267075987, test_loss 0.4410932437189089\n",
            "Step 18995, train_loss 0.4529254209411755, test_loss 0.44109299050851797\n",
            "Step 18996, train_loss 0.45292491521690315, test_loss 0.4410927373494552\n",
            "Step 18997, train_loss 0.4529244095347757, test_loss 0.441092484241711\n",
            "Step 18998, train_loss 0.4529239038947863, test_loss 0.44109223118527624\n",
            "Step 18999, train_loss 0.4529233982969289, test_loss 0.4410919781801417\n",
            "Step 19001, train_loss 0.4529223872275845, test_loss 0.4410914723237349\n",
            "Step 19002, train_loss 0.45292188175608483, test_loss 0.4410912194724442\n",
            "Step 19003, train_loss 0.45292137632669166, test_loss 0.4410909666724161\n",
            "Step 19004, train_loss 0.45292087093939887, test_loss 0.44109071392364124\n",
            "Step 19005, train_loss 0.4529203655941999, test_loss 0.44109046122611045\n",
            "Step 19006, train_loss 0.45291986029108844, test_loss 0.44109020857981407\n",
            "Step 19007, train_loss 0.45291935503005826, test_loss 0.441089955984743\n",
            "Step 19008, train_loss 0.45291884981110303, test_loss 0.4410897034408877\n",
            "Step 19009, train_loss 0.4529183446342163, test_loss 0.4410894509482392\n",
            "Step 19010, train_loss 0.45291783949939185, test_loss 0.4410891985067877\n",
            "Step 19011, train_loss 0.45291733440662335, test_loss 0.4410889461165242\n",
            "Step 19012, train_loss 0.4529168293559044, test_loss 0.44108869377743926\n",
            "Step 19013, train_loss 0.45291632434722884, test_loss 0.4410884414895235\n",
            "Step 19014, train_loss 0.4529158193805901, test_loss 0.44108818925276766\n",
            "Step 19015, train_loss 0.45291531445598215, test_loss 0.4410879370671623\n",
            "Step 19016, train_loss 0.45291480957339847, test_loss 0.4410876849326983\n",
            "Step 19017, train_loss 0.45291430473283284, test_loss 0.4410874328493662\n",
            "Step 19018, train_loss 0.4529137999342789, test_loss 0.4410871808171568\n",
            "Step 19019, train_loss 0.4529132951777303, test_loss 0.44108692883606054\n",
            "Step 19020, train_loss 0.4529127904631809, test_loss 0.44108667690606823\n",
            "Step 19021, train_loss 0.45291228579062426, test_loss 0.44108642502717077\n",
            "Step 19022, train_loss 0.4529117811600541, test_loss 0.44108617319935856\n",
            "Step 19023, train_loss 0.4529112765714641, test_loss 0.4410859214226225\n",
            "Step 19024, train_loss 0.45291077202484803, test_loss 0.44108566969695323\n",
            "Step 19025, train_loss 0.45291026752019947, test_loss 0.4410854180223413\n",
            "Step 19026, train_loss 0.45290976305751224, test_loss 0.44108516639877743\n",
            "Step 19027, train_loss 0.4529092586367799, test_loss 0.4410849148262526\n",
            "Step 19028, train_loss 0.4529087542579962, test_loss 0.4410846633047572\n",
            "Step 19029, train_loss 0.452908249921155, test_loss 0.4410844118342822\n",
            "Step 19030, train_loss 0.4529077456262498, test_loss 0.44108416041481796\n",
            "Step 19031, train_loss 0.4529072413732745, test_loss 0.44108390904635547\n",
            "Step 19032, train_loss 0.45290673716222263, test_loss 0.4410836577288854\n",
            "Step 19033, train_loss 0.4529062329930881, test_loss 0.44108340646239846\n",
            "Step 19034, train_loss 0.4529057288658645, test_loss 0.4410831552468853\n",
            "Step 19035, train_loss 0.4529052247805455, test_loss 0.44108290408233675\n",
            "Step 19036, train_loss 0.45290472073712496, test_loss 0.4410826529687435\n",
            "Step 19037, train_loss 0.45290421673559644, test_loss 0.4410824019060961\n",
            "Step 19038, train_loss 0.45290371277595376, test_loss 0.44108215089438546\n",
            "Step 19039, train_loss 0.45290320885819063, test_loss 0.44108189993360225\n",
            "Step 19040, train_loss 0.45290270498230073, test_loss 0.4410816490237372\n",
            "Step 19041, train_loss 0.4529022011482779, test_loss 0.44108139816478115\n",
            "Step 19042, train_loss 0.4529016973561157, test_loss 0.4410811473567247\n",
            "Step 19043, train_loss 0.45290119360580794, test_loss 0.4410808965995585\n",
            "Step 19044, train_loss 0.4529006898973485, test_loss 0.4410806458932736\n",
            "Step 19045, train_loss 0.45290018623073086, test_loss 0.4410803952378605\n",
            "Step 19046, train_loss 0.45289968260594893, test_loss 0.44108014463330997\n",
            "Step 19047, train_loss 0.45289917902299637, test_loss 0.4410798940796128\n",
            "Step 19048, train_loss 0.4528986754818669, test_loss 0.4410796435767597\n",
            "Step 19049, train_loss 0.45289817198255433, test_loss 0.4410793931247415\n",
            "Step 19050, train_loss 0.4528976685250523, test_loss 0.4410791427235488\n",
            "Step 19051, train_loss 0.45289716510935474, test_loss 0.44107889237317255\n",
            "Step 19052, train_loss 0.4528966617354551, test_loss 0.4410786420736035\n",
            "Step 19053, train_loss 0.4528961584033475, test_loss 0.44107839182483205\n",
            "Step 19054, train_loss 0.4528956551130254, test_loss 0.44107814162684944\n",
            "Step 19055, train_loss 0.4528951518644826, test_loss 0.44107789147964616\n",
            "Step 19056, train_loss 0.4528946486577129, test_loss 0.441077641383213\n",
            "Step 19057, train_loss 0.45289414549270995, test_loss 0.4410773913375408\n",
            "Step 19058, train_loss 0.4528936423694678, test_loss 0.44107714134262016\n",
            "Step 19059, train_loss 0.4528931392879798, test_loss 0.44107689139844214\n",
            "Step 19060, train_loss 0.45289263624823994, test_loss 0.44107664150499737\n",
            "Step 19061, train_loss 0.45289213325024213, test_loss 0.4410763916622766\n",
            "Step 19062, train_loss 0.4528916302939798, test_loss 0.44107614187027055\n",
            "Step 19063, train_loss 0.45289112737944687, test_loss 0.4410758921289702\n",
            "Step 19064, train_loss 0.45289062450663714, test_loss 0.44107564243836606\n",
            "Step 19065, train_loss 0.45289012167554427, test_loss 0.44107539279844915\n",
            "Step 19066, train_loss 0.45288961888616214, test_loss 0.4410751432092102\n",
            "Step 19067, train_loss 0.4528891161384844, test_loss 0.44107489367064007\n",
            "Step 19068, train_loss 0.45288861343250486, test_loss 0.44107464418272924\n",
            "Step 19069, train_loss 0.4528881107682174, test_loss 0.4410743947454688\n",
            "Step 19070, train_loss 0.4528876081456158, test_loss 0.4410741453588494\n",
            "Step 19071, train_loss 0.4528871055646935, test_loss 0.44107389602286207\n",
            "Step 19072, train_loss 0.45288660302544476, test_loss 0.44107364673749727\n",
            "Step 19073, train_loss 0.452886100527863, test_loss 0.4410733975027461\n",
            "Step 19074, train_loss 0.4528855980719422, test_loss 0.44107314831859923\n",
            "Step 19075, train_loss 0.45288509565767604, test_loss 0.44107289918504733\n",
            "Step 19076, train_loss 0.4528845932850583, test_loss 0.44107265010208146\n",
            "Step 19077, train_loss 0.4528840909540829, test_loss 0.44107240106969237\n",
            "Step 19078, train_loss 0.45288358866474343, test_loss 0.4410721520878708\n",
            "Step 19079, train_loss 0.45288308641703373, test_loss 0.4410719031566076\n",
            "Step 19080, train_loss 0.4528825842109477, test_loss 0.44107165427589345\n",
            "Step 19081, train_loss 0.4528820820464791, test_loss 0.4410714054457196\n",
            "Step 19082, train_loss 0.45288157992362166, test_loss 0.4410711566660763\n",
            "Step 19083, train_loss 0.4528810778423692, test_loss 0.4410709079369548\n",
            "Step 19084, train_loss 0.45288057580271557, test_loss 0.44107065925834565\n",
            "Step 19085, train_loss 0.45288007380465445, test_loss 0.44107041063024\n",
            "Step 19086, train_loss 0.45287957184817973, test_loss 0.44107016205262833\n",
            "Step 19087, train_loss 0.45287906993328514, test_loss 0.4410699135255016\n",
            "Step 19088, train_loss 0.4528785680599647, test_loss 0.44106966504885087\n",
            "Step 19089, train_loss 0.4528780662282119, test_loss 0.4410694166226667\n",
            "Step 19090, train_loss 0.4528775644380208, test_loss 0.4410691682469399\n",
            "Step 19091, train_loss 0.452877062689385, test_loss 0.4410689199216615\n",
            "Step 19092, train_loss 0.45287656098229856, test_loss 0.4410686716468223\n",
            "Step 19093, train_loss 0.45287605931675506, test_loss 0.44106842342241304\n",
            "Step 19094, train_loss 0.4528755576927484, test_loss 0.4410681752484247\n",
            "Step 19095, train_loss 0.4528750561102724, test_loss 0.4410679271248481\n",
            "Step 19096, train_loss 0.4528745545693209, test_loss 0.441067679051674\n",
            "Step 19097, train_loss 0.4528740530698877, test_loss 0.4410674310288933\n",
            "Step 19098, train_loss 0.4528735516119666, test_loss 0.44106718305649695\n",
            "Step 19099, train_loss 0.4528730501955514, test_loss 0.44106693513447576\n",
            "Step 19101, train_loss 0.4528720474872142, test_loss 0.4410664394415221\n",
            "Step 19102, train_loss 0.45287154619527975, test_loss 0.44106619167057143\n",
            "Step 19103, train_loss 0.45287104494482655, test_loss 0.44106594394995924\n",
            "Step 19104, train_loss 0.45287054373584845, test_loss 0.44106569627967673\n",
            "Step 19105, train_loss 0.45287004256833924, test_loss 0.4410654486597145\n",
            "Step 19106, train_loss 0.4528695414422928, test_loss 0.4410652010900634\n",
            "Step 19107, train_loss 0.4528690403577028, test_loss 0.44106495357071446\n",
            "Step 19108, train_loss 0.4528685393145633, test_loss 0.4410647061016584\n",
            "Step 19109, train_loss 0.4528680383128681, test_loss 0.4410644586828863\n",
            "Step 19110, train_loss 0.4528675373526108, test_loss 0.44106421131438883\n",
            "Step 19111, train_loss 0.45286703643378556, test_loss 0.441063963996157\n",
            "Step 19112, train_loss 0.452866535556386, test_loss 0.44106371672818173\n",
            "Step 19113, train_loss 0.4528660347204061, test_loss 0.4410634695104539\n",
            "Step 19114, train_loss 0.45286553392583967, test_loss 0.44106322234296413\n",
            "Step 19115, train_loss 0.4528650331726805, test_loss 0.4410629752257037\n",
            "Step 19116, train_loss 0.45286453246092245, test_loss 0.4410627281586634\n",
            "Step 19117, train_loss 0.4528640317905594, test_loss 0.44106248114183394\n",
            "Step 19118, train_loss 0.4528635311615852, test_loss 0.4410622341752063\n",
            "Step 19119, train_loss 0.4528630305739938, test_loss 0.44106198725877155\n",
            "Step 19120, train_loss 0.4528625300277789, test_loss 0.44106174039252033\n",
            "Step 19121, train_loss 0.45286202952293436, test_loss 0.44106149357644386\n",
            "Step 19122, train_loss 0.4528615290594541, test_loss 0.4410612468105327\n",
            "Step 19123, train_loss 0.4528610286373321, test_loss 0.4410610000947781\n",
            "Step 19124, train_loss 0.4528605282565619, test_loss 0.44106075342917084\n",
            "Step 19125, train_loss 0.45286002791713775, test_loss 0.4410605068137016\n",
            "Step 19126, train_loss 0.4528595276190533, test_loss 0.4410602602483617\n",
            "Step 19127, train_loss 0.4528590273623023, test_loss 0.4410600137331418\n",
            "Step 19128, train_loss 0.452858527146879, test_loss 0.4410597672680329\n",
            "Step 19129, train_loss 0.4528580269727769, test_loss 0.4410595208530259\n",
            "Step 19130, train_loss 0.45285752683999, test_loss 0.4410592744881116\n",
            "Step 19131, train_loss 0.45285702674851214, test_loss 0.44105902817328124\n",
            "Step 19132, train_loss 0.4528565266983373, test_loss 0.4410587819085255\n",
            "Step 19133, train_loss 0.45285602668945923, test_loss 0.44105853569383546\n",
            "Step 19134, train_loss 0.45285552672187185, test_loss 0.4410582895292019\n",
            "Step 19135, train_loss 0.4528550267955692, test_loss 0.4410580434146159\n",
            "Step 19136, train_loss 0.4528545269105449, test_loss 0.4410577973500683\n",
            "Step 19137, train_loss 0.4528540270667929, test_loss 0.44105755133555014\n",
            "Step 19138, train_loss 0.45285352726430717, test_loss 0.44105730537105226\n",
            "Step 19139, train_loss 0.4528530275030816, test_loss 0.44105705945656554\n",
            "Step 19140, train_loss 0.4528525277831101, test_loss 0.4410568135920811\n",
            "Step 19141, train_loss 0.45285202810438635, test_loss 0.44105656777758984\n",
            "Step 19142, train_loss 0.4528515284669045, test_loss 0.4410563220130827\n",
            "Step 19143, train_loss 0.45285102887065837, test_loss 0.44105607629855065\n",
            "Step 19144, train_loss 0.45285052931564174, test_loss 0.4410558306339845\n",
            "Step 19145, train_loss 0.4528500298018485, test_loss 0.4410555850193754\n",
            "Step 19146, train_loss 0.4528495303292727, test_loss 0.44105533945471426\n",
            "Step 19147, train_loss 0.45284903089790823, test_loss 0.4410550939399919\n",
            "Step 19148, train_loss 0.45284853150774884, test_loss 0.4410548484751996\n",
            "Step 19149, train_loss 0.45284803215878866, test_loss 0.441054603060328\n",
            "Step 19150, train_loss 0.45284753285102125, test_loss 0.4410543576953683\n",
            "Step 19151, train_loss 0.4528470335844409, test_loss 0.4410541123803113\n",
            "Step 19152, train_loss 0.4528465343590412, test_loss 0.441053867115148\n",
            "Step 19153, train_loss 0.45284603517481625, test_loss 0.4410536218998695\n",
            "Step 19154, train_loss 0.45284553603175987, test_loss 0.44105337673446665\n",
            "Step 19155, train_loss 0.45284503692986605, test_loss 0.4410531316189304\n",
            "Step 19156, train_loss 0.45284453786912865, test_loss 0.44105288655325187\n",
            "Step 19157, train_loss 0.45284403884954155, test_loss 0.441052641537422\n",
            "Step 19158, train_loss 0.45284353987109877, test_loss 0.44105239657143186\n",
            "Step 19159, train_loss 0.4528430409337941, test_loss 0.4410521516552722\n",
            "Step 19160, train_loss 0.4528425420376216, test_loss 0.4410519067889342\n",
            "Step 19161, train_loss 0.45284204318257515, test_loss 0.4410516619724087\n",
            "Step 19162, train_loss 0.4528415443686486, test_loss 0.4410514172056868\n",
            "Step 19163, train_loss 0.45284104559583593, test_loss 0.4410511724887596\n",
            "Step 19164, train_loss 0.45284054686413094, test_loss 0.44105092782161776\n",
            "Step 19165, train_loss 0.45284004817352774, test_loss 0.4410506832042527\n",
            "Step 19166, train_loss 0.4528395495240202, test_loss 0.4410504386366552\n",
            "Step 19167, train_loss 0.4528390509156022, test_loss 0.4410501941188162\n",
            "Step 19168, train_loss 0.4528385523482678, test_loss 0.44104994965072686\n",
            "Step 19169, train_loss 0.45283805382201076, test_loss 0.4410497052323781\n",
            "Step 19170, train_loss 0.4528375553368252, test_loss 0.44104946086376096\n",
            "Step 19171, train_loss 0.45283705689270487, test_loss 0.4410492165448665\n",
            "Step 19172, train_loss 0.4528365584896438, test_loss 0.44104897227568557\n",
            "Step 19173, train_loss 0.45283606012763605, test_loss 0.44104872805620937\n",
            "Step 19174, train_loss 0.45283556180667534, test_loss 0.4410484838864289\n",
            "Step 19175, train_loss 0.4528350635267557, test_loss 0.44104823976633506\n",
            "Step 19176, train_loss 0.4528345652878711, test_loss 0.44104799569591896\n",
            "Step 19177, train_loss 0.4528340670900155, test_loss 0.4410477516751716\n",
            "Step 19178, train_loss 0.45283356893318283, test_loss 0.44104750770408396\n",
            "Step 19179, train_loss 0.452833070817367, test_loss 0.4410472637826473\n",
            "Step 19180, train_loss 0.452832572742562, test_loss 0.44104701991085227\n",
            "Step 19181, train_loss 0.4528320747087618, test_loss 0.4410467760886903\n",
            "Step 19182, train_loss 0.4528315767159603, test_loss 0.44104653231615215\n",
            "Step 19183, train_loss 0.45283107876415146, test_loss 0.4410462885932289\n",
            "Step 19184, train_loss 0.4528305808533293, test_loss 0.4410460449199118\n",
            "Step 19185, train_loss 0.45283008298348776, test_loss 0.4410458012961916\n",
            "Step 19186, train_loss 0.4528295851546208, test_loss 0.44104555772205956\n",
            "Step 19187, train_loss 0.45282908736672234, test_loss 0.44104531419750653\n",
            "Step 19188, train_loss 0.4528285896197864, test_loss 0.4410450707225237\n",
            "Step 19189, train_loss 0.4528280919138069, test_loss 0.44104482729710215\n",
            "Step 19190, train_loss 0.45282759424877783, test_loss 0.44104458392123286\n",
            "Step 19191, train_loss 0.45282709662469317, test_loss 0.44104434059490694\n",
            "Step 19192, train_loss 0.45282659904154676, test_loss 0.4410440973181152\n",
            "Step 19193, train_loss 0.4528261014993328, test_loss 0.4410438540908492\n",
            "Step 19194, train_loss 0.4528256039980452, test_loss 0.4410436109130995\n",
            "Step 19195, train_loss 0.45282510653767777, test_loss 0.44104336778485753\n",
            "Step 19196, train_loss 0.45282460911822464, test_loss 0.44104312470611406\n",
            "Step 19197, train_loss 0.45282411173967974, test_loss 0.44104288167686045\n",
            "Step 19198, train_loss 0.45282361440203706, test_loss 0.44104263869708743\n",
            "Step 19199, train_loss 0.45282311710529066, test_loss 0.44104239576678633\n",
            "Step 19201, train_loss 0.45282212263446225, test_loss 0.441041910054564\n",
            "Step 19202, train_loss 0.45282162546036825, test_loss 0.441041667272625\n",
            "Step 19203, train_loss 0.4528211283271464, test_loss 0.4410414245401221\n",
            "Step 19204, train_loss 0.4528206312347907, test_loss 0.4410411818570464\n",
            "Step 19205, train_loss 0.4528201341832952, test_loss 0.44104093922338905\n",
            "Step 19206, train_loss 0.4528196371726537, test_loss 0.44104069663914114\n",
            "Step 19207, train_loss 0.45281914020286035, test_loss 0.44104045410429366\n",
            "Step 19208, train_loss 0.4528186432739091, test_loss 0.4410402116188378\n",
            "Step 19209, train_loss 0.452818146385794, test_loss 0.44103996918276467\n",
            "Step 19210, train_loss 0.45281764953850895, test_loss 0.4410397267960653\n",
            "Step 19211, train_loss 0.45281715273204803, test_loss 0.4410394844587307\n",
            "Step 19212, train_loss 0.4528166559664052, test_loss 0.4410392421707523\n",
            "Step 19213, train_loss 0.4528161592415743, test_loss 0.4410389999321208\n",
            "Step 19214, train_loss 0.45281566255754974, test_loss 0.44103875774282747\n",
            "Step 19215, train_loss 0.4528151659143253, test_loss 0.44103851560286345\n",
            "Step 19216, train_loss 0.4528146693118948, test_loss 0.44103827351221986\n",
            "Step 19217, train_loss 0.4528141727502526, test_loss 0.4410380314708878\n",
            "Step 19218, train_loss 0.4528136762293925, test_loss 0.4410377894788584\n",
            "Step 19219, train_loss 0.45281317974930857, test_loss 0.4410375475361226\n",
            "Step 19220, train_loss 0.45281268330999475, test_loss 0.44103730564267174\n",
            "Step 19221, train_loss 0.4528121869114451, test_loss 0.4410370637984967\n",
            "Step 19222, train_loss 0.4528116905536538, test_loss 0.4410368220035889\n",
            "Step 19223, train_loss 0.4528111942366146, test_loss 0.44103658025793924\n",
            "Step 19224, train_loss 0.4528106979603217, test_loss 0.44103633856153895\n",
            "Step 19225, train_loss 0.45281020172476905, test_loss 0.44103609691437906\n",
            "Step 19226, train_loss 0.45280970552995065, test_loss 0.4410358553164508\n",
            "Step 19227, train_loss 0.45280920937586056, test_loss 0.44103561376774514\n",
            "Step 19228, train_loss 0.4528087132624929, test_loss 0.44103537226825357\n",
            "Step 19229, train_loss 0.45280821718984154, test_loss 0.44103513081796675\n",
            "Step 19230, train_loss 0.45280772115790063, test_loss 0.4410348894168762\n",
            "Step 19231, train_loss 0.4528072251666641, test_loss 0.4410346480649728\n",
            "Step 19232, train_loss 0.452806729216126, test_loss 0.44103440676224787\n",
            "Step 19233, train_loss 0.4528062333062805, test_loss 0.44103416550869246\n",
            "Step 19234, train_loss 0.45280573743712144, test_loss 0.4410339243042978\n",
            "Step 19235, train_loss 0.4528052416086431, test_loss 0.44103368314905483\n",
            "Step 19236, train_loss 0.4528047458208392, test_loss 0.4410334420429549\n",
            "Step 19237, train_loss 0.452804250073704, test_loss 0.44103320098598897\n",
            "Step 19238, train_loss 0.4528037543672315, test_loss 0.4410329599781487\n",
            "Step 19239, train_loss 0.4528032587014158, test_loss 0.4410327190194246\n",
            "Step 19240, train_loss 0.4528027630762508, test_loss 0.44103247810980817\n",
            "Step 19241, train_loss 0.4528022674917307, test_loss 0.4410322372492904\n",
            "Step 19242, train_loss 0.4528017719478494, test_loss 0.4410319964378626\n",
            "Step 19243, train_loss 0.45280127644460116, test_loss 0.4410317556755159\n",
            "Step 19244, train_loss 0.45280078098197973, test_loss 0.4410315149622416\n",
            "Step 19245, train_loss 0.4528002855599794, test_loss 0.4410312742980306\n",
            "Step 19246, train_loss 0.45279979017859423, test_loss 0.44103103368287416\n",
            "Step 19247, train_loss 0.4527992948378181, test_loss 0.4410307931167635\n",
            "Step 19248, train_loss 0.45279879953764524, test_loss 0.4410305525996898\n",
            "Step 19249, train_loss 0.45279830427806966, test_loss 0.44103031213164423\n",
            "Step 19250, train_loss 0.45279780905908534, test_loss 0.4410300717126179\n",
            "Step 19251, train_loss 0.4527973138806864, test_loss 0.4410298313426021\n",
            "Step 19252, train_loss 0.4527968187428669, test_loss 0.4410295910215879\n",
            "Step 19253, train_loss 0.45279632364562106, test_loss 0.44102935074956656\n",
            "Step 19254, train_loss 0.4527958285889427, test_loss 0.44102911052652927\n",
            "Step 19255, train_loss 0.452795333572826, test_loss 0.4410288703524672\n",
            "Step 19256, train_loss 0.452794838597265, test_loss 0.4410286302273715\n",
            "Step 19257, train_loss 0.4527943436622538, test_loss 0.44102839015123335\n",
            "Step 19258, train_loss 0.45279384876778644, test_loss 0.441028150124044\n",
            "Step 19259, train_loss 0.452793353913857, test_loss 0.4410279101457946\n",
            "Step 19260, train_loss 0.45279285910045963, test_loss 0.44102767021647654\n",
            "Step 19261, train_loss 0.4527923643275883, test_loss 0.4410274303360808\n",
            "Step 19262, train_loss 0.4527918695952372, test_loss 0.4410271905045986\n",
            "Step 19263, train_loss 0.45279137490340043, test_loss 0.44102695072202125\n",
            "Step 19264, train_loss 0.4527908802520718, test_loss 0.4410267109883398\n",
            "Step 19265, train_loss 0.4527903856412457, test_loss 0.4410264713035457\n",
            "Step 19266, train_loss 0.45278989107091605, test_loss 0.4410262316676299\n",
            "Step 19267, train_loss 0.4527893965410771, test_loss 0.44102599208058385\n",
            "Step 19268, train_loss 0.45278890205172284, test_loss 0.4410257525423987\n",
            "Step 19269, train_loss 0.45278840760284716, test_loss 0.4410255130530655\n",
            "Step 19270, train_loss 0.45278791319444456, test_loss 0.44102527361257554\n",
            "Step 19271, train_loss 0.45278741882650886, test_loss 0.44102503422092015\n",
            "Step 19272, train_loss 0.4527869244990341, test_loss 0.44102479487809054\n",
            "Step 19273, train_loss 0.45278643021201453, test_loss 0.4410245555840779\n",
            "Step 19274, train_loss 0.4527859359654443, test_loss 0.44102431633887346\n",
            "Step 19275, train_loss 0.45278544175931734, test_loss 0.4410240771424683\n",
            "Step 19276, train_loss 0.45278494759362786, test_loss 0.4410238379948539\n",
            "Step 19277, train_loss 0.4527844534683699, test_loss 0.4410235988960214\n",
            "Step 19278, train_loss 0.45278395938353755, test_loss 0.4410233598459619\n",
            "Step 19279, train_loss 0.45278346533912495, test_loss 0.4410231208446669\n",
            "Step 19280, train_loss 0.45278297133512635, test_loss 0.4410228818921274\n",
            "Step 19281, train_loss 0.4527824773715356, test_loss 0.4410226429883347\n",
            "Step 19282, train_loss 0.45278198344834697, test_loss 0.4410224041332801\n",
            "Step 19283, train_loss 0.4527814895655545, test_loss 0.4410221653269548\n",
            "Step 19284, train_loss 0.4527809957231525, test_loss 0.4410219265693503\n",
            "Step 19285, train_loss 0.45278050192113467, test_loss 0.4410216878604575\n",
            "Step 19286, train_loss 0.4527800081594956, test_loss 0.44102144920026776\n",
            "Step 19287, train_loss 0.45277951443822906, test_loss 0.4410212105887723\n",
            "Step 19288, train_loss 0.45277902075732923, test_loss 0.44102097202596247\n",
            "Step 19289, train_loss 0.4527785271167905, test_loss 0.4410207335118295\n",
            "Step 19290, train_loss 0.45277803351660656, test_loss 0.44102049504636465\n",
            "Step 19291, train_loss 0.45277753995677195, test_loss 0.4410202566295591\n",
            "Step 19292, train_loss 0.4527770464372805, test_loss 0.4410200182614043\n",
            "Step 19293, train_loss 0.4527765529581266, test_loss 0.44101977994189145\n",
            "Step 19294, train_loss 0.45277605951930405, test_loss 0.4410195416710116\n",
            "Step 19295, train_loss 0.4527755661208071, test_loss 0.4410193034487562\n",
            "Step 19296, train_loss 0.45277507276263007, test_loss 0.44101906527511664\n",
            "Step 19297, train_loss 0.452774579444767, test_loss 0.44101882715008395\n",
            "Step 19298, train_loss 0.4527740861672118, test_loss 0.44101858907364966\n",
            "Step 19299, train_loss 0.4527735929299589, test_loss 0.4410183510458048\n",
            "Step 19301, train_loss 0.45277260657633617, test_loss 0.4410178751358488\n",
            "Step 19302, train_loss 0.4527721134599548, test_loss 0.4410176372537204\n",
            "Step 19303, train_loss 0.45277162038385194, test_loss 0.44101739942014645\n",
            "Step 19304, train_loss 0.4527711273480221, test_loss 0.44101716163511867\n",
            "Step 19305, train_loss 0.4527706343524592, test_loss 0.44101692389862807\n",
            "Step 19306, train_loss 0.4527701413971575, test_loss 0.44101668621066603\n",
            "Step 19307, train_loss 0.4527696484821112, test_loss 0.44101644857122385\n",
            "Step 19308, train_loss 0.45276915560731434, test_loss 0.4410162109802928\n",
            "Step 19309, train_loss 0.45276866277276107, test_loss 0.44101597343786414\n",
            "Step 19310, train_loss 0.45276816997844566, test_loss 0.44101573594392934\n",
            "Step 19311, train_loss 0.45276767722436206, test_loss 0.44101549849847954\n",
            "Step 19312, train_loss 0.4527671845105047, test_loss 0.4410152611015061\n",
            "Step 19313, train_loss 0.4527666918368674, test_loss 0.4410150237530003\n",
            "Step 19314, train_loss 0.45276619920344474, test_loss 0.4410147864529535\n",
            "Step 19315, train_loss 0.4527657066102305, test_loss 0.44101454920135696\n",
            "Step 19316, train_loss 0.4527652140572191, test_loss 0.441014311998202\n",
            "Step 19317, train_loss 0.45276472154440445, test_loss 0.44101407484347993\n",
            "Step 19318, train_loss 0.45276422907178093, test_loss 0.44101383773718206\n",
            "Step 19319, train_loss 0.4527637366393426, test_loss 0.4410136006792999\n",
            "Step 19320, train_loss 0.4527632442470837, test_loss 0.44101336366982447\n",
            "Step 19321, train_loss 0.45276275189499837, test_loss 0.44101312670874726\n",
            "Step 19322, train_loss 0.45276225958308075, test_loss 0.4410128897960596\n",
            "Step 19323, train_loss 0.45276176731132517, test_loss 0.44101265293175274\n",
            "Step 19324, train_loss 0.4527612750797255, test_loss 0.4410124161158181\n",
            "Step 19325, train_loss 0.45276078288827615, test_loss 0.441012179348247\n",
            "Step 19326, train_loss 0.45276029073697127, test_loss 0.4410119426290307\n",
            "Step 19327, train_loss 0.4527597986258049, test_loss 0.44101170595816047\n",
            "Step 19328, train_loss 0.45275930655477137, test_loss 0.44101146933562796\n",
            "Step 19329, train_loss 0.4527588145238647, test_loss 0.4410112327614241\n",
            "Step 19330, train_loss 0.4527583225330794, test_loss 0.4410109962355406\n",
            "Step 19331, train_loss 0.45275783058240926, test_loss 0.44101075975796855\n",
            "Step 19332, train_loss 0.45275733867184875, test_loss 0.44101052332869944\n",
            "Step 19333, train_loss 0.45275684680139183, test_loss 0.44101028694772443\n",
            "Step 19334, train_loss 0.4527563549710329, test_loss 0.44101005061503507\n",
            "Step 19335, train_loss 0.45275586318076605, test_loss 0.44100981433062275\n",
            "Step 19336, train_loss 0.4527553714305855, test_loss 0.4410095780944786\n",
            "Step 19337, train_loss 0.4527548797204854, test_loss 0.4410093419065941\n",
            "Step 19338, train_loss 0.4527543880504599, test_loss 0.4410091057669607\n",
            "Step 19339, train_loss 0.45275389642050334, test_loss 0.44100886967556957\n",
            "Step 19340, train_loss 0.45275340483060983, test_loss 0.44100863363241216\n",
            "Step 19341, train_loss 0.45275291328077366, test_loss 0.44100839763747984\n",
            "Step 19342, train_loss 0.45275242177098884, test_loss 0.44100816169076396\n",
            "Step 19343, train_loss 0.4527519303012497, test_loss 0.4410079257922559\n",
            "Step 19344, train_loss 0.45275143887155045, test_loss 0.4410076899419471\n",
            "Step 19345, train_loss 0.45275094748188527, test_loss 0.4410074541398287\n",
            "Step 19346, train_loss 0.4527504561322484, test_loss 0.44100721838589246\n",
            "Step 19347, train_loss 0.452749964822634, test_loss 0.4410069826801295\n",
            "Step 19348, train_loss 0.4527494735530363, test_loss 0.44100674702253106\n",
            "Step 19349, train_loss 0.4527489823234495, test_loss 0.4410065114130888\n",
            "Step 19350, train_loss 0.4527484911338679, test_loss 0.44100627585179397\n",
            "Step 19351, train_loss 0.4527479999842857, test_loss 0.44100604033863805\n",
            "Step 19352, train_loss 0.45274750887469684, test_loss 0.4410058048736122\n",
            "Step 19353, train_loss 0.45274701780509585, test_loss 0.44100556945670805\n",
            "Step 19354, train_loss 0.4527465267754769, test_loss 0.44100533408791703\n",
            "Step 19355, train_loss 0.4527460357858341, test_loss 0.44100509876723026\n",
            "Step 19356, train_loss 0.45274554483616175, test_loss 0.4410048634946392\n",
            "Step 19357, train_loss 0.45274505392645403, test_loss 0.44100462827013537\n",
            "Step 19358, train_loss 0.4527445630567052, test_loss 0.44100439309371015\n",
            "Step 19359, train_loss 0.4527440722269095, test_loss 0.441004157965355\n",
            "Step 19360, train_loss 0.45274358143706117, test_loss 0.44100392288506113\n",
            "Step 19361, train_loss 0.45274309068715435, test_loss 0.44100368785282\n",
            "Step 19362, train_loss 0.45274259997718325, test_loss 0.441003452868623\n",
            "Step 19363, train_loss 0.45274210930714237, test_loss 0.44100321793246183\n",
            "Step 19364, train_loss 0.45274161867702567, test_loss 0.4410029830443276\n",
            "Step 19365, train_loss 0.45274112808682737, test_loss 0.4410027482042117\n",
            "Step 19366, train_loss 0.4527406375365418, test_loss 0.44100251341210567\n",
            "Step 19367, train_loss 0.4527401470261634, test_loss 0.44100227866800085\n",
            "Step 19368, train_loss 0.4527396565556861, test_loss 0.44100204397188875\n",
            "Step 19369, train_loss 0.45273916612510423, test_loss 0.44100180932376076\n",
            "Step 19370, train_loss 0.45273867573441207, test_loss 0.44100157472360835\n",
            "Step 19371, train_loss 0.4527381853836039, test_loss 0.4410013401714227\n",
            "Step 19372, train_loss 0.4527376950726738, test_loss 0.44100110566719547\n",
            "Step 19373, train_loss 0.4527372048016162, test_loss 0.441000871210918\n",
            "Step 19374, train_loss 0.4527367145704252, test_loss 0.4410006368025819\n",
            "Step 19375, train_loss 0.45273622437909533, test_loss 0.4410004024421783\n",
            "Step 19376, train_loss 0.4527357342276205, test_loss 0.4410001681296988\n",
            "Step 19377, train_loss 0.4527352441159951, test_loss 0.4409999338651348\n",
            "Step 19378, train_loss 0.45273475404421326, test_loss 0.4409996996484779\n",
            "Step 19379, train_loss 0.4527342640122696, test_loss 0.4409994654797193\n",
            "Step 19380, train_loss 0.4527337740201581, test_loss 0.4409992313588506\n",
            "Step 19381, train_loss 0.45273328406787305, test_loss 0.44099899728586306\n",
            "Step 19382, train_loss 0.4527327941554086, test_loss 0.4409987632607483\n",
            "Step 19383, train_loss 0.45273230428275923, test_loss 0.4409985292834978\n",
            "Step 19384, train_loss 0.45273181444991906, test_loss 0.4409982953541028\n",
            "Step 19385, train_loss 0.45273132465688254, test_loss 0.440998061472555\n",
            "Step 19386, train_loss 0.45273083490364374, test_loss 0.4409978276388457\n",
            "Step 19387, train_loss 0.45273034519019695, test_loss 0.4409975938529664\n",
            "Step 19388, train_loss 0.4527298555165364, test_loss 0.4409973601149085\n",
            "Step 19389, train_loss 0.4527293658826566, test_loss 0.44099712642466343\n",
            "Step 19390, train_loss 0.4527288762885515, test_loss 0.44099689278222287\n",
            "Step 19391, train_loss 0.4527283867342156, test_loss 0.4409966591875781\n",
            "Step 19392, train_loss 0.4527278972196432, test_loss 0.4409964256407206\n",
            "Step 19393, train_loss 0.4527274077448282, test_loss 0.44099619214164193\n",
            "Step 19394, train_loss 0.45272691830976547, test_loss 0.4409959586903334\n",
            "Step 19395, train_loss 0.4527264289144488, test_loss 0.44099572528678666\n",
            "Step 19396, train_loss 0.4527259395588727, test_loss 0.4409954919309931\n",
            "Step 19397, train_loss 0.45272545024303157, test_loss 0.4409952586229441\n",
            "Step 19398, train_loss 0.4527249609669193, test_loss 0.44099502536263135\n",
            "Step 19399, train_loss 0.45272447173053043, test_loss 0.44099479215004606\n",
            "Step 19401, train_loss 0.45272349337690015, test_loss 0.44099432586802445\n",
            "Step 19402, train_loss 0.45272300425964723, test_loss 0.44099409279857105\n",
            "Step 19403, train_loss 0.45272251518209483, test_loss 0.4409938597768111\n",
            "Step 19404, train_loss 0.4527220261442372, test_loss 0.44099362680273624\n",
            "Step 19405, train_loss 0.45272153714606866, test_loss 0.44099339387633796\n",
            "Step 19406, train_loss 0.4527210481875836, test_loss 0.4409931609976076\n",
            "Step 19407, train_loss 0.4527205592687762, test_loss 0.4409929281665368\n",
            "Step 19408, train_loss 0.4527200703896409, test_loss 0.440992695383117\n",
            "Step 19409, train_loss 0.4527195815501719, test_loss 0.44099246264733977\n",
            "Step 19410, train_loss 0.4527190927503634, test_loss 0.44099222995919646\n",
            "Step 19411, train_loss 0.4527186039902099, test_loss 0.4409919973186788\n",
            "Step 19412, train_loss 0.4527181152697056, test_loss 0.4409917647257781\n",
            "Step 19413, train_loss 0.45271762658884485, test_loss 0.440991532180486\n",
            "Step 19414, train_loss 0.45271713794762203, test_loss 0.4409912996827938\n",
            "Step 19415, train_loss 0.4527166493460312, test_loss 0.4409910672326934\n",
            "Step 19416, train_loss 0.45271616078406685, test_loss 0.44099083483017576\n",
            "Step 19417, train_loss 0.45271567226172327, test_loss 0.4409906024752329\n",
            "Step 19418, train_loss 0.4527151837789948, test_loss 0.44099037016785614\n",
            "Step 19419, train_loss 0.45271469533587566, test_loss 0.4409901379080369\n",
            "Step 19420, train_loss 0.4527142069323603, test_loss 0.4409899056957669\n",
            "Step 19421, train_loss 0.4527137185684429, test_loss 0.44098967353103746\n",
            "Step 19422, train_loss 0.4527132302441179, test_loss 0.44098944141384033\n",
            "Step 19423, train_loss 0.45271274195937955, test_loss 0.44098920934416697\n",
            "Step 19424, train_loss 0.45271225371422213, test_loss 0.44098897732200854\n",
            "Step 19425, train_loss 0.45271176550864006, test_loss 0.44098874534735716\n",
            "Step 19426, train_loss 0.45271127734262767, test_loss 0.44098851342020395\n",
            "Step 19427, train_loss 0.4527107892161792, test_loss 0.4409882815405407\n",
            "Step 19428, train_loss 0.4527103011292891, test_loss 0.4409880497083588\n",
            "Step 19429, train_loss 0.4527098130819516, test_loss 0.44098781792364966\n",
            "Step 19430, train_loss 0.45270932507416095, test_loss 0.4409875861864052\n",
            "Step 19431, train_loss 0.4527088371059117, test_loss 0.4409873544966166\n",
            "Step 19432, train_loss 0.452708349177198, test_loss 0.44098712285427555\n",
            "Step 19433, train_loss 0.45270786128801427, test_loss 0.4409868912593737\n",
            "Step 19434, train_loss 0.4527073734383547, test_loss 0.44098665971190243\n",
            "Step 19435, train_loss 0.45270688562821393, test_loss 0.44098642821185335\n",
            "Step 19436, train_loss 0.4527063978575861, test_loss 0.440986196759218\n",
            "Step 19437, train_loss 0.45270591012646555, test_loss 0.44098596535398793\n",
            "Step 19438, train_loss 0.4527054224348466, test_loss 0.4409857339961548\n",
            "Step 19439, train_loss 0.4527049347827237, test_loss 0.44098550268570996\n",
            "Step 19440, train_loss 0.4527044471700912, test_loss 0.44098527142264515\n",
            "Step 19441, train_loss 0.4527039595969433, test_loss 0.4409850402069519\n",
            "Step 19442, train_loss 0.4527034720632745, test_loss 0.4409848090386217\n",
            "Step 19443, train_loss 0.45270298456907904, test_loss 0.4409845779176463\n",
            "Step 19444, train_loss 0.45270249711435134, test_loss 0.440984346844017\n",
            "Step 19445, train_loss 0.45270200969908564, test_loss 0.44098411581772556\n",
            "Step 19446, train_loss 0.4527015223232765, test_loss 0.44098388483876344\n",
            "Step 19447, train_loss 0.45270103498691805, test_loss 0.4409836539071222\n",
            "Step 19448, train_loss 0.45270054769000473, test_loss 0.4409834230227937\n",
            "Step 19449, train_loss 0.45270006043253114, test_loss 0.4409831921857691\n",
            "Step 19450, train_loss 0.45269957321449134, test_loss 0.4409829613960402\n",
            "Step 19451, train_loss 0.4526990860358797, test_loss 0.44098273065359866\n",
            "Step 19452, train_loss 0.4526985988966907, test_loss 0.44098249995843586\n",
            "Step 19453, train_loss 0.45269811179691866, test_loss 0.44098226931054363\n",
            "Step 19454, train_loss 0.45269762473655784, test_loss 0.4409820387099133\n",
            "Step 19455, train_loss 0.4526971377156029, test_loss 0.4409818081565366\n",
            "Step 19456, train_loss 0.45269665073404786, test_loss 0.4409815776504052\n",
            "Step 19457, train_loss 0.4526961637918874, test_loss 0.44098134719151044\n",
            "Step 19458, train_loss 0.4526956768891156, test_loss 0.4409811167798442\n",
            "Step 19459, train_loss 0.452695190025727, test_loss 0.44098088641539784\n",
            "Step 19460, train_loss 0.45269470320171606, test_loss 0.4409806560981631\n",
            "Step 19461, train_loss 0.4526942164170769, test_loss 0.44098042582813163\n",
            "Step 19462, train_loss 0.45269372967180416, test_loss 0.44098019560529494\n",
            "Step 19463, train_loss 0.45269324296589203, test_loss 0.44097996542964446\n",
            "Step 19464, train_loss 0.45269275629933503, test_loss 0.4409797353011721\n",
            "Step 19465, train_loss 0.4526922696721275, test_loss 0.4409795052198695\n",
            "Step 19466, train_loss 0.4526917830842637, test_loss 0.44097927518572794\n",
            "Step 19467, train_loss 0.4526912965357382, test_loss 0.44097904519873926\n",
            "Step 19468, train_loss 0.45269081002654515, test_loss 0.4409788152588951\n",
            "Step 19469, train_loss 0.45269032355667926, test_loss 0.44097858536618695\n",
            "Step 19470, train_loss 0.4526898371261346, test_loss 0.44097835552060644\n",
            "Step 19471, train_loss 0.4526893507349058, test_loss 0.44097812572214534\n",
            "Step 19472, train_loss 0.4526888643829871, test_loss 0.44097789597079495\n",
            "Step 19473, train_loss 0.452688378070373, test_loss 0.44097766626654733\n",
            "Step 19474, train_loss 0.4526878917970578, test_loss 0.4409774366093938\n",
            "Step 19475, train_loss 0.45268740556303594, test_loss 0.4409772069993261\n",
            "Step 19476, train_loss 0.45268691936830185, test_loss 0.44097697743633585\n",
            "Step 19477, train_loss 0.4526864332128498, test_loss 0.44097674792041464\n",
            "Step 19478, train_loss 0.4526859470966743, test_loss 0.4409765184515541\n",
            "Step 19479, train_loss 0.45268546101976986, test_loss 0.4409762890297459\n",
            "Step 19480, train_loss 0.4526849749821307, test_loss 0.44097605965498177\n",
            "Step 19481, train_loss 0.4526844889837512, test_loss 0.4409758303272533\n",
            "Step 19482, train_loss 0.45268400302462597, test_loss 0.440975601046552\n",
            "Step 19483, train_loss 0.4526835171047492, test_loss 0.4409753718128695\n",
            "Step 19484, train_loss 0.4526830312241155, test_loss 0.4409751426261977\n",
            "Step 19485, train_loss 0.45268254538271907, test_loss 0.440974913486528\n",
            "Step 19486, train_loss 0.45268205958055446, test_loss 0.4409746843938522\n",
            "Step 19487, train_loss 0.45268157381761603, test_loss 0.4409744553481619\n",
            "Step 19488, train_loss 0.45268108809389834, test_loss 0.44097422634944866\n",
            "Step 19489, train_loss 0.45268060240939556, test_loss 0.4409739973977045\n",
            "Step 19490, train_loss 0.4526801167641022, test_loss 0.4409737684929205\n",
            "Step 19491, train_loss 0.45267963115801274, test_loss 0.44097353963508873\n",
            "Step 19492, train_loss 0.4526791455911215, test_loss 0.4409733108242008\n",
            "Step 19493, train_loss 0.45267866006342306, test_loss 0.44097308206024816\n",
            "Step 19494, train_loss 0.4526781745749117, test_loss 0.4409728533432228\n",
            "Step 19495, train_loss 0.452677689125582, test_loss 0.44097262467311604\n",
            "Step 19496, train_loss 0.4526772037154282, test_loss 0.44097239604991995\n",
            "Step 19497, train_loss 0.4526767183444448, test_loss 0.4409721674736257\n",
            "Step 19498, train_loss 0.4526762330126263, test_loss 0.44097193894422554\n",
            "Step 19499, train_loss 0.452675747719967, test_loss 0.44097171046171063\n",
            "Step 19501, train_loss 0.4526747772521039, test_loss 0.44097125363730405\n",
            "Step 19502, train_loss 0.45267429207688914, test_loss 0.44097102529539567\n",
            "Step 19503, train_loss 0.45267380694081116, test_loss 0.44097079700033953\n",
            "Step 19504, train_loss 0.4526733218438648, test_loss 0.4409705687521271\n",
            "Step 19505, train_loss 0.45267283678604414, test_loss 0.4409703405507503\n",
            "Step 19506, train_loss 0.45267235176734394, test_loss 0.4409701123962009\n",
            "Step 19507, train_loss 0.45267186678775845, test_loss 0.4409698842884702\n",
            "Step 19508, train_loss 0.4526713818472821, test_loss 0.44096965622755024\n",
            "Step 19509, train_loss 0.45267089694590945, test_loss 0.44096942821343255\n",
            "Step 19510, train_loss 0.45267041208363495, test_loss 0.4409692002461088\n",
            "Step 19511, train_loss 0.45266992726045296, test_loss 0.4409689723255709\n",
            "Step 19512, train_loss 0.4526694424763579, test_loss 0.44096874445181045\n",
            "Step 19513, train_loss 0.4526689577313443, test_loss 0.44096851662481906\n",
            "Step 19514, train_loss 0.4526684730254065, test_loss 0.44096828884458833\n",
            "Step 19515, train_loss 0.4526679883585392, test_loss 0.44096806111111025\n",
            "Step 19516, train_loss 0.45266750373073655, test_loss 0.44096783342437634\n",
            "Step 19517, train_loss 0.45266701914199337, test_loss 0.4409676057843784\n",
            "Step 19518, train_loss 0.4526665345923037, test_loss 0.4409673781911081\n",
            "Step 19519, train_loss 0.45266605008166233, test_loss 0.4409671506445572\n",
            "Step 19520, train_loss 0.4526655656100635, test_loss 0.44096692314471736\n",
            "Step 19521, train_loss 0.4526650811775017, test_loss 0.4409666956915803\n",
            "Step 19522, train_loss 0.45266459678397153, test_loss 0.4409664682851378\n",
            "Step 19523, train_loss 0.4526641124294674, test_loss 0.44096624092538134\n",
            "Step 19524, train_loss 0.4526636281139837, test_loss 0.4409660136123029\n",
            "Step 19525, train_loss 0.452663143837515, test_loss 0.4409657863458943\n",
            "Step 19526, train_loss 0.45266265960005564, test_loss 0.44096555912614693\n",
            "Step 19527, train_loss 0.4526621754016002, test_loss 0.4409653319530527\n",
            "Step 19528, train_loss 0.45266169124214306, test_loss 0.4409651048266035\n",
            "Step 19529, train_loss 0.4526612071216789, test_loss 0.4409648777467908\n",
            "Step 19530, train_loss 0.4526607230402019, test_loss 0.44096465071360624\n",
            "Step 19531, train_loss 0.45266023899770674, test_loss 0.440964423727042\n",
            "Step 19532, train_loss 0.4526597549941878, test_loss 0.4409641967870894\n",
            "Step 19533, train_loss 0.45265927102963965, test_loss 0.4409639698937405\n",
            "Step 19534, train_loss 0.45265878710405666, test_loss 0.4409637430469867\n",
            "Step 19535, train_loss 0.45265830321743344, test_loss 0.44096351624682\n",
            "Step 19536, train_loss 0.4526578193697643, test_loss 0.44096328949323205\n",
            "Step 19537, train_loss 0.4526573355610438, test_loss 0.4409630627862148\n",
            "Step 19538, train_loss 0.4526568517912666, test_loss 0.4409628361257595\n",
            "Step 19539, train_loss 0.45265636806042686, test_loss 0.44096260951185834\n",
            "Step 19540, train_loss 0.45265588436851933, test_loss 0.440962382944503\n",
            "Step 19541, train_loss 0.4526554007155384, test_loss 0.4409621564236852\n",
            "Step 19542, train_loss 0.4526549171014786, test_loss 0.44096192994939654\n",
            "Step 19543, train_loss 0.45265443352633433, test_loss 0.4409617035216291\n",
            "Step 19544, train_loss 0.45265394999010017, test_loss 0.4409614771403743\n",
            "Step 19545, train_loss 0.4526534664927706, test_loss 0.4409612508056242\n",
            "Step 19546, train_loss 0.4526529830343401, test_loss 0.44096102451737035\n",
            "Step 19547, train_loss 0.45265249961480325, test_loss 0.44096079827560464\n",
            "Step 19548, train_loss 0.4526520162341543, test_loss 0.4409605720803187\n",
            "Step 19549, train_loss 0.45265153289238813, test_loss 0.44096034593150446\n",
            "Step 19550, train_loss 0.45265104958949887, test_loss 0.4409601198291537\n",
            "Step 19551, train_loss 0.4526505663254813, test_loss 0.44095989377325795\n",
            "Step 19552, train_loss 0.4526500831003297, test_loss 0.4409596677638092\n",
            "Step 19553, train_loss 0.45264959991403875, test_loss 0.4409594418007992\n",
            "Step 19554, train_loss 0.4526491167666029, test_loss 0.4409592158842198\n",
            "Step 19555, train_loss 0.4526486336580167, test_loss 0.4409589900140626\n",
            "Step 19556, train_loss 0.4526481505882746, test_loss 0.4409587641903195\n",
            "Step 19557, train_loss 0.4526476675573711, test_loss 0.44095853841298216\n",
            "Step 19558, train_loss 0.4526471845653008, test_loss 0.4409583126820426\n",
            "Step 19559, train_loss 0.45264670161205806, test_loss 0.44095808699749245\n",
            "Step 19560, train_loss 0.4526462186976376, test_loss 0.44095786135932336\n",
            "Step 19561, train_loss 0.4526457358220338, test_loss 0.44095763576752756\n",
            "Step 19562, train_loss 0.45264525298524116, test_loss 0.44095741022209634\n",
            "Step 19563, train_loss 0.45264477018725435, test_loss 0.4409571847230218\n",
            "Step 19564, train_loss 0.45264428742806767, test_loss 0.4409569592702957\n",
            "Step 19565, train_loss 0.45264380470767585, test_loss 0.4409567338639097\n",
            "Step 19566, train_loss 0.4526433220260732, test_loss 0.44095650850385587\n",
            "Step 19567, train_loss 0.45264283938325456, test_loss 0.44095628319012575\n",
            "Step 19568, train_loss 0.45264235677921416, test_loss 0.4409560579227112\n",
            "Step 19569, train_loss 0.45264187421394675, test_loss 0.44095583270160427\n",
            "Step 19570, train_loss 0.45264139168744666, test_loss 0.4409556075267964\n",
            "Step 19571, train_loss 0.4526409091997085, test_loss 0.44095538239827964\n",
            "Step 19572, train_loss 0.4526404267507268, test_loss 0.4409551573160457\n",
            "Step 19573, train_loss 0.4526399443404962, test_loss 0.4409549322800865\n",
            "Step 19574, train_loss 0.45263946196901117, test_loss 0.44095470729039377\n",
            "Step 19575, train_loss 0.4526389796362661, test_loss 0.4409544823469594\n",
            "Step 19576, train_loss 0.4526384973422557, test_loss 0.44095425744977507\n",
            "Step 19577, train_loss 0.4526380150869745, test_loss 0.4409540325988327\n",
            "Step 19578, train_loss 0.4526375328704169, test_loss 0.44095380779412413\n",
            "Step 19579, train_loss 0.4526370506925776, test_loss 0.44095358303564114\n",
            "Step 19580, train_loss 0.4526365685534511, test_loss 0.4409533583233757\n",
            "Step 19581, train_loss 0.4526360864530319, test_loss 0.4409531336573193\n",
            "Step 19582, train_loss 0.4526356043913146, test_loss 0.4409529090374642\n",
            "Step 19583, train_loss 0.45263512236829373, test_loss 0.440952684463802\n",
            "Step 19584, train_loss 0.4526346403839638, test_loss 0.4409524599363244\n",
            "Step 19585, train_loss 0.45263415843831944, test_loss 0.44095223545502366\n",
            "Step 19586, train_loss 0.45263367653135517, test_loss 0.4409520110198911\n",
            "Step 19587, train_loss 0.4526331946630655, test_loss 0.4409517866309191\n",
            "Step 19588, train_loss 0.452632712833445, test_loss 0.44095156228809906\n",
            "Step 19589, train_loss 0.4526322310424883, test_loss 0.44095133799142294\n",
            "Step 19590, train_loss 0.45263174929018973, test_loss 0.44095111374088275\n",
            "Step 19591, train_loss 0.45263126757654426, test_loss 0.4409508895364702\n",
            "Step 19592, train_loss 0.45263078590154604, test_loss 0.44095066537817723\n",
            "Step 19593, train_loss 0.4526303042651898, test_loss 0.4409504412659956\n",
            "Step 19594, train_loss 0.45262982266747026, test_loss 0.44095021719991717\n",
            "Step 19595, train_loss 0.45262934110838166, test_loss 0.4409499931799339\n",
            "Step 19596, train_loss 0.45262885958791876, test_loss 0.4409497692060375\n",
            "Step 19597, train_loss 0.4526283781060762, test_loss 0.4409495452782199\n",
            "Step 19598, train_loss 0.45262789666284853, test_loss 0.4409493213964729\n",
            "Step 19599, train_loss 0.45262741525822997, test_loss 0.4409490975607886\n",
            "Step 19601, train_loss 0.4526264525647996, test_loss 0.4409486500275748\n",
            "Step 19602, train_loss 0.4526259712759767, test_loss 0.4409484263300293\n",
            "Step 19603, train_loss 0.45262549002574154, test_loss 0.44094820267851365\n",
            "Step 19604, train_loss 0.4526250088140886, test_loss 0.44094797907301986\n",
            "Step 19605, train_loss 0.4526245276410126, test_loss 0.4409477555135399\n",
            "Step 19606, train_loss 0.45262404650650795, test_loss 0.4409475320000656\n",
            "Step 19607, train_loss 0.45262356541056925, test_loss 0.4409473085325888\n",
            "Step 19608, train_loss 0.45262308435319104, test_loss 0.44094708511110126\n",
            "Step 19609, train_loss 0.4526226033343681, test_loss 0.4409468617355951\n",
            "Step 19610, train_loss 0.45262212235409494, test_loss 0.4409466384060621\n",
            "Step 19611, train_loss 0.452621641412366, test_loss 0.4409464151224941\n",
            "Step 19612, train_loss 0.45262116050917606, test_loss 0.4409461918848831\n",
            "Step 19613, train_loss 0.4526206796445195, test_loss 0.44094596869322084\n",
            "Step 19614, train_loss 0.45262019881839116, test_loss 0.4409457455474994\n",
            "Step 19615, train_loss 0.45261971803078543, test_loss 0.44094552244771035\n",
            "Step 19616, train_loss 0.4526192372816971, test_loss 0.4409452993938461\n",
            "Step 19617, train_loss 0.4526187565711205, test_loss 0.440945076385898\n",
            "Step 19618, train_loss 0.4526182758990505, test_loss 0.44094485342385836\n",
            "Step 19619, train_loss 0.45261779526548146, test_loss 0.4409446305077187\n",
            "Step 19620, train_loss 0.4526173146704082, test_loss 0.4409444076374714\n",
            "Step 19621, train_loss 0.45261683411382514, test_loss 0.440944184813108\n",
            "Step 19622, train_loss 0.45261635359572694, test_loss 0.44094396203462044\n",
            "Step 19623, train_loss 0.45261587311610824, test_loss 0.4409437393020008\n",
            "Step 19624, train_loss 0.4526153926749636, test_loss 0.44094351661524084\n",
            "Step 19625, train_loss 0.45261491227228756, test_loss 0.4409432939743326\n",
            "Step 19626, train_loss 0.45261443190807493, test_loss 0.4409430713792679\n",
            "Step 19627, train_loss 0.45261395158232015, test_loss 0.44094284883003854\n",
            "Step 19628, train_loss 0.45261347129501794, test_loss 0.4409426263266367\n",
            "Step 19629, train_loss 0.45261299104616276, test_loss 0.4409424038690542\n",
            "Step 19630, train_loss 0.4526125108357494, test_loss 0.44094218145728287\n",
            "Step 19631, train_loss 0.45261203066377226, test_loss 0.4409419590913148\n",
            "Step 19632, train_loss 0.4526115505302262, test_loss 0.4409417367711416\n",
            "Step 19633, train_loss 0.4526110704351057, test_loss 0.4409415144967555\n",
            "Step 19634, train_loss 0.4526105903784053, test_loss 0.44094129226814843\n",
            "Step 19635, train_loss 0.4526101103601198, test_loss 0.44094107008531214\n",
            "Step 19636, train_loss 0.45260963038024365, test_loss 0.4409408479482388\n",
            "Step 19637, train_loss 0.4526091504387717, test_loss 0.44094062585692\n",
            "Step 19638, train_loss 0.45260867053569837, test_loss 0.4409404038113479\n",
            "Step 19639, train_loss 0.45260819067101826, test_loss 0.4409401818115145\n",
            "Step 19640, train_loss 0.4526077108447261, test_loss 0.44093995985741163\n",
            "Step 19641, train_loss 0.4526072310568166, test_loss 0.44093973794903124\n",
            "Step 19642, train_loss 0.4526067513072842, test_loss 0.44093951608636517\n",
            "Step 19643, train_loss 0.4526062715961236, test_loss 0.4409392942694056\n",
            "Step 19644, train_loss 0.4526057919233296, test_loss 0.44093907249814435\n",
            "Step 19645, train_loss 0.45260531228889656, test_loss 0.4409388507725734\n",
            "Step 19646, train_loss 0.45260483269281915, test_loss 0.4409386290926847\n",
            "Step 19647, train_loss 0.4526043531350923, test_loss 0.44093840745847007\n",
            "Step 19648, train_loss 0.45260387361571025, test_loss 0.4409381858699217\n",
            "Step 19649, train_loss 0.4526033941346679, test_loss 0.44093796432703136\n",
            "Step 19650, train_loss 0.4526029146919598, test_loss 0.44093774282979115\n",
            "Step 19651, train_loss 0.4526024352875806, test_loss 0.4409375213781928\n",
            "Step 19652, train_loss 0.452601955921525, test_loss 0.4409372999722285\n",
            "Step 19653, train_loss 0.4526014765937876, test_loss 0.44093707861189024\n",
            "Step 19654, train_loss 0.45260099730436304, test_loss 0.44093685729716975\n",
            "Step 19655, train_loss 0.4526005180532458, test_loss 0.44093663602805916\n",
            "Step 19656, train_loss 0.4526000388404308, test_loss 0.4409364148045505\n",
            "Step 19657, train_loss 0.4525995596659126, test_loss 0.4409361936266355\n",
            "Step 19658, train_loss 0.4525990805296859, test_loss 0.4409359724943064\n",
            "Step 19659, train_loss 0.4525986014317452, test_loss 0.4409357514075549\n",
            "Step 19660, train_loss 0.45259812237208513, test_loss 0.44093553036637334\n",
            "Step 19661, train_loss 0.45259764335070063, test_loss 0.44093530937075337\n",
            "Step 19662, train_loss 0.45259716436758607, test_loss 0.44093508842068724\n",
            "Step 19663, train_loss 0.45259668542273634, test_loss 0.4409348675161668\n",
            "Step 19664, train_loss 0.4525962065161458, test_loss 0.4409346466571838\n",
            "Step 19665, train_loss 0.4525957276478093, test_loss 0.44093442584373055\n",
            "Step 19666, train_loss 0.4525952488177216, test_loss 0.44093420507579895\n",
            "Step 19667, train_loss 0.4525947700258771, test_loss 0.4409339843533809\n",
            "Step 19668, train_loss 0.45259429127227085, test_loss 0.44093376367646864\n",
            "Step 19669, train_loss 0.452593812556897, test_loss 0.44093354304505383\n",
            "Step 19670, train_loss 0.45259333387975065, test_loss 0.4409333224591287\n",
            "Step 19671, train_loss 0.4525928552408264, test_loss 0.44093310191868523\n",
            "Step 19672, train_loss 0.45259237664011864, test_loss 0.4409328814237153\n",
            "Step 19673, train_loss 0.45259189807762223, test_loss 0.44093266097421085\n",
            "Step 19674, train_loss 0.452591419553332, test_loss 0.44093244057016423\n",
            "Step 19675, train_loss 0.45259094106724224, test_loss 0.4409322202115671\n",
            "Step 19676, train_loss 0.452590462619348, test_loss 0.4409319998984116\n",
            "Step 19677, train_loss 0.45258998420964375, test_loss 0.4409317796306897\n",
            "Step 19678, train_loss 0.45258950583812424, test_loss 0.44093155940839335\n",
            "Step 19679, train_loss 0.4525890275047841, test_loss 0.4409313392315148\n",
            "Step 19680, train_loss 0.45258854920961805, test_loss 0.44093111910004584\n",
            "Step 19681, train_loss 0.45258807095262077, test_loss 0.44093089901397853\n",
            "Step 19682, train_loss 0.452587592733787, test_loss 0.440930678973305\n",
            "Step 19683, train_loss 0.4525871145531113, test_loss 0.4409304589780171\n",
            "Step 19684, train_loss 0.45258663641058844, test_loss 0.4409302390281069\n",
            "Step 19685, train_loss 0.4525861583062131, test_loss 0.44093001912356655\n",
            "Step 19686, train_loss 0.4525856802399798, test_loss 0.44092979926438797\n",
            "Step 19687, train_loss 0.4525852022118835, test_loss 0.440929579450563\n",
            "Step 19688, train_loss 0.4525847242219188, test_loss 0.440929359682084\n",
            "Step 19689, train_loss 0.4525842462700803, test_loss 0.4409291399589429\n",
            "Step 19690, train_loss 0.45258376835636277, test_loss 0.4409289202811317\n",
            "Step 19691, train_loss 0.45258329048076085, test_loss 0.4409287006486423\n",
            "Step 19692, train_loss 0.4525828126432693, test_loss 0.44092848106146687\n",
            "Step 19693, train_loss 0.45258233484388277, test_loss 0.4409282615195976\n",
            "Step 19694, train_loss 0.4525818570825961, test_loss 0.4409280420230262\n",
            "Step 19695, train_loss 0.45258137935940373, test_loss 0.4409278225717451\n",
            "Step 19696, train_loss 0.45258090167430054, test_loss 0.4409276031657458\n",
            "Step 19697, train_loss 0.4525804240272812, test_loss 0.44092738380502083\n",
            "Step 19698, train_loss 0.45257994641834043, test_loss 0.4409271644895621\n",
            "Step 19699, train_loss 0.45257946884747274, test_loss 0.44092694521936154\n",
            "Step 19701, train_loss 0.45257851381993636, test_loss 0.4409265068147035\n",
            "Step 19702, train_loss 0.45257803636325683, test_loss 0.4409262876802301\n",
            "Step 19703, train_loss 0.4525775589446293, test_loss 0.44092606859098316\n",
            "Step 19704, train_loss 0.4525770815640487, test_loss 0.44092584954695463\n",
            "Step 19705, train_loss 0.4525766042215096, test_loss 0.44092563054813677\n",
            "Step 19706, train_loss 0.45257612691700666, test_loss 0.44092541159452153\n",
            "Step 19707, train_loss 0.4525756496505348, test_loss 0.4409251926861011\n",
            "Step 19708, train_loss 0.45257517242208845, test_loss 0.44092497382286727\n",
            "Step 19709, train_loss 0.45257469523166255, test_loss 0.4409247550048124\n",
            "Step 19710, train_loss 0.4525742180792517, test_loss 0.4409245362319284\n",
            "Step 19711, train_loss 0.4525737409648507, test_loss 0.4409243175042073\n",
            "Step 19712, train_loss 0.4525732638884543, test_loss 0.4409240988216413\n",
            "Step 19713, train_loss 0.4525727868500571, test_loss 0.4409238801842224\n",
            "Step 19714, train_loss 0.4525723098496539, test_loss 0.4409236615919427\n",
            "Step 19715, train_loss 0.4525718328872394, test_loss 0.44092344304479414\n",
            "Step 19716, train_loss 0.45257135596280834, test_loss 0.44092322454276905\n",
            "Step 19717, train_loss 0.45257087907635557, test_loss 0.44092300608585955\n",
            "Step 19718, train_loss 0.4525704022278756, test_loss 0.4409227876740573\n",
            "Step 19719, train_loss 0.4525699254173633, test_loss 0.4409225693073548\n",
            "Step 19720, train_loss 0.45256944864481335, test_loss 0.4409223509857439\n",
            "Step 19721, train_loss 0.45256897191022055, test_loss 0.4409221327092168\n",
            "Step 19722, train_loss 0.45256849521357956, test_loss 0.4409219144777655\n",
            "Step 19723, train_loss 0.4525680185548852, test_loss 0.4409216962913822\n",
            "Step 19724, train_loss 0.4525675419341321, test_loss 0.44092147815005905\n",
            "Step 19725, train_loss 0.4525670653513151, test_loss 0.4409212600537879\n",
            "Step 19726, train_loss 0.4525665888064289, test_loss 0.44092104200256105\n",
            "Step 19727, train_loss 0.4525661122994683, test_loss 0.44092082399637045\n",
            "Step 19728, train_loss 0.45256563583042786, test_loss 0.4409206060352083\n",
            "Step 19729, train_loss 0.4525651593993024, test_loss 0.4409203881190667\n",
            "Step 19730, train_loss 0.45256468300608693, test_loss 0.44092017024793784\n",
            "Step 19731, train_loss 0.45256420665077585, test_loss 0.44091995242181353\n",
            "Step 19732, train_loss 0.452563730333364, test_loss 0.44091973464068623\n",
            "Step 19733, train_loss 0.4525632540538463, test_loss 0.4409195169045478\n",
            "Step 19734, train_loss 0.45256277781221727, test_loss 0.4409192992133904\n",
            "Step 19735, train_loss 0.4525623016084719, test_loss 0.44091908156720633\n",
            "Step 19736, train_loss 0.4525618254426047, test_loss 0.44091886396598745\n",
            "Step 19737, train_loss 0.45256134931461056, test_loss 0.44091864640972595\n",
            "Step 19738, train_loss 0.4525608732244843, test_loss 0.4409184288984141\n",
            "Step 19739, train_loss 0.45256039717222035, test_loss 0.44091821143204374\n",
            "Step 19740, train_loss 0.45255992115781396, test_loss 0.4409179940106074\n",
            "Step 19741, train_loss 0.4525594451812596, test_loss 0.4409177766340968\n",
            "Step 19742, train_loss 0.452558969242552, test_loss 0.4409175593025043\n",
            "Step 19743, train_loss 0.45255849334168613, test_loss 0.44091734201582183\n",
            "Step 19744, train_loss 0.4525580174786565, test_loss 0.4409171247740416\n",
            "Step 19745, train_loss 0.452557541653458, test_loss 0.44091690757715596\n",
            "Step 19746, train_loss 0.45255706586608546, test_loss 0.44091669042515674\n",
            "Step 19747, train_loss 0.4525565901165337, test_loss 0.44091647331803635\n",
            "Step 19748, train_loss 0.4525561144047972, test_loss 0.4409162562557866\n",
            "Step 19749, train_loss 0.4525556387308709, test_loss 0.44091603923839984\n",
            "Step 19750, train_loss 0.4525551630947497, test_loss 0.44091582226586823\n",
            "Step 19751, train_loss 0.4525546874964282, test_loss 0.44091560533818375\n",
            "Step 19752, train_loss 0.4525542119359012, test_loss 0.4409153884553388\n",
            "Step 19753, train_loss 0.4525537364131635, test_loss 0.44091517161732524\n",
            "Step 19754, train_loss 0.45255326092820997, test_loss 0.44091495482413545\n",
            "Step 19755, train_loss 0.4525527854810353, test_loss 0.44091473807576126\n",
            "Step 19756, train_loss 0.4525523100716341, test_loss 0.4409145213721952\n",
            "Step 19757, train_loss 0.45255183470000154, test_loss 0.4409143047134293\n",
            "Step 19758, train_loss 0.45255135936613206, test_loss 0.4409140880994556\n",
            "Step 19759, train_loss 0.45255088407002064, test_loss 0.4409138715302664\n",
            "Step 19760, train_loss 0.452550408811662, test_loss 0.44091365500585367\n",
            "Step 19761, train_loss 0.45254993359105095, test_loss 0.44091343852620973\n",
            "Step 19762, train_loss 0.45254945840818217, test_loss 0.4409132220913267\n",
            "Step 19763, train_loss 0.45254898326305054, test_loss 0.4409130057011968\n",
            "Step 19764, train_loss 0.45254850815565084, test_loss 0.44091278935581213\n",
            "Step 19765, train_loss 0.45254803308597796, test_loss 0.4409125730551648\n",
            "Step 19766, train_loss 0.4525475580540266, test_loss 0.44091235679924706\n",
            "Step 19767, train_loss 0.45254708305979147, test_loss 0.44091214058805095\n",
            "Step 19768, train_loss 0.45254660810326747, test_loss 0.4409119244215689\n",
            "Step 19769, train_loss 0.4525461331844495, test_loss 0.44091170829979287\n",
            "Step 19770, train_loss 0.45254565830333215, test_loss 0.4409114922227152\n",
            "Step 19771, train_loss 0.4525451834599103, test_loss 0.44091127619032766\n",
            "Step 19772, train_loss 0.4525447086541788, test_loss 0.4409110602026231\n",
            "Step 19773, train_loss 0.45254423388613224, test_loss 0.4409108442595932\n",
            "Step 19774, train_loss 0.45254375915576583, test_loss 0.4409106283612302\n",
            "Step 19775, train_loss 0.45254328446307407, test_loss 0.4409104125075265\n",
            "Step 19776, train_loss 0.45254280980805184, test_loss 0.4409101966984741\n",
            "Step 19777, train_loss 0.4525423351906939, test_loss 0.4409099809340651\n",
            "Step 19778, train_loss 0.4525418606109951, test_loss 0.44090976521429187\n",
            "Step 19779, train_loss 0.4525413860689504, test_loss 0.4409095495391467\n",
            "Step 19780, train_loss 0.4525409115645543, test_loss 0.4409093339086215\n",
            "Step 19781, train_loss 0.45254043709780184, test_loss 0.4409091183227087\n",
            "Step 19782, train_loss 0.45253996266868773, test_loss 0.4409089027814003\n",
            "Step 19783, train_loss 0.45253948827720697, test_loss 0.4409086872846886\n",
            "Step 19784, train_loss 0.4525390139233541, test_loss 0.440908471832566\n",
            "Step 19785, train_loss 0.45253853960712415, test_loss 0.4409082564250243\n",
            "Step 19786, train_loss 0.4525380653285118, test_loss 0.440908041062056\n",
            "Step 19787, train_loss 0.452537591087512, test_loss 0.4409078257436532\n",
            "Step 19788, train_loss 0.45253711688411946, test_loss 0.4409076104698079\n",
            "Step 19789, train_loss 0.4525366427183291, test_loss 0.44090739524051276\n",
            "Step 19790, train_loss 0.45253616859013585, test_loss 0.44090718005575985\n",
            "Step 19791, train_loss 0.4525356944995341, test_loss 0.4409069649155411\n",
            "Step 19792, train_loss 0.4525352204465191, test_loss 0.440906749819849\n",
            "Step 19793, train_loss 0.45253474643108554, test_loss 0.4409065347686756\n",
            "Step 19794, train_loss 0.4525342724532282, test_loss 0.44090631976201317\n",
            "Step 19795, train_loss 0.45253379851294195, test_loss 0.440906104799854\n",
            "Step 19796, train_loss 0.4525333246102217, test_loss 0.44090588988219026\n",
            "Step 19797, train_loss 0.4525328507450623, test_loss 0.44090567500901423\n",
            "Step 19798, train_loss 0.4525323769174583, test_loss 0.44090546018031823\n",
            "Step 19799, train_loss 0.4525319031274049, test_loss 0.4409052453960942\n",
            "Step 19801, train_loss 0.45253095565992857, test_loss 0.4409048159610312\n",
            "Step 19802, train_loss 0.4525304819824954, test_loss 0.4409046013101769\n",
            "Step 19803, train_loss 0.45253000834259216, test_loss 0.44090438670376364\n",
            "Step 19804, train_loss 0.45252953474021346, test_loss 0.44090417214178357\n",
            "Step 19805, train_loss 0.4525290611753542, test_loss 0.44090395762422907\n",
            "Step 19806, train_loss 0.45252858764800935, test_loss 0.44090374315109243\n",
            "Step 19807, train_loss 0.45252811415817357, test_loss 0.4409035287223656\n",
            "Step 19808, train_loss 0.45252764070584195, test_loss 0.4409033143380409\n",
            "Step 19809, train_loss 0.45252716729100906, test_loss 0.4409030999981109\n",
            "Step 19810, train_loss 0.45252669391367, test_loss 0.44090288570256747\n",
            "Step 19811, train_loss 0.4525262205738195, test_loss 0.4409026714514031\n",
            "Step 19812, train_loss 0.4525257472714524, test_loss 0.44090245724460997\n",
            "Step 19813, train_loss 0.4525252740065636, test_loss 0.44090224308218023\n",
            "Step 19814, train_loss 0.4525248007791479, test_loss 0.44090202896410624\n",
            "Step 19815, train_loss 0.4525243275892003, test_loss 0.4409018148903803\n",
            "Step 19816, train_loss 0.45252385443671544, test_loss 0.4409016008609945\n",
            "Step 19817, train_loss 0.45252338132168834, test_loss 0.44090138687594116\n",
            "Step 19818, train_loss 0.4525229082441138, test_loss 0.4409011729352127\n",
            "Step 19819, train_loss 0.4525224352039867, test_loss 0.4409009590388012\n",
            "Step 19820, train_loss 0.45252196220130186, test_loss 0.4409007451866989\n",
            "Step 19821, train_loss 0.4525214892360541, test_loss 0.44090053137889823\n",
            "Step 19822, train_loss 0.45252101630823865, test_loss 0.4409003176153914\n",
            "Step 19823, train_loss 0.4525205434178498, test_loss 0.44090010389617057\n",
            "Step 19824, train_loss 0.4525200705648829, test_loss 0.4408998902212281\n",
            "Step 19825, train_loss 0.4525195977493326, test_loss 0.44089967659055634\n",
            "Step 19826, train_loss 0.45251912497119373, test_loss 0.4408994630041474\n",
            "Step 19827, train_loss 0.4525186522304612, test_loss 0.4408992494619936\n",
            "Step 19828, train_loss 0.4525181795271302, test_loss 0.44089903596408736\n",
            "Step 19829, train_loss 0.4525177068611951, test_loss 0.4408988225104207\n",
            "Step 19830, train_loss 0.45251723423265094, test_loss 0.4408986091009861\n",
            "Step 19831, train_loss 0.4525167616414928, test_loss 0.44089839573577577\n",
            "Step 19832, train_loss 0.4525162890877154, test_loss 0.4408981824147821\n",
            "Step 19833, train_loss 0.45251581657131346, test_loss 0.4408979691379972\n",
            "Step 19834, train_loss 0.4525153440922823, test_loss 0.4408977559054135\n",
            "Step 19835, train_loss 0.4525148716506163, test_loss 0.44089754271702325\n",
            "Step 19836, train_loss 0.45251439924631076, test_loss 0.44089732957281874\n",
            "Step 19837, train_loss 0.4525139268793603, test_loss 0.44089711647279217\n",
            "Step 19838, train_loss 0.45251345454976, test_loss 0.440896903416936\n",
            "Step 19839, train_loss 0.45251298225750447, test_loss 0.44089669040524243\n",
            "Step 19840, train_loss 0.45251251000258885, test_loss 0.44089647743770377\n",
            "Step 19841, train_loss 0.452512037785008, test_loss 0.4408962645143123\n",
            "Step 19842, train_loss 0.4525115656047567, test_loss 0.44089605163506035\n",
            "Step 19843, train_loss 0.45251109346182994, test_loss 0.44089583879994027\n",
            "Step 19844, train_loss 0.4525106213562226, test_loss 0.44089562600894433\n",
            "Step 19845, train_loss 0.4525101492879295, test_loss 0.44089541326206466\n",
            "Step 19846, train_loss 0.4525096772569456, test_loss 0.44089520055929393\n",
            "Step 19847, train_loss 0.45250920526326577, test_loss 0.4408949879006241\n",
            "Step 19848, train_loss 0.452508733306885, test_loss 0.4408947752860477\n",
            "Step 19849, train_loss 0.45250826138779804, test_loss 0.4408945627155568\n",
            "Step 19850, train_loss 0.4525077895059999, test_loss 0.44089435018914425\n",
            "Step 19851, train_loss 0.4525073176614855, test_loss 0.4408941377068018\n",
            "Step 19852, train_loss 0.45250684585424955, test_loss 0.44089392526852206\n",
            "Step 19853, train_loss 0.45250637408428723, test_loss 0.44089371287429713\n",
            "Step 19854, train_loss 0.4525059023515934, test_loss 0.4408935005241195\n",
            "Step 19855, train_loss 0.4525054306561627, test_loss 0.4408932882179815\n",
            "Step 19856, train_loss 0.45250495899799026, test_loss 0.4408930759558755\n",
            "Step 19857, train_loss 0.45250448737707105, test_loss 0.44089286373779374\n",
            "Step 19858, train_loss 0.4525040157933998, test_loss 0.44089265156372853\n",
            "Step 19859, train_loss 0.4525035442469716, test_loss 0.44089243943367223\n",
            "Step 19860, train_loss 0.4525030727377811, test_loss 0.4408922273476172\n",
            "Step 19861, train_loss 0.45250260126582353, test_loss 0.44089201530555566\n",
            "Step 19862, train_loss 0.45250212983109356, test_loss 0.4408918033074802\n",
            "Step 19863, train_loss 0.4525016584335863, test_loss 0.44089159135338285\n",
            "Step 19864, train_loss 0.45250118707329656, test_loss 0.4408913794432561\n",
            "Step 19865, train_loss 0.4525007157502193, test_loss 0.44089116757709235\n",
            "Step 19866, train_loss 0.45250024446434933, test_loss 0.4408909557548838\n",
            "Step 19867, train_loss 0.45249977321568186, test_loss 0.440890743976623\n",
            "Step 19868, train_loss 0.4524993020042114, test_loss 0.4408905322423021\n",
            "Step 19869, train_loss 0.4524988308299333, test_loss 0.4408903205519135\n",
            "Step 19870, train_loss 0.45249835969284213, test_loss 0.4408901089054497\n",
            "Step 19871, train_loss 0.452497888592933, test_loss 0.4408898973029028\n",
            "Step 19872, train_loss 0.45249741753020084, test_loss 0.44088968574426535\n",
            "Step 19873, train_loss 0.4524969465046406, test_loss 0.4408894742295295\n",
            "Step 19874, train_loss 0.4524964755162471, test_loss 0.44088926275868795\n",
            "Step 19875, train_loss 0.45249600456501526, test_loss 0.44088905133173273\n",
            "Step 19876, train_loss 0.4524955336509402, test_loss 0.4408888399486563\n",
            "Step 19877, train_loss 0.4524950627740167, test_loss 0.44088862860945105\n",
            "Step 19878, train_loss 0.4524945919342397, test_loss 0.4408884173141094\n",
            "Step 19879, train_loss 0.4524941211316043, test_loss 0.4408882060626235\n",
            "Step 19880, train_loss 0.4524936503661052, test_loss 0.44088799485498603\n",
            "Step 19881, train_loss 0.4524931796377376, test_loss 0.44088778369118925\n",
            "Step 19882, train_loss 0.45249270894649624, test_loss 0.44088757257122535\n",
            "Step 19883, train_loss 0.452492238292376, test_loss 0.44088736149508684\n",
            "Step 19884, train_loss 0.4524917676753721, test_loss 0.44088715046276605\n",
            "Step 19885, train_loss 0.4524912970954793, test_loss 0.4408869394742555\n",
            "Step 19886, train_loss 0.45249082655269257, test_loss 0.4408867285295475\n",
            "Step 19887, train_loss 0.4524903560470068, test_loss 0.4408865176286343\n",
            "Step 19888, train_loss 0.45248988557841713, test_loss 0.4408863067715084\n",
            "Step 19889, train_loss 0.4524894151469183, test_loss 0.44088609595816214\n",
            "Step 19890, train_loss 0.4524889447525054, test_loss 0.440885885188588\n",
            "Step 19891, train_loss 0.45248847439517326, test_loss 0.4408856744627782\n",
            "Step 19892, train_loss 0.45248800407491696, test_loss 0.44088546378072535\n",
            "Step 19893, train_loss 0.4524875337917315, test_loss 0.44088525314242144\n",
            "Step 19894, train_loss 0.45248706354561163, test_loss 0.44088504254785943\n",
            "Step 19895, train_loss 0.4524865933365524, test_loss 0.44088483199703127\n",
            "Step 19896, train_loss 0.45248612316454884, test_loss 0.4408846214899295\n",
            "Step 19897, train_loss 0.4524856530295958, test_loss 0.4408844110265465\n",
            "Step 19898, train_loss 0.4524851829316883, test_loss 0.4408842006068747\n",
            "Step 19899, train_loss 0.45248471287082137, test_loss 0.4408839902309065\n",
            "Step 19901, train_loss 0.45248377286018887, test_loss 0.4408835696100505\n",
            "Step 19902, train_loss 0.4524833029104132, test_loss 0.44088335936514733\n",
            "Step 19903, train_loss 0.45248283299765796, test_loss 0.4408831491639176\n",
            "Step 19904, train_loss 0.452482363121918, test_loss 0.4408829390063533\n",
            "Step 19905, train_loss 0.45248189328318833, test_loss 0.440882728892447\n",
            "Step 19906, train_loss 0.4524814234814639, test_loss 0.4408825188221912\n",
            "Step 19907, train_loss 0.45248095371673985, test_loss 0.44088230879557827\n",
            "Step 19908, train_loss 0.452480483989011, test_loss 0.44088209881260065\n",
            "Step 19909, train_loss 0.45248001429827217, test_loss 0.44088188887325064\n",
            "Step 19910, train_loss 0.4524795446445188, test_loss 0.4408816789775208\n",
            "Step 19911, train_loss 0.4524790750277454, test_loss 0.4408814691254034\n",
            "Step 19912, train_loss 0.45247860544794727, test_loss 0.44088125931689087\n",
            "Step 19913, train_loss 0.45247813590511915, test_loss 0.4408810495519759\n",
            "Step 19914, train_loss 0.4524776663992561, test_loss 0.44088083983065063\n",
            "Step 19915, train_loss 0.45247719693035326, test_loss 0.44088063015290757\n",
            "Step 19916, train_loss 0.45247672749840534, test_loss 0.4408804205187391\n",
            "Step 19917, train_loss 0.45247625810340747, test_loss 0.4408802109281378\n",
            "Step 19918, train_loss 0.4524757887453547, test_loss 0.44088000138109595\n",
            "Step 19919, train_loss 0.4524753194242421, test_loss 0.440879791877606\n",
            "Step 19920, train_loss 0.4524748501400644, test_loss 0.44087958241766056\n",
            "Step 19921, train_loss 0.4524743808928166, test_loss 0.4408793730012519\n",
            "Step 19922, train_loss 0.45247391168249385, test_loss 0.4408791636283726\n",
            "Step 19923, train_loss 0.4524734425090911, test_loss 0.4408789542990148\n",
            "Step 19924, train_loss 0.4524729733726034, test_loss 0.44087874501317126\n",
            "Step 19925, train_loss 0.45247250427302554, test_loss 0.44087853577083425\n",
            "Step 19926, train_loss 0.4524720352103528, test_loss 0.4408783265719963\n",
            "Step 19927, train_loss 0.45247156618458, test_loss 0.4408781174166499\n",
            "Step 19928, train_loss 0.4524710971957021, test_loss 0.4408779083047873\n",
            "Step 19929, train_loss 0.45247062824371415, test_loss 0.4408776992364012\n",
            "Step 19930, train_loss 0.45247015932861123, test_loss 0.4408774902114839\n",
            "Step 19931, train_loss 0.45246969045038843, test_loss 0.44087728123002784\n",
            "Step 19932, train_loss 0.4524692216090404, test_loss 0.4408770722920256\n",
            "Step 19933, train_loss 0.45246875280456245, test_loss 0.44087686339746956\n",
            "Step 19934, train_loss 0.45246828403694944, test_loss 0.4408766545463521\n",
            "Step 19935, train_loss 0.4524678153061965, test_loss 0.4408764457386659\n",
            "Step 19936, train_loss 0.4524673466122985, test_loss 0.4408762369744032\n",
            "Step 19937, train_loss 0.4524668779552506, test_loss 0.44087602825355665\n",
            "Step 19938, train_loss 0.4524664093350476, test_loss 0.4408758195761185\n",
            "Step 19939, train_loss 0.4524659407516848, test_loss 0.4408756109420814\n",
            "Step 19940, train_loss 0.4524654722051569, test_loss 0.4408754023514378\n",
            "Step 19941, train_loss 0.45246500369545917, test_loss 0.44087519380418\n",
            "Step 19942, train_loss 0.4524645352225865, test_loss 0.4408749853003007\n",
            "Step 19943, train_loss 0.452464066786534, test_loss 0.4408747768397923\n",
            "Step 19944, train_loss 0.4524635983872965, test_loss 0.4408745684226472\n",
            "Step 19945, train_loss 0.4524631300248692, test_loss 0.440874360048858\n",
            "Step 19946, train_loss 0.45246266169924704, test_loss 0.44087415171841715\n",
            "Step 19947, train_loss 0.45246219341042515, test_loss 0.44087394343131703\n",
            "Step 19948, train_loss 0.4524617251583984, test_loss 0.44087373518755024\n",
            "Step 19949, train_loss 0.45246125694316186, test_loss 0.4408735269871091\n",
            "Step 19950, train_loss 0.4524607887647107, test_loss 0.44087331882998626\n",
            "Step 19951, train_loss 0.4524603206230397, test_loss 0.4408731107161742\n",
            "Step 19952, train_loss 0.452459852518144, test_loss 0.4408729026456654\n",
            "Step 19953, train_loss 0.45245938445001876, test_loss 0.44087269461845224\n",
            "Step 19954, train_loss 0.4524589164186589, test_loss 0.44087248663452744\n",
            "Step 19955, train_loss 0.45245844842405936, test_loss 0.4408722786938832\n",
            "Step 19956, train_loss 0.4524579804662152, test_loss 0.44087207079651236\n",
            "Step 19957, train_loss 0.45245751254512173, test_loss 0.44087186294240693\n",
            "Step 19958, train_loss 0.45245704466077363, test_loss 0.44087165513156\n",
            "Step 19959, train_loss 0.45245657681316614, test_loss 0.44087144736396366\n",
            "Step 19960, train_loss 0.45245610900229416, test_loss 0.4408712396396105\n",
            "Step 19961, train_loss 0.45245564122815296, test_loss 0.44087103195849314\n",
            "Step 19962, train_loss 0.4524551734907373, test_loss 0.440870824320604\n",
            "Step 19963, train_loss 0.45245470579004243, test_loss 0.4408706167259355\n",
            "Step 19964, train_loss 0.4524542381260633, test_loss 0.4408704091744803\n",
            "Step 19965, train_loss 0.45245377049879487, test_loss 0.4408702016662308\n",
            "Step 19966, train_loss 0.4524533029082324, test_loss 0.4408699942011797\n",
            "Step 19967, train_loss 0.45245283535437075, test_loss 0.4408697867793194\n",
            "Step 19968, train_loss 0.4524523678372051, test_loss 0.44086957940064236\n",
            "Step 19969, train_loss 0.4524519003567304, test_loss 0.44086937206514115\n",
            "Step 19970, train_loss 0.45245143291294176, test_loss 0.44086916477280813\n",
            "Step 19971, train_loss 0.4524509655058342, test_loss 0.4408689575236363\n",
            "Step 19972, train_loss 0.4524504981354027, test_loss 0.44086875031761763\n",
            "Step 19973, train_loss 0.4524500308016425, test_loss 0.4408685431547449\n",
            "Step 19974, train_loss 0.45244956350454857, test_loss 0.4408683360350106\n",
            "Step 19975, train_loss 0.45244909624411594, test_loss 0.44086812895840743\n",
            "Step 19976, train_loss 0.4524486290203397, test_loss 0.4408679219249276\n",
            "Step 19977, train_loss 0.45244816183321485, test_loss 0.44086771493456384\n",
            "Step 19978, train_loss 0.45244769468273655, test_loss 0.4408675079873086\n",
            "Step 19979, train_loss 0.45244722756889966, test_loss 0.4408673010831546\n",
            "Step 19980, train_loss 0.45244676049169946, test_loss 0.4408670942220942\n",
            "Step 19981, train_loss 0.4524462934511309, test_loss 0.44086688740411994\n",
            "Step 19982, train_loss 0.45244582644718906, test_loss 0.4408666806292244\n",
            "Step 19983, train_loss 0.4524453594798692, test_loss 0.4408664738974002\n",
            "Step 19984, train_loss 0.4524448925491661, test_loss 0.44086626720863986\n",
            "Step 19985, train_loss 0.4524444256550748, test_loss 0.4408660605629356\n",
            "Step 19986, train_loss 0.45244395879759075, test_loss 0.44086585396028055\n",
            "Step 19987, train_loss 0.4524434919767087, test_loss 0.44086564740066664\n",
            "Step 19988, train_loss 0.45244302519242374, test_loss 0.4408654408840869\n",
            "Step 19989, train_loss 0.4524425584447311, test_loss 0.4408652344105338\n",
            "Step 19990, train_loss 0.45244209173362565, test_loss 0.44086502797999977\n",
            "Step 19991, train_loss 0.45244162505910274, test_loss 0.4408648215924773\n",
            "Step 19992, train_loss 0.4524411584211572, test_loss 0.44086461524795906\n",
            "Step 19993, train_loss 0.45244069181978414, test_loss 0.4408644089464378\n",
            "Step 19994, train_loss 0.45244022525497885, test_loss 0.44086420268790577\n",
            "Step 19995, train_loss 0.45243975872673614, test_loss 0.4408639964723556\n",
            "Step 19996, train_loss 0.45243929223505125, test_loss 0.44086379029977985\n",
            "Step 19997, train_loss 0.4524388257799192, test_loss 0.4408635841701712\n",
            "Step 19998, train_loss 0.45243835936133514, test_loss 0.4408633780835221\n",
            "Step 19999, train_loss 0.4524378929792941, test_loss 0.4408631720398252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yXlcgeRwZ3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4197b3bb-2c64-4219-92ca-b1081a953425"
      },
      "source": [
        "# Evaluation\n",
        "print(\"Accuracy score on test set: %f\" % accuracy_score(y_test, predictions))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# Show parameters\n",
        "print('w = ', w)\n",
        "print('b = ', b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score on test set: 0.815642\n",
            "Confusion Matrix:\n",
            "[[101  13]\n",
            " [ 20  45]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       114\n",
            "           1       0.78      0.69      0.73        65\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.81      0.79      0.80       179\n",
            "weighted avg       0.81      0.82      0.81       179\n",
            "\n",
            "w =  [[-1.18387774  2.56284417 -0.04074789 -0.21591208]]\n",
            "b =  [2.84100084]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd03Qlx8woet"
      },
      "source": [
        "# Accuracy score on train set: 0.799157\n",
        "# Accuracy score on test set: 0.793296\n",
        "# Confusion Matrix:\n",
        "# [[97 17]\n",
        "#  [20 45]]\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.83      0.85      0.84       114\n",
        "#            1       0.73      0.69      0.71        65\n",
        "\n",
        "#     accuracy                           0.79       179\n",
        "#    macro avg       0.78      0.77      0.77       179\n",
        "# weighted avg       0.79      0.79      0.79       179\n",
        "\n",
        "# -----------\n",
        "# Log loss on train set: 0.4452845704337929\n",
        "# Log loss on test set: 0.4466709454181867\n",
        "# w =  [[-1.18387774  2.56284417 -0.04074789 -0.21591208]]\n",
        "# b =  [2.84100084]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "xaHxH1Ay8m-V",
        "outputId": "1cb755a0-4e45-4181-f100-ab3801fd7809"
      },
      "source": [
        "# If you do everything correctly, you can plot train and test loss to see the learning curve\n",
        "# Yours should look something like this\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['train_loss'], label='Training Loss', alpha=0.5)\n",
        "plt.plot(history['test_loss'], label='Test Loss', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIj9GKy8Y9Pp"
      },
      "source": [
        "**Well done!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvEKOtJDg6P"
      },
      "source": [
        "![](https://www.researchgate.net/profile/Hayder_Al-Behadili/publication/325999203/figure/fig4/AS:641844216074241@1530038994324/Overfitting-and-underfitting-effect-on-error.png)"
      ]
    }
  ]
}